{"meta":{"version":1,"warehouse":"2.2.0"},"models":{"Asset":[{"_id":"themes/landscape/source/css/style.styl","path":"css/style.styl","modified":1,"renderable":1},{"_id":"themes/landscape/source/fancybox/blank.gif","path":"fancybox/blank.gif","modified":1,"renderable":1},{"_id":"themes/landscape/source/fancybox/fancybox_loading.gif","path":"fancybox/fancybox_loading.gif","modified":1,"renderable":1},{"_id":"themes/landscape/source/fancybox/fancybox_loading@2x.gif","path":"fancybox/fancybox_loading@2x.gif","modified":1,"renderable":1},{"_id":"themes/landscape/source/fancybox/fancybox_overlay.png","path":"fancybox/fancybox_overlay.png","modified":1,"renderable":1},{"_id":"themes/landscape/source/fancybox/fancybox_sprite.png","path":"fancybox/fancybox_sprite.png","modified":1,"renderable":1},{"_id":"themes/landscape/source/fancybox/fancybox_sprite@2x.png","path":"fancybox/fancybox_sprite@2x.png","modified":1,"renderable":1},{"_id":"themes/landscape/source/fancybox/jquery.fancybox.css","path":"fancybox/jquery.fancybox.css","modified":1,"renderable":1},{"_id":"themes/landscape/source/fancybox/jquery.fancybox.pack.js","path":"fancybox/jquery.fancybox.pack.js","modified":1,"renderable":1},{"_id":"themes/landscape/source/js/script.js","path":"js/script.js","modified":1,"renderable":1},{"_id":"themes/landscape/source/fancybox/jquery.fancybox.js","path":"fancybox/jquery.fancybox.js","modified":1,"renderable":1},{"_id":"themes/landscape/source/css/fonts/fontawesome-webfont.eot","path":"css/fonts/fontawesome-webfont.eot","modified":1,"renderable":1},{"_id":"themes/landscape/source/css/fonts/FontAwesome.otf","path":"css/fonts/FontAwesome.otf","modified":1,"renderable":1},{"_id":"themes/landscape/source/css/fonts/fontawesome-webfont.woff","path":"css/fonts/fontawesome-webfont.woff","modified":1,"renderable":1},{"_id":"themes/landscape/source/fancybox/helpers/fancybox_buttons.png","path":"fancybox/helpers/fancybox_buttons.png","modified":1,"renderable":1},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-buttons.css","path":"fancybox/helpers/jquery.fancybox-buttons.css","modified":1,"renderable":1},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-buttons.js","path":"fancybox/helpers/jquery.fancybox-buttons.js","modified":1,"renderable":1},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-media.js","path":"fancybox/helpers/jquery.fancybox-media.js","modified":1,"renderable":1},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-thumbs.css","path":"fancybox/helpers/jquery.fancybox-thumbs.css","modified":1,"renderable":1},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-thumbs.js","path":"fancybox/helpers/jquery.fancybox-thumbs.js","modified":1,"renderable":1},{"_id":"themes/landscape/source/css/fonts/fontawesome-webfont.ttf","path":"css/fonts/fontawesome-webfont.ttf","modified":1,"renderable":1},{"_id":"themes/landscape/source/css/fonts/fontawesome-webfont.svg","path":"css/fonts/fontawesome-webfont.svg","modified":1,"renderable":1},{"_id":"themes/landscape/source/css/images/banner.jpg","path":"css/images/banner.jpg","modified":1,"renderable":1},{"_id":"themes/flexy/source/LICENSE","path":"LICENSE","modified":0,"renderable":1},{"_id":"themes/flexy/source/scripts/index.js","path":"scripts/index.js","modified":0,"renderable":1},{"_id":"themes/flexy/source/images/apple-touch-icon-114x114.png","path":"images/apple-touch-icon-114x114.png","modified":0,"renderable":1},{"_id":"themes/flexy/source/images/apple-touch-icon-120x120.png","path":"images/apple-touch-icon-120x120.png","modified":0,"renderable":1},{"_id":"themes/flexy/source/images/apple-touch-icon-144x144.png","path":"images/apple-touch-icon-144x144.png","modified":0,"renderable":1},{"_id":"themes/flexy/source/images/apple-touch-icon-152x152.png","path":"images/apple-touch-icon-152x152.png","modified":0,"renderable":1},{"_id":"themes/flexy/source/images/apple-touch-icon-180x180.png","path":"images/apple-touch-icon-180x180.png","modified":0,"renderable":1},{"_id":"themes/flexy/source/images/apple-touch-icon-60x60.png","path":"images/apple-touch-icon-60x60.png","modified":0,"renderable":1},{"_id":"themes/flexy/source/images/apple-touch-icon-57x57.png","path":"images/apple-touch-icon-57x57.png","modified":0,"renderable":1},{"_id":"themes/flexy/source/images/apple-touch-icon-72x72.png","path":"images/apple-touch-icon-72x72.png","modified":0,"renderable":1},{"_id":"themes/flexy/source/images/apple-touch-icon-76x76.png","path":"images/apple-touch-icon-76x76.png","modified":0,"renderable":1},{"_id":"themes/flexy/source/images/apple-touch-icon-precomposed.png","path":"images/apple-touch-icon-precomposed.png","modified":0,"renderable":1},{"_id":"themes/flexy/source/images/apple-touch-icon.png","path":"images/apple-touch-icon.png","modified":0,"renderable":1},{"_id":"themes/flexy/source/images/favicon-160x160.png","path":"images/favicon-160x160.png","modified":0,"renderable":1},{"_id":"themes/flexy/source/images/favicon-16x16.png","path":"images/favicon-16x16.png","modified":0,"renderable":1},{"_id":"themes/flexy/source/images/favicon-192x192.png","path":"images/favicon-192x192.png","modified":0,"renderable":1},{"_id":"themes/flexy/source/images/favicon-32x32.png","path":"images/favicon-32x32.png","modified":0,"renderable":1},{"_id":"themes/flexy/source/images/favicon-96x96.png","path":"images/favicon-96x96.png","modified":0,"renderable":1},{"_id":"themes/flexy/source/images/mstile-144x144.png","path":"images/mstile-144x144.png","modified":0,"renderable":1},{"_id":"themes/flexy/source/images/mstile-150x150.png","path":"images/mstile-150x150.png","modified":0,"renderable":1},{"_id":"themes/flexy/source/images/mstile-70x70.png","path":"images/mstile-70x70.png","modified":0,"renderable":1},{"_id":"themes/flexy/source/images/mstile-310x310.png","path":"images/mstile-310x310.png","modified":0,"renderable":1},{"_id":"themes/flexy/source/images/mstile-310x150.png","path":"images/mstile-310x150.png","modified":0,"renderable":1},{"_id":"themes/flexy/source/images/svdb.png","path":"images/svdb.png","modified":0,"renderable":1},{"_id":"themes/flexy/source/images/stars.svg","path":"images/stars.svg","modified":0,"renderable":1},{"_id":"themes/flexy/source/styles/normalize.css","path":"styles/normalize.css","modified":0,"renderable":1},{"_id":"themes/flexy/source/styles/screen.styl","path":"styles/screen.styl","modified":0,"renderable":1},{"_id":"themes/flexy/source/images/screenshot.png","path":"images/screenshot.png","modified":0,"renderable":1},{"_id":"themes/flexy/source/images/header.jpg","path":"images/header.jpg","modified":0,"renderable":1},{"_id":"themes/flexy/source/images/screenshot_mobile.png","path":"images/screenshot_mobile.png","modified":0,"renderable":1},{"_id":"themes/flexy/source/fonts/droidserif/400.woff","path":"fonts/droidserif/400.woff","modified":0,"renderable":1},{"_id":"themes/flexy/source/fonts/droidserif/400.woff2","path":"fonts/droidserif/400.woff2","modified":0,"renderable":1},{"_id":"themes/flexy/source/fonts/droidserif/400i.woff2","path":"fonts/droidserif/400i.woff2","modified":0,"renderable":1},{"_id":"themes/flexy/source/fonts/droidserif/400i.woff","path":"fonts/droidserif/400i.woff","modified":0,"renderable":1},{"_id":"themes/flexy/source/fonts/droidserif/700.woff2","path":"fonts/droidserif/700.woff2","modified":0,"renderable":1},{"_id":"themes/flexy/source/fonts/droidserif/700.woff","path":"fonts/droidserif/700.woff","modified":0,"renderable":1},{"_id":"themes/flexy/source/fonts/droidserif/700i.woff","path":"fonts/droidserif/700i.woff","modified":0,"renderable":1},{"_id":"themes/flexy/source/fonts/droidserif/700i.woff2","path":"fonts/droidserif/700i.woff2","modified":0,"renderable":1},{"_id":"themes/flexy/source/fonts/fira/400.woff2","path":"fonts/fira/400.woff2","modified":0,"renderable":1},{"_id":"themes/flexy/source/fonts/fira/500.woff2","path":"fonts/fira/500.woff2","modified":0,"renderable":1},{"_id":"themes/flexy/source/fonts/opensans/300.woff","path":"fonts/opensans/300.woff","modified":0,"renderable":1},{"_id":"themes/flexy/source/fonts/opensans/300.woff2","path":"fonts/opensans/300.woff2","modified":0,"renderable":1},{"_id":"themes/flexy/source/fonts/opensans/400.woff2","path":"fonts/opensans/400.woff2","modified":0,"renderable":1},{"_id":"themes/flexy/source/fonts/opensans/300i.woff2","path":"fonts/opensans/300i.woff2","modified":0,"renderable":1},{"_id":"themes/flexy/source/fonts/opensans/400.woff","path":"fonts/opensans/400.woff","modified":0,"renderable":1},{"_id":"themes/flexy/source/fonts/opensans/400i.woff2","path":"fonts/opensans/400i.woff2","modified":0,"renderable":1},{"_id":"themes/flexy/source/fonts/opensans/600.woff2","path":"fonts/opensans/600.woff2","modified":0,"renderable":1},{"_id":"themes/flexy/source/fonts/opensans/700.woff","path":"fonts/opensans/700.woff","modified":0,"renderable":1},{"_id":"themes/flexy/source/fonts/opensans/600i.woff2","path":"fonts/opensans/600i.woff2","modified":0,"renderable":1},{"_id":"themes/flexy/source/fonts/opensans/700.woff2","path":"fonts/opensans/700.woff2","modified":0,"renderable":1},{"_id":"themes/flexy/source/fonts/opensans/700i.woff2","path":"fonts/opensans/700i.woff2","modified":0,"renderable":1},{"_id":"themes/flexy/source/fonts/opensans/800.woff2","path":"fonts/opensans/800.woff2","modified":0,"renderable":1},{"_id":"themes/flexy/source/fonts/opensans/800i.woff2","path":"fonts/opensans/800i.woff2","modified":0,"renderable":1},{"_id":"themes/flexy/source/fonts/fira/500.woff","path":"fonts/fira/500.woff","modified":0,"renderable":1},{"_id":"themes/flexy/source/fonts/fira/400.woff","path":"fonts/fira/400.woff","modified":0,"renderable":1},{"_id":"themes/flexy/source/fonts/fira/700.woff","path":"fonts/fira/700.woff","modified":0,"renderable":1},{"_id":"themes/flexy/source/fonts/fira/700.woff2","path":"fonts/fira/700.woff2","modified":0,"renderable":1},{"_id":"themes/flexy/source/fonts/opensans/300i.woff","path":"fonts/opensans/300i.woff","modified":0,"renderable":1},{"_id":"themes/flexy/source/fonts/opensans/400i.woff","path":"fonts/opensans/400i.woff","modified":0,"renderable":1},{"_id":"themes/flexy/source/fonts/opensans/600.woff","path":"fonts/opensans/600.woff","modified":0,"renderable":1},{"_id":"themes/flexy/source/fonts/opensans/600i.woff","path":"fonts/opensans/600i.woff","modified":0,"renderable":1},{"_id":"themes/flexy/source/fonts/opensans/700i.woff","path":"fonts/opensans/700i.woff","modified":0,"renderable":1},{"_id":"themes/flexy/source/fonts/opensans/800.woff","path":"fonts/opensans/800.woff","modified":0,"renderable":1},{"_id":"themes/flexy/source/fonts/opensans/800i.woff","path":"fonts/opensans/800i.woff","modified":0,"renderable":1},{"_id":"themes/even/source/favicon.ico","path":"favicon.ico","modified":1,"renderable":1},{"_id":"themes/even/source/css/style.css","path":"css/style.css","modified":1,"renderable":1},{"_id":"themes/even/source/js/back2top.js","path":"js/back2top.js","modified":1,"renderable":1},{"_id":"themes/even/source/fonts/iconfont/iconfont.ttf","path":"fonts/iconfont/iconfont.ttf","modified":1,"renderable":1},{"_id":"themes/even/source/fonts/iconfont/iconfont.css","path":"fonts/iconfont/iconfont.css","modified":1,"renderable":1},{"_id":"themes/even/source/fonts/iconfont/iconfont.svg","path":"fonts/iconfont/iconfont.svg","modified":1,"renderable":1},{"_id":"themes/even/source/fonts/iconfont/iconfont.woff","path":"fonts/iconfont/iconfont.woff","modified":1,"renderable":1},{"_id":"themes/even/source/fonts/iconfont/iconfont.eot","path":"fonts/iconfont/iconfont.eot","modified":1,"renderable":1},{"_id":"themes/even/source/fonts/chancery/apple-chancery-webfont.ttf","path":"fonts/chancery/apple-chancery-webfont.ttf","modified":1,"renderable":1},{"_id":"themes/even/source/fonts/chancery/apple-chancery-webfont.eot","path":"fonts/chancery/apple-chancery-webfont.eot","modified":1,"renderable":1},{"_id":"themes/even/source/fonts/chancery/apple-chancery-webfont.woff2","path":"fonts/chancery/apple-chancery-webfont.woff2","modified":1,"renderable":1},{"_id":"themes/even/source/fonts/chancery/apple-chancery-webfont.woff","path":"fonts/chancery/apple-chancery-webfont.woff","modified":1,"renderable":1},{"_id":"themes/even/source/fonts/chancery/apple-chancery-webfont.svg","path":"fonts/chancery/apple-chancery-webfont.svg","modified":1,"renderable":1},{"_id":"themes/apollo/source/favicon.png","path":"favicon.png","modified":1,"renderable":1},{"_id":"themes/apollo/source/css/apollo.css","path":"css/apollo.css","modified":1,"renderable":1},{"_id":"themes/apollo/source/scss/apollo.scss","path":"scss/apollo.scss","modified":1,"renderable":1}],"Cache":[{"_id":"themes/landscape/.gitignore","hash":"58d26d4b5f2f94c2d02a4e4a448088e4a2527c77","modified":1467174358000},{"_id":"themes/landscape/Gruntfile.js","hash":"71adaeaac1f3cc56e36c49d549b8d8a72235c9b9","modified":1467174358000},{"_id":"themes/landscape/LICENSE","hash":"c480fce396b23997ee23cc535518ffaaf7f458f8","modified":1467174358000},{"_id":"themes/landscape/README.md","hash":"c7e83cfe8f2c724fc9cac32bd71bb5faf9ceeddb","modified":1467174358000},{"_id":"themes/landscape/_config.yml","hash":"fb8c98a0f6ff9f962637f329c22699721854cd73","modified":1467174358000},{"_id":"themes/landscape/package.json","hash":"85358dc34311c6662e841584e206a4679183943f","modified":1467174358000},{"_id":"source/_posts/hello-world.md","hash":"8a02477044e2b77f1b262da2c48c01429e4a32e4","modified":1467174356000},{"_id":"themes/landscape/layout/archive.ejs","hash":"2703b07cc8ac64ae46d1d263f4653013c7e1666b","modified":1467174358000},{"_id":"themes/landscape/layout/category.ejs","hash":"765426a9c8236828dc34759e604cc2c52292835a","modified":1467174358000},{"_id":"themes/landscape/layout/index.ejs","hash":"aa1b4456907bdb43e629be3931547e2d29ac58c8","modified":1467174358000},{"_id":"themes/landscape/layout/layout.ejs","hash":"f155824ca6130080bb057fa3e868a743c69c4cf5","modified":1467174358000},{"_id":"themes/landscape/layout/page.ejs","hash":"7d80e4e36b14d30a7cd2ac1f61376d9ebf264e8b","modified":1467174358000},{"_id":"themes/landscape/layout/post.ejs","hash":"7d80e4e36b14d30a7cd2ac1f61376d9ebf264e8b","modified":1467174358000},{"_id":"themes/landscape/layout/tag.ejs","hash":"eaa7b4ccb2ca7befb90142e4e68995fb1ea68b2e","modified":1467174358000},{"_id":"themes/landscape/languages/fr.yml","hash":"84ab164b37c6abf625473e9a0c18f6f815dd5fd9","modified":1467174358000},{"_id":"themes/landscape/languages/nl.yml","hash":"12ed59faba1fc4e8cdd1d42ab55ef518dde8039c","modified":1467174358000},{"_id":"themes/landscape/languages/default.yml","hash":"3083f319b352d21d80fc5e20113ddf27889c9d11","modified":1467174358000},{"_id":"themes/landscape/languages/ru.yml","hash":"4fda301bbd8b39f2c714e2c934eccc4b27c0a2b0","modified":1467174358000},{"_id":"themes/landscape/languages/no.yml","hash":"965a171e70347215ec726952e63f5b47930931ef","modified":1467174358000},{"_id":"themes/landscape/languages/zh-CN.yml","hash":"ca40697097ab0b3672a80b455d3f4081292d1eed","modified":1467174358000},{"_id":"themes/landscape/languages/zh-TW.yml","hash":"53ce3000c5f767759c7d2c4efcaa9049788599c3","modified":1467174358000},{"_id":"themes/landscape/scripts/fancybox.js","hash":"aa411cd072399df1ddc8e2181a3204678a5177d9","modified":1467174358000},{"_id":"themes/landscape/layout/_partial/after-footer.ejs","hash":"82a30f81c0e8ba4a8af17acd6cc99e93834e4d5e","modified":1467174358000},{"_id":"themes/landscape/layout/_partial/archive-post.ejs","hash":"c7a71425a946d05414c069ec91811b5c09a92c47","modified":1467174358000},{"_id":"themes/landscape/layout/_partial/archive.ejs","hash":"931aaaffa0910a48199388ede576184ff15793ee","modified":1467174358000},{"_id":"themes/landscape/layout/_partial/footer.ejs","hash":"93518893cf91287e797ebac543c560e2a63b8d0e","modified":1467174358000},{"_id":"themes/landscape/layout/_partial/article.ejs","hash":"c4c835615d96a950d51fa2c3b5d64d0596534fed","modified":1467174358000},{"_id":"themes/landscape/layout/_partial/google-analytics.ejs","hash":"f921e7f9223d7c95165e0f835f353b2938e40c45","modified":1467174358000},{"_id":"themes/landscape/layout/_partial/head.ejs","hash":"4fe8853e864d192701c03e5cd3a5390287b90612","modified":1467174358000},{"_id":"themes/landscape/layout/_partial/header.ejs","hash":"c21ca56f419d01a9f49c27b6be9f4a98402b2aa3","modified":1467174358000},{"_id":"themes/landscape/layout/_partial/mobile-nav.ejs","hash":"e952a532dfc583930a666b9d4479c32d4a84b44e","modified":1467174358000},{"_id":"themes/landscape/layout/_widget/archive.ejs","hash":"beb4a86fcc82a9bdda9289b59db5a1988918bec3","modified":1467174358000},{"_id":"themes/landscape/layout/_partial/sidebar.ejs","hash":"930da35cc2d447a92e5ee8f835735e6fd2232469","modified":1467174358000},{"_id":"themes/landscape/layout/_widget/category.ejs","hash":"dd1e5af3c6af3f5d6c85dfd5ca1766faed6a0b05","modified":1467174358000},{"_id":"themes/landscape/layout/_widget/recent_posts.ejs","hash":"0d4f064733f8b9e45c0ce131fe4a689d570c883a","modified":1467174358000},{"_id":"themes/landscape/layout/_widget/tag.ejs","hash":"2de380865df9ab5f577f7d3bcadf44261eb5faae","modified":1467174358000},{"_id":"themes/landscape/layout/_widget/tagcloud.ejs","hash":"b4a2079101643f63993dcdb32925c9b071763b46","modified":1467174358000},{"_id":"themes/landscape/source/css/_extend.styl","hash":"222fbe6d222531d61c1ef0f868c90f747b1c2ced","modified":1467174358000},{"_id":"themes/landscape/source/css/_variables.styl","hash":"5e37a6571caf87149af83ac1cc0cdef99f117350","modified":1467174358000},{"_id":"themes/landscape/source/css/style.styl","hash":"a70d9c44dac348d742702f6ba87e5bb3084d65db","modified":1467174358000},{"_id":"themes/landscape/source/fancybox/blank.gif","hash":"2daeaa8b5f19f0bc209d976c02bd6acb51b00b0a","modified":1467174358000},{"_id":"themes/landscape/source/fancybox/fancybox_loading.gif","hash":"1a755fb2599f3a313cc6cfdb14df043f8c14a99c","modified":1467174358000},{"_id":"themes/landscape/source/fancybox/fancybox_loading@2x.gif","hash":"273b123496a42ba45c3416adb027cd99745058b0","modified":1467174358000},{"_id":"themes/landscape/source/fancybox/fancybox_overlay.png","hash":"b3a4ee645ba494f52840ef8412015ba0f465dbe0","modified":1467174358000},{"_id":"themes/landscape/source/fancybox/fancybox_sprite.png","hash":"17df19f97628e77be09c352bf27425faea248251","modified":1467174358000},{"_id":"themes/landscape/source/fancybox/fancybox_sprite@2x.png","hash":"30c58913f327e28f466a00f4c1ac8001b560aed8","modified":1467174358000},{"_id":"themes/landscape/source/fancybox/jquery.fancybox.css","hash":"aaa582fb9eb4b7092dc69fcb2d5b1c20cca58ab6","modified":1467174358000},{"_id":"themes/landscape/source/fancybox/jquery.fancybox.pack.js","hash":"9e0d51ca1dbe66f6c0c7aefd552dc8122e694a6e","modified":1467174358000},{"_id":"themes/landscape/source/js/script.js","hash":"2876e0b19ce557fca38d7c6f49ca55922ab666a1","modified":1467174358000},{"_id":"themes/landscape/source/fancybox/jquery.fancybox.js","hash":"d08b03a42d5c4ba456ef8ba33116fdbb7a9cabed","modified":1467174358000},{"_id":"themes/landscape/layout/_partial/post/date.ejs","hash":"6197802873157656e3077c5099a7dda3d3b01c29","modified":1467174358000},{"_id":"themes/landscape/layout/_partial/post/category.ejs","hash":"c6bcd0e04271ffca81da25bcff5adf3d46f02fc0","modified":1467174358000},{"_id":"themes/landscape/layout/_partial/post/gallery.ejs","hash":"3d9d81a3c693ff2378ef06ddb6810254e509de5b","modified":1467174358000},{"_id":"themes/landscape/layout/_partial/post/nav.ejs","hash":"16a904de7bceccbb36b4267565f2215704db2880","modified":1467174358000},{"_id":"themes/landscape/layout/_partial/post/tag.ejs","hash":"2fcb0bf9c8847a644167a27824c9bb19ac74dd14","modified":1467174358000},{"_id":"themes/landscape/layout/_partial/post/title.ejs","hash":"2f275739b6f1193c123646a5a31f37d48644c667","modified":1467174358000},{"_id":"themes/landscape/source/css/_partial/archive.styl","hash":"db15f5677dc68f1730e82190bab69c24611ca292","modified":1467174358000},{"_id":"themes/landscape/source/css/_partial/article.styl","hash":"10685f8787a79f79c9a26c2f943253450c498e3e","modified":1467174358000},{"_id":"themes/landscape/source/css/_partial/comment.styl","hash":"79d280d8d203abb3bd933ca9b8e38c78ec684987","modified":1467174358000},{"_id":"themes/landscape/source/css/_partial/footer.styl","hash":"e35a060b8512031048919709a8e7b1ec0e40bc1b","modified":1467174358000},{"_id":"themes/landscape/source/css/_partial/header.styl","hash":"85ab11e082f4dd86dde72bed653d57ec5381f30c","modified":1467174358000},{"_id":"themes/landscape/source/css/_partial/highlight.styl","hash":"bf4e7be1968dad495b04e83c95eac14c4d0ad7c0","modified":1467174358000},{"_id":"themes/landscape/source/css/_partial/mobile.styl","hash":"a399cf9e1e1cec3e4269066e2948d7ae5854d745","modified":1467174358000},{"_id":"themes/landscape/source/css/_partial/sidebar-aside.styl","hash":"890349df5145abf46ce7712010c89237900b3713","modified":1467174358000},{"_id":"themes/landscape/source/css/_partial/sidebar-bottom.styl","hash":"8fd4f30d319542babfd31f087ddbac550f000a8a","modified":1467174358000},{"_id":"themes/landscape/source/css/_partial/sidebar.styl","hash":"404ec059dc674a48b9ab89cd83f258dec4dcb24d","modified":1467174358000},{"_id":"themes/landscape/source/css/_util/grid.styl","hash":"0bf55ee5d09f193e249083602ac5fcdb1e571aed","modified":1467174358000},{"_id":"themes/landscape/source/css/_util/mixin.styl","hash":"44f32767d9fd3c1c08a60d91f181ee53c8f0dbb3","modified":1467174358000},{"_id":"themes/landscape/source/css/fonts/fontawesome-webfont.eot","hash":"7619748fe34c64fb157a57f6d4ef3678f63a8f5e","modified":1467174358000},{"_id":"themes/landscape/source/css/fonts/FontAwesome.otf","hash":"b5b4f9be85f91f10799e87a083da1d050f842734","modified":1467174358000},{"_id":"themes/landscape/source/css/fonts/fontawesome-webfont.woff","hash":"04c3bf56d87a0828935bd6b4aee859995f321693","modified":1467174358000},{"_id":"themes/landscape/source/fancybox/helpers/fancybox_buttons.png","hash":"e385b139516c6813dcd64b8fc431c364ceafe5f3","modified":1467174358000},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-buttons.css","hash":"1a9d8e5c22b371fcc69d4dbbb823d9c39f04c0c8","modified":1467174358000},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-buttons.js","hash":"dc3645529a4bf72983a39fa34c1eb9146e082019","modified":1467174358000},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-media.js","hash":"294420f9ff20f4e3584d212b0c262a00a96ecdb3","modified":1467174358000},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-thumbs.css","hash":"4ac329c16a5277592fc12a37cca3d72ca4ec292f","modified":1467174358000},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-thumbs.js","hash":"47da1ae5401c24b5c17cc18e2730780f5c1a7a0c","modified":1467174358000},{"_id":"themes/landscape/source/css/fonts/fontawesome-webfont.ttf","hash":"7f09c97f333917034ad08fa7295e916c9f72fd3f","modified":1467174358000},{"_id":"themes/landscape/source/css/fonts/fontawesome-webfont.svg","hash":"46fcc0194d75a0ddac0a038aee41b23456784814","modified":1467174358000},{"_id":"themes/landscape/source/css/images/banner.jpg","hash":"f44aa591089fcb3ec79770a1e102fd3289a7c6a6","modified":1467174358000},{"_id":"themes/flexy/_config.yml","hash":"e2f923e63a624d92780afe7fc25542dae9e890d2","modified":1467174997000},{"_id":"themes/flexy/README.md","hash":"74c5c6ec0499b8fc4c5349c085877662720d0d3e","modified":1467174997000},{"_id":"source/_posts/NGS-Workflows.md","hash":"be0bd52d8a5086d180604d40dd629ba489c00ea3","modified":1467174793000},{"_id":"themes/flexy/.git/HEAD","hash":"acbaef275e46a7f14c1ef456fff2c8bbe8c84724","modified":1467174997000},{"_id":"themes/flexy/.git/config","hash":"87df85ce47d87ecfd6d1d6ba58e45a3e1c2273b6","modified":1467174997000},{"_id":"themes/flexy/.git/description","hash":"9635f1b7e12c045212819dd934d809ef07efa2f4","modified":1467174995000},{"_id":"themes/flexy/.git/packed-refs","hash":"89ff3de45108526527249998cf562a913e1ddf86","modified":1467174997000},{"_id":"themes/flexy/.git/index","hash":"178b34db3bb4c9d07bbcdf2064c0631f689516a8","modified":1467174997000},{"_id":"themes/flexy/layout/_comments.jade","hash":"4ac668fa7883ecb1ae8230d5f023e6936f17403a","modified":1467174997000},{"_id":"themes/flexy/layout/_content.jade","hash":"3575890bd361d7de67f43e474a385b99d20c7892","modified":1467174997000},{"_id":"themes/flexy/layout/_head.jade","hash":"95ec1e0b9b638fdd5358e8828d7b726f8dc73487","modified":1467174997000},{"_id":"themes/flexy/layout/_footer.jade","hash":"173bc1c1e85324b478df9bb288d28cbdd79757fb","modified":1467174997000},{"_id":"themes/flexy/layout/_mixins.jade","hash":"fef1a26fef90db4c5f41e6c56cdbf284e6786b8a","modified":1467174997000},{"_id":"themes/flexy/layout/index.jade","hash":"17f6ec2ed0ab4e0fc6cf2007265e4ade41bc6714","modified":1467174997000},{"_id":"themes/flexy/layout/_scripts.jade","hash":"bc1900431636422256c96a51ff7a3d5943cf2275","modified":1467174997000},{"_id":"themes/flexy/layout/_header.jade","hash":"c02055dcc09d0ef115361627e963027e9f6771a1","modified":1467174997000},{"_id":"themes/flexy/layout/layout.jade","hash":"d97e0842a86732f4f54b5034ecad18ff15a5e232","modified":1467174997000},{"_id":"themes/flexy/source/LICENSE","hash":"5fb246d94c78591270f46f8cdd241f3fc06fe73c","modified":1467174997000},{"_id":"themes/flexy/source/scripts/index.js","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1467174997000},{"_id":"themes/flexy/.git/hooks/commit-msg.sample","hash":"ee1ed5aad98a435f2020b6de35c173b75d9affac","modified":1467174995000},{"_id":"themes/flexy/.git/hooks/applypatch-msg.sample","hash":"86b9655a9ebbde13ac8dd5795eb4d5b539edab0f","modified":1467174995000},{"_id":"themes/flexy/.git/hooks/post-update.sample","hash":"b614c2f63da7dca9f1db2e7ade61ef30448fc96c","modified":1467174995000},{"_id":"themes/flexy/.git/hooks/pre-applypatch.sample","hash":"42fa41564917b44183a50c4d94bb03e1768ddad8","modified":1467174995000},{"_id":"themes/flexy/.git/hooks/pre-commit.sample","hash":"36aed8976dcc08b5076844f0ec645b18bc37758f","modified":1467174995000},{"_id":"themes/flexy/.git/hooks/pre-push.sample","hash":"5c8518bfd1d1d3d2c1a7194994c0a16d8a313a41","modified":1467174995000},{"_id":"themes/flexy/.git/hooks/prepare-commit-msg.sample","hash":"2b6275eda365cad50d167fe3a387c9bc9fedd54f","modified":1467174995000},{"_id":"themes/flexy/.git/hooks/pre-rebase.sample","hash":"5885a56ab4fca8075a05a562d005e922cde9853b","modified":1467174995000},{"_id":"themes/flexy/.git/hooks/update.sample","hash":"39355a075977d05708ef74e1b66d09a36e486df1","modified":1467174995000},{"_id":"themes/flexy/.git/info/exclude","hash":"c879df015d97615050afa7b9641e3352a1e701ac","modified":1467174995000},{"_id":"themes/flexy/.git/logs/HEAD","hash":"d17503afab9f4a71746278f351e59c4a7ecdaf5a","modified":1467174997000},{"_id":"themes/flexy/source/images/apple-touch-icon-114x114.png","hash":"cbef1e3433399811316b6bd8f519ad9992c489b6","modified":1467174997000},{"_id":"themes/flexy/source/images/apple-touch-icon-120x120.png","hash":"1ea59bfbc25f77f4e1fb69b38df47a7c88655a8f","modified":1467174997000},{"_id":"themes/flexy/source/images/apple-touch-icon-144x144.png","hash":"cf59bb1fbcded183fc8c5b7fa37047357e93fce7","modified":1467174997000},{"_id":"themes/flexy/source/images/apple-touch-icon-152x152.png","hash":"a05135fb4a24124c09926ac4bca04923eabfac1a","modified":1467174997000},{"_id":"themes/flexy/source/images/apple-touch-icon-180x180.png","hash":"3348da88abbbe24d7117cb7db23f3e568bf996ed","modified":1467174997000},{"_id":"themes/flexy/source/images/apple-touch-icon-60x60.png","hash":"ca44431ad1de284c2f9ce34efbc7afc2d1ad89dc","modified":1467174997000},{"_id":"themes/flexy/source/images/apple-touch-icon-57x57.png","hash":"0f65353cb3689abd41b9300cf0b501a89dea820b","modified":1467174997000},{"_id":"themes/flexy/source/images/apple-touch-icon-72x72.png","hash":"b262f00b3f2a4724cac9a8286e7e6a4bfd2bce63","modified":1467174997000},{"_id":"themes/flexy/source/images/apple-touch-icon-76x76.png","hash":"060bed69bbea9e733e4ca93d6dc062110d8d10b0","modified":1467174997000},{"_id":"themes/flexy/source/images/apple-touch-icon-precomposed.png","hash":"3348da88abbbe24d7117cb7db23f3e568bf996ed","modified":1467174997000},{"_id":"themes/flexy/source/images/apple-touch-icon.png","hash":"3348da88abbbe24d7117cb7db23f3e568bf996ed","modified":1467174997000},{"_id":"themes/flexy/source/images/favicon-160x160.png","hash":"89d50f4cee2676d5372591c3b2fa0d166a252300","modified":1467174997000},{"_id":"themes/flexy/source/images/favicon-16x16.png","hash":"1fcbd57e850c669c07e6c285e9e88db81aa3f36d","modified":1467174997000},{"_id":"themes/flexy/source/images/favicon-192x192.png","hash":"87465e5670cbbdbd61822f4b954dc924d54637f4","modified":1467174997000},{"_id":"themes/flexy/source/images/favicon-32x32.png","hash":"a69146990bb70d055be62955dba89f2cb792a240","modified":1467174997000},{"_id":"themes/flexy/source/images/favicon-96x96.png","hash":"6bece8a5e01061495ed265dc64f5ac20863db850","modified":1467174997000},{"_id":"themes/flexy/source/images/mstile-144x144.png","hash":"cf59bb1fbcded183fc8c5b7fa37047357e93fce7","modified":1467174997000},{"_id":"themes/flexy/source/images/mstile-150x150.png","hash":"88d146aac9893b81c271efcb5b00c4e8f9ef2c6e","modified":1467174997000},{"_id":"themes/flexy/source/images/mstile-70x70.png","hash":"f49e0ac8c5b02c6b16b54e2413353674f59144cb","modified":1467174997000},{"_id":"themes/flexy/source/images/mstile-310x310.png","hash":"0c9086e2ae8ff6be27abedb39aba04f94155d28b","modified":1467174997000},{"_id":"themes/flexy/source/images/mstile-310x150.png","hash":"bc2db261605be6077d9363fc15d595a778c30e7b","modified":1467174997000},{"_id":"themes/flexy/source/images/svdb.png","hash":"fe82debb84a345bf144bb0b2b86ee9dc3fa0a4cc","modified":1467174997000},{"_id":"themes/flexy/source/images/stars.svg","hash":"0edad947c46ffcddbb0ce51a74661437adbb0b67","modified":1467174997000},{"_id":"themes/flexy/source/styles/_mixins.styl","hash":"9a6d2de6090e9d7ab140a8c95d634012b26031dd","modified":1467174997000},{"_id":"themes/flexy/source/styles/normalize.css","hash":"02fe53286d071637534d5aa2c57c76c168c0d521","modified":1467174997000},{"_id":"themes/flexy/source/styles/screen.styl","hash":"3253250aa7a6b10256a5bea5cfcb2388932e9f1b","modified":1467174997000},{"_id":"themes/flexy/source/images/screenshot.png","hash":"9543ed2967fe81e127ad8b264d1a54eb93ae2642","modified":1467174997000},{"_id":"themes/flexy/source/images/header.jpg","hash":"0374099fa16559f4c834c8a5c9150fff2cbefa86","modified":1467174997000},{"_id":"themes/flexy/source/images/screenshot_mobile.png","hash":"e19001e7019c993e2c714b83e0105bd617522c71","modified":1467174997000},{"_id":"themes/flexy/.git/objects/pack/pack-d2f60c962dc721cb113abb9eaeaff52de651c260.idx","hash":"23b58d7220323c9657ebdcc5c3d5a9cc92a2c140","modified":1467174997000},{"_id":"themes/flexy/.git/refs/heads/master","hash":"d37b182008fc636ed6f63b55aa7b13273d7a51f1","modified":1467174997000},{"_id":"themes/flexy/source/fonts/droidserif/400.woff","hash":"b76e07a7810ea30c4d5b0b75d0cf0dfdbcbe9638","modified":1467174997000},{"_id":"themes/flexy/source/fonts/droidserif/400.woff2","hash":"049f6150916410a91db63b2706e9092e3a4dd658","modified":1467174997000},{"_id":"themes/flexy/source/fonts/droidserif/400i.woff2","hash":"5bae168b5d9bafd7f9b687abd504e36c4ae96fc7","modified":1467174997000},{"_id":"themes/flexy/source/fonts/droidserif/400i.woff","hash":"52a11ef2a5cee34ae60baae0d469f2a9a2eda32f","modified":1467174997000},{"_id":"themes/flexy/source/fonts/droidserif/700.woff2","hash":"33c8bad63f49c0af82034fdc9e5fa61319282bd7","modified":1467174997000},{"_id":"themes/flexy/source/fonts/droidserif/700.woff","hash":"a2e50d611e6db7009168cc66cb8253da02c2b170","modified":1467174997000},{"_id":"themes/flexy/source/fonts/droidserif/700i.woff","hash":"f47662a676ecfed7cc038b35879ecc10b2ddcb96","modified":1467174997000},{"_id":"themes/flexy/source/fonts/droidserif/700i.woff2","hash":"1821452abc2980b3e7f3b0fba39f89d03b98e235","modified":1467174997000},{"_id":"themes/flexy/source/fonts/fira/400.woff2","hash":"a8e246df0368fd8eef4806d9a1b36cf914beae15","modified":1467174997000},{"_id":"themes/flexy/source/fonts/fira/500.woff2","hash":"86d5b01469d71971cb0aa0f6f6f7946c045ff628","modified":1467174997000},{"_id":"themes/flexy/source/fonts/opensans/300.woff","hash":"d763a1e180d642818df3d38f4869e2378f0b55d0","modified":1467174997000},{"_id":"themes/flexy/source/fonts/opensans/300.woff2","hash":"ad2af0f0a073835100e66ee93b50def2e57a28df","modified":1467174997000},{"_id":"themes/flexy/source/fonts/opensans/400.woff2","hash":"9b18ae04f11fc74d27f281737b23b45a4bad5937","modified":1467174997000},{"_id":"themes/flexy/source/fonts/opensans/300i.woff2","hash":"d45729f6376fe48d2b8a2c611b651a7f1502ae01","modified":1467174997000},{"_id":"themes/flexy/source/fonts/opensans/400.woff","hash":"7dd17593d3947f4ea10be937634ef8f553443e5a","modified":1467174997000},{"_id":"themes/flexy/source/fonts/opensans/400i.woff2","hash":"24cc685201e9dd17ba9fed66f61fa4626f83211b","modified":1467174997000},{"_id":"themes/flexy/source/fonts/opensans/600.woff2","hash":"931f5105f0e909f90bdea2e246a1a230809a699a","modified":1467174997000},{"_id":"themes/flexy/source/fonts/opensans/700.woff","hash":"7956fd048338c3c6253aa58f65442441866d4b4e","modified":1467174997000},{"_id":"themes/flexy/source/fonts/opensans/600i.woff2","hash":"e51180cddda569a91790c2dc4c352d06831bb07b","modified":1467174997000},{"_id":"themes/flexy/source/fonts/opensans/700.woff2","hash":"074d6e274d90a1b510d6eff3a5f8d26f567c1575","modified":1467174997000},{"_id":"themes/flexy/source/fonts/opensans/700i.woff2","hash":"649af46dba2ec76c793426335e4026ca6c7a6109","modified":1467174997000},{"_id":"themes/flexy/source/fonts/opensans/800.woff2","hash":"ee323b2accd80d6845d5cb1ba426bea25f087abe","modified":1467174997000},{"_id":"themes/flexy/source/fonts/opensans/800i.woff2","hash":"649af46dba2ec76c793426335e4026ca6c7a6109","modified":1467174997000},{"_id":"themes/flexy/source/fonts/fira/500.woff","hash":"5d4c15722ac01fb9c74662496967d6ebeda078d7","modified":1467174997000},{"_id":"themes/flexy/source/fonts/fira/400.woff","hash":"4ee1b0356eac96f44b48a5ecc36584c4c81e33bd","modified":1467174997000},{"_id":"themes/flexy/source/fonts/fira/700.woff","hash":"4b2ef8d1724a48a42afc5e7ff8ee77a960ed6895","modified":1467174997000},{"_id":"themes/flexy/source/fonts/fira/700.woff2","hash":"798f8fe64d02e466b546e16cf82a6ce914e96db5","modified":1467174997000},{"_id":"themes/flexy/source/fonts/opensans/300i.woff","hash":"dae3f536b867ea3d6cce0ef1406ab9b0813c16ea","modified":1467174997000},{"_id":"themes/flexy/source/fonts/opensans/400i.woff","hash":"9599e5da71b62e5227becb48e5498939e636e923","modified":1467174997000},{"_id":"themes/flexy/source/fonts/opensans/600.woff","hash":"7c9f1210d31388fe5df9e368bd0e73a8f4091b28","modified":1467174997000},{"_id":"themes/flexy/source/fonts/opensans/600i.woff","hash":"9406a2e087eb0d1790c2a7348b940f773ae6dda3","modified":1467174997000},{"_id":"themes/flexy/source/fonts/opensans/700i.woff","hash":"87f22f6a8759ae4bc18d97469091d9b218210b45","modified":1467174997000},{"_id":"themes/flexy/source/fonts/opensans/800.woff","hash":"31075459cd2010120a805d95506a9f8d7005b228","modified":1467174997000},{"_id":"themes/flexy/source/fonts/opensans/800i.woff","hash":"87f22f6a8759ae4bc18d97469091d9b218210b45","modified":1467174997000},{"_id":"themes/flexy/.git/logs/refs/heads/master","hash":"d17503afab9f4a71746278f351e59c4a7ecdaf5a","modified":1467174997000},{"_id":"themes/flexy/.git/refs/remotes/origin/HEAD","hash":"d9427cda09aba1cdde5c69c2b13c905bddb0bc51","modified":1467174997000},{"_id":"themes/flexy/.git/logs/refs/remotes/origin/HEAD","hash":"d17503afab9f4a71746278f351e59c4a7ecdaf5a","modified":1467174997000},{"_id":"themes/flexy/.git/objects/pack/pack-d2f60c962dc721cb113abb9eaeaff52de651c260.pack","hash":"771544de1cd4ca9592295042ebab27c732877b83","modified":1467174997000},{"_id":"themes/even/_config.yml","hash":"f64554c716a01bd4d23701cea99ad98597f89682","modified":1467175579000},{"_id":"themes/even/LICENSE","hash":"65da563ec8598aecdbf3f49968b85a117543ac54","modified":1467175579000},{"_id":"themes/even/README.md","hash":"d95c92dfcec2fc7348ada9ef3a9ed6e90d8665f3","modified":1467175579000},{"_id":"themes/even/.gitignore","hash":"002830f5f0f5555affef5a1e949f88e9ecab28b2","modified":1467175579000},{"_id":"themes/even/gulpfile.js","hash":"20747491ffcb79b530452d3e64bd50ca2d79fa3a","modified":1467175579000},{"_id":"themes/even/package.json","hash":"f8d631edc9acb1c4bdbbd5054d44391bb56c2145","modified":1467175579000},{"_id":"themes/even/.git/HEAD","hash":"acbaef275e46a7f14c1ef456fff2c8bbe8c84724","modified":1467175579000},{"_id":"themes/even/.git/description","hash":"9635f1b7e12c045212819dd934d809ef07efa2f4","modified":1467175579000},{"_id":"themes/even/.git/config","hash":"5ef1223485809abf438784735be8518631c5a612","modified":1467175579000},{"_id":"themes/even/.git/index","hash":"5951932c60e1fad5df470d7cf564a08004f554ce","modified":1467175579000},{"_id":"themes/even/.git/packed-refs","hash":"33bbee9d5ae7cbcb685175e8094af7220bb3a01a","modified":1467175579000},{"_id":"themes/even/languages/en.yml","hash":"0d745fc98735e1863b3386bff10ff7f5dab27453","modified":1467175579000},{"_id":"themes/even/doc/doc_zh.md","hash":"3bf01ffa30709d46262a2e0ca6ff42ca753a40df","modified":1467175579000},{"_id":"themes/even/languages/zh-cn.yml","hash":"869c33c149cb34a09c9d9f97e2f48e62cd1ff9ed","modified":1467175579000},{"_id":"themes/even/layout/about.jade","hash":"afcb22356b081fa93bb5f16486a05d2307e1025f","modified":1467175579000},{"_id":"themes/even/layout/index.jade","hash":"bb47055acff9f0f78e3da3b1b8597658523cc990","modified":1467175579000},{"_id":"themes/even/layout/archive.jade","hash":"491dc2d2dcedcbe87e8eafa37714e5b0cb6b2966","modified":1467175579000},{"_id":"themes/even/layout/post.jade","hash":"c3e62d46a8b05b9a3bb06ebce900b00dfff748c7","modified":1467175579000},{"_id":"themes/even/layout/tags.jade","hash":"0a28e85a89daaf5edf82c374402a8e15340c4cda","modified":1467175579000},{"_id":"themes/even/source/favicon.ico","hash":"4155a682010b1ca8b54916734ac868a1e28e5341","modified":1467175579000},{"_id":"themes/even/style/_global.scss","hash":"cc8d92b57340bf41cb0dce18436431585b752c63","modified":1467175579000},{"_id":"themes/even/style/_normalize.scss","hash":"87d5cd2a60780e1796dc27deeb5337a9d48e39a8","modified":1467175579000},{"_id":"themes/even/style/style.scss","hash":"665276e16712b890b56fcd12e8382cad538bb367","modified":1467175579000},{"_id":"themes/even/.git/hooks/applypatch-msg.sample","hash":"86b9655a9ebbde13ac8dd5795eb4d5b539edab0f","modified":1467175579000},{"_id":"themes/even/.git/hooks/pre-applypatch.sample","hash":"42fa41564917b44183a50c4d94bb03e1768ddad8","modified":1467175579000},{"_id":"themes/even/.git/hooks/pre-commit.sample","hash":"36aed8976dcc08b5076844f0ec645b18bc37758f","modified":1467175579000},{"_id":"themes/even/.git/hooks/commit-msg.sample","hash":"ee1ed5aad98a435f2020b6de35c173b75d9affac","modified":1467175579000},{"_id":"themes/even/.git/hooks/post-update.sample","hash":"b614c2f63da7dca9f1db2e7ade61ef30448fc96c","modified":1467175579000},{"_id":"themes/even/.git/hooks/prepare-commit-msg.sample","hash":"2b6275eda365cad50d167fe3a387c9bc9fedd54f","modified":1467175579000},{"_id":"themes/even/.git/hooks/update.sample","hash":"39355a075977d05708ef74e1b66d09a36e486df1","modified":1467175579000},{"_id":"themes/even/.git/hooks/pre-rebase.sample","hash":"5885a56ab4fca8075a05a562d005e922cde9853b","modified":1467175579000},{"_id":"themes/even/.git/hooks/pre-push.sample","hash":"5c8518bfd1d1d3d2c1a7194994c0a16d8a313a41","modified":1467175579000},{"_id":"themes/even/.git/info/exclude","hash":"c879df015d97615050afa7b9641e3352a1e701ac","modified":1467175579000},{"_id":"themes/even/.git/logs/HEAD","hash":"d350908680e6b5056979da36925cb81fecc23331","modified":1467175579000},{"_id":"themes/even/layout/mixins/comment.jade","hash":"5cbab04f2580a4c8663a69ec92567287a98d60e8","modified":1467175579000},{"_id":"themes/even/layout/mixins/analytics.jade","hash":"e4650d862d74e7e60cb6532b570fe1c59d2e6a94","modified":1467175579000},{"_id":"themes/even/layout/mixins/container.jade","hash":"b768e314ecac948696834eefb2c8812c3775afbe","modified":1467175579000},{"_id":"themes/even/layout/mixins/paginator.jade","hash":"cd0d402152eeb4a370205b6010ef8858206ab283","modified":1467175579000},{"_id":"themes/even/layout/mixins/tags.jade","hash":"f3a20ac226ba97bdd4be2c26a716e76b0ac2c3bf","modified":1467175579000},{"_id":"themes/even/layout/partial/footer.jade","hash":"0e66115ae887354790abed740b678ae72ef96492","modified":1467175579000},{"_id":"themes/even/layout/partial/head.jade","hash":"6c198d4e590e20fdc9ae27939cae208f5f2f6611","modified":1467175579000},{"_id":"themes/even/layout/partial/script.jade","hash":"97d96bd450d593b13e902e5427dafcda63ce789b","modified":1467175579000},{"_id":"themes/even/layout/partial/header.jade","hash":"4571928252e0226b5b013349782e8ed9bf9d51aa","modified":1467175579000},{"_id":"themes/even/layout/partial/layout.jade","hash":"bb483eba219dceb6170b542e1080a7cef9c167af","modified":1467175579000},{"_id":"themes/even/source/css/style.css","hash":"b3ae2762cf9f43784be7b0ee60434d38bb7af793","modified":1467175579000},{"_id":"themes/even/source/js/back2top.js","hash":"73bb560b8ef0b5fc41b520bae2837eabbf901e88","modified":1467175579000},{"_id":"themes/even/style/partial/_header.scss","hash":"0071f09a703ecf805378b8ef529cec74bc53bce1","modified":1467175579000},{"_id":"themes/even/style/partial/_code.scss","hash":"cf2b4db6ce24c5b5caa6cf1d6feb7b7eba025dff","modified":1467175579000},{"_id":"themes/even/style/partial/_archive.scss","hash":"dc699bcdf5256d411220d0417a0169d2dfa829e9","modified":1467175579000},{"_id":"themes/even/style/partial/_footer.scss","hash":"81b68e18a6bbfa95f7eb1688180ff10412177642","modified":1467175579000},{"_id":"themes/even/style/partial/_media.scss","hash":"924e1426af1334dbcccbd8eae2a8667e6de48064","modified":1467175579000},{"_id":"themes/even/style/partial/_home.scss","hash":"ef2e44695632aaa53b9ac599127297fe7bf602ca","modified":1467175579000},{"_id":"themes/even/style/partial/_paginator.scss","hash":"75e6bd9adf4cc361c04b49caa6856e65a443c840","modified":1467175579000},{"_id":"themes/even/style/partial/_post.scss","hash":"225dc290b50582fc41e35cb36ea7ff49658af0b7","modified":1467175579000},{"_id":"themes/even/style/partial/_tags.scss","hash":"e64ac66ac7ed2fcf3a9fc81236f0c24e74bc8867","modified":1467175579000},{"_id":"themes/even/.git/objects/pack/pack-06a78c998c8ebd63d31ab358e5e2fb0d8a2f2be5.idx","hash":"63bb46c94b9845031c3844ca5e35be84554c27d0","modified":1467175579000},{"_id":"themes/even/.git/refs/heads/master","hash":"6850514de6dff86cb498ef132dcab06010b9d1a7","modified":1467175579000},{"_id":"themes/even/source/fonts/iconfont/iconfont.ttf","hash":"da2d84481c667c83d1bc3a8052a217bf4a6006f0","modified":1467175579000},{"_id":"themes/even/source/fonts/iconfont/iconfont.css","hash":"83ac1a475d821fe39a6179af10a313f79160a2f4","modified":1467175579000},{"_id":"themes/even/source/fonts/iconfont/iconfont.svg","hash":"08e1725b7bad5646b212064cf51a5d0b34db2ebb","modified":1467175579000},{"_id":"themes/even/source/fonts/iconfont/iconfont.woff","hash":"ab912976ee2da4d7f05fc689da0273abbb379fb9","modified":1467175579000},{"_id":"themes/even/source/fonts/iconfont/iconfont.eot","hash":"7bb18035b46a904ac1d25c0db64ba874de956678","modified":1467175579000},{"_id":"themes/even/source/fonts/chancery/apple-chancery-webfont.ttf","hash":"5e25c531901d8a9e37ab45a7f4acdbe5324b51b6","modified":1467175579000},{"_id":"themes/even/source/fonts/chancery/apple-chancery-webfont.eot","hash":"fef78bd502f74fdbf0316123e176454cb3eb4e50","modified":1467175579000},{"_id":"themes/even/source/fonts/chancery/apple-chancery-webfont.woff2","hash":"afd0f74128f1c21c5a542b2e100870e74da663b6","modified":1467175579000},{"_id":"themes/even/source/fonts/chancery/apple-chancery-webfont.woff","hash":"95beafe485d4bdbddfecbcf3b2bc9b2d9cf5f5c5","modified":1467175579000},{"_id":"themes/even/source/fonts/chancery/apple-chancery-webfont.svg","hash":"a94e508f306a742637653f98c6e8827b11d3c142","modified":1467175579000},{"_id":"themes/even/.git/logs/refs/heads/master","hash":"d350908680e6b5056979da36925cb81fecc23331","modified":1467175579000},{"_id":"themes/even/.git/refs/remotes/origin/HEAD","hash":"d9427cda09aba1cdde5c69c2b13c905bddb0bc51","modified":1467175579000},{"_id":"themes/even/.git/logs/refs/remotes/origin/HEAD","hash":"d350908680e6b5056979da36925cb81fecc23331","modified":1467175579000},{"_id":"themes/even/.git/objects/pack/pack-06a78c998c8ebd63d31ab358e5e2fb0d8a2f2be5.pack","hash":"21fda5b1c514604497032f4d22a059e12418cf4d","modified":1467175579000},{"_id":"themes/apollo/README.md","hash":"a3e70662d82b7f7dcc76bd57e95389ba67dd52a4","modified":1467175916000},{"_id":"themes/apollo/.gitignore","hash":"a006beea0877a0aa3610ee00e73f62cb1d45125b","modified":1467175916000},{"_id":"themes/apollo/LICENSE","hash":"6e31ac9076bfc8f09ae47977419eee4edfb63e5b","modified":1467175916000},{"_id":"themes/apollo/_config.yml","hash":"bd0e5cd3449b0f4a1c6d2bdb1968b061522d4fae","modified":1467175916000},{"_id":"themes/apollo/gulpfile.js","hash":"857a026b6643a2cd52c65d4ae0dc7fe9618206ee","modified":1467175916000},{"_id":"themes/apollo/package.json","hash":"9426138c09ebb95969021d951590c0c54b187a43","modified":1467175916000},{"_id":"themes/apollo/.git/description","hash":"9635f1b7e12c045212819dd934d809ef07efa2f4","modified":1467175916000},{"_id":"themes/apollo/.git/HEAD","hash":"acbaef275e46a7f14c1ef456fff2c8bbe8c84724","modified":1467175916000},{"_id":"themes/apollo/.git/config","hash":"e8486e77527181934f2dda23e50b879ab7641244","modified":1467175916000},{"_id":"themes/apollo/.git/index","hash":"321b8a22aea9e528e0981de1a19899882d25c320","modified":1467175916000},{"_id":"themes/apollo/.git/packed-refs","hash":"913519bdbe4788abb394fc193315c12598d002d3","modified":1467175916000},{"_id":"themes/apollo/doc/doc-en.md","hash":"d6d9756b2085cdd8ee51eb5594427e2abf170e94","modified":1467175916000},{"_id":"themes/apollo/doc/doc-zh.md","hash":"d43c1f6bff66426744a2c6f031d19ff09cdeb2d1","modified":1467175916000},{"_id":"themes/apollo/languages/en.yml","hash":"40292f2a48e4e6361132033a47c516cb33127b6c","modified":1467175916000},{"_id":"themes/apollo/layout/index.jade","hash":"55f2f1b4b5364a0e09cb18e1112664c6415fb881","modified":1467175916000},{"_id":"themes/apollo/languages/zh-cn.yml","hash":"9e4b03e14c094000257ea254fd660dde4c7af63c","modified":1467175916000},{"_id":"themes/apollo/layout/archive.jade","hash":"62797414355bf4474092bc3a32726c8340820ffb","modified":1467175916000},{"_id":"themes/apollo/layout/post.jade","hash":"245c26244c075c3632d1545c3b228ee9d112f15d","modified":1467175916000},{"_id":"themes/apollo/source/favicon.png","hash":"a9cdcb22d1e74d5480323e19d1983de5a6873b8c","modified":1467175916000},{"_id":"themes/apollo/.git/hooks/applypatch-msg.sample","hash":"86b9655a9ebbde13ac8dd5795eb4d5b539edab0f","modified":1467175916000},{"_id":"themes/apollo/.git/hooks/commit-msg.sample","hash":"ee1ed5aad98a435f2020b6de35c173b75d9affac","modified":1467175916000},{"_id":"themes/apollo/.git/hooks/post-update.sample","hash":"b614c2f63da7dca9f1db2e7ade61ef30448fc96c","modified":1467175916000},{"_id":"themes/apollo/.git/hooks/pre-applypatch.sample","hash":"42fa41564917b44183a50c4d94bb03e1768ddad8","modified":1467175916000},{"_id":"themes/apollo/.git/hooks/pre-commit.sample","hash":"36aed8976dcc08b5076844f0ec645b18bc37758f","modified":1467175916000},{"_id":"themes/apollo/.git/hooks/pre-push.sample","hash":"5c8518bfd1d1d3d2c1a7194994c0a16d8a313a41","modified":1467175916000},{"_id":"themes/apollo/.git/hooks/pre-rebase.sample","hash":"5885a56ab4fca8075a05a562d005e922cde9853b","modified":1467175916000},{"_id":"themes/apollo/.git/hooks/prepare-commit-msg.sample","hash":"2b6275eda365cad50d167fe3a387c9bc9fedd54f","modified":1467175916000},{"_id":"themes/apollo/.git/hooks/update.sample","hash":"39355a075977d05708ef74e1b66d09a36e486df1","modified":1467175916000},{"_id":"themes/apollo/.git/info/exclude","hash":"c879df015d97615050afa7b9641e3352a1e701ac","modified":1467175916000},{"_id":"themes/apollo/.git/logs/HEAD","hash":"79eaf435dedf89fe6714c48aca8df99592ae54b6","modified":1467175916000},{"_id":"themes/apollo/layout/mixins/paginator.jade","hash":"f4ee2fb61a32e199b48cf93771749edc8a007391","modified":1467175916000},{"_id":"themes/apollo/layout/mixins/post.jade","hash":"cd9447e9b0ad22213e6a17d9a9b948f9f998f921","modified":1467175916000},{"_id":"themes/apollo/layout/partial/copyright.jade","hash":"1ac04b9dbcff6cc5c1ac8304d5ec86ffeed12183","modified":1467175916000},{"_id":"themes/apollo/layout/partial/head.jade","hash":"51b2ba6a1cebb275730eb7131eea211c91f0986a","modified":1467175916000},{"_id":"themes/apollo/layout/partial/comment.jade","hash":"ff0a2c269c2434da2ac5529872f1d6184a71f96d","modified":1467175916000},{"_id":"themes/apollo/layout/partial/nav.jade","hash":"c35d3061da4b053b73150d9741c542d660798270","modified":1467175916000},{"_id":"themes/apollo/layout/partial/layout.jade","hash":"eb15573ff66bdbb110ac77cf360b740170cee738","modified":1467175916000},{"_id":"themes/apollo/layout/partial/scripts.jade","hash":"cf3339de8979b5f75a3011f0f6b5451091b77dc8","modified":1467175916000},{"_id":"themes/apollo/source/css/apollo.css","hash":"56691cd1644fc210464187cf0780f020e27d5548","modified":1467175916000},{"_id":"themes/apollo/source/scss/apollo.scss","hash":"a3153dfedbaff08c20930b2bde95f2db49f6d518","modified":1467175916000},{"_id":"themes/apollo/.git/objects/pack/pack-73aa816875db6139c1f65abdc7a6e77ea74dcfb9.idx","hash":"061399d6e6635f99c5ce8324039193e0bb5d959c","modified":1467175916000},{"_id":"themes/apollo/.git/refs/heads/master","hash":"de8d7e2a5cf7324e25b7e08629fe6b745e73d723","modified":1467175916000},{"_id":"themes/apollo/source/scss/_partial/copyright.scss","hash":"1309667e3000037170cfbb5b8c9c65f4ffcf6814","modified":1467175916000},{"_id":"themes/apollo/source/scss/_partial/archive-post-list.scss","hash":"d2f740a7d48349b7536777c795f82ab740836d0f","modified":1467175916000},{"_id":"themes/apollo/source/scss/_partial/footer.scss","hash":"094aca6e52f11b139ac7980ca03fa7b9d8fc7b2f","modified":1467175916000},{"_id":"themes/apollo/source/scss/_partial/base.scss","hash":"f7e3e6f06bb81321673a546d51973a052c6d99b4","modified":1467175916000},{"_id":"themes/apollo/source/scss/_partial/mq.scss","hash":"b5eb0fb35fb275cbb6452b5d98702d461af3e6d5","modified":1467175916000},{"_id":"themes/apollo/source/scss/_partial/header.scss","hash":"153bde88bf8ffeae4ffd813d8cc694dd83d33d94","modified":1467175916000},{"_id":"themes/apollo/source/scss/_partial/normalize.scss","hash":"fd0b27bed6f103ea95b08f698ea663ff576dbcf1","modified":1467175916000},{"_id":"themes/apollo/source/scss/_partial/home-post-list.scss","hash":"6b5c59f3d2295944f934aee2c1156012a3306d5d","modified":1467175916000},{"_id":"themes/apollo/source/scss/_partial/post.scss","hash":"aec29b69af865249617624e1ee849792fd524cf9","modified":1467175916000},{"_id":"themes/apollo/.git/logs/refs/heads/master","hash":"79eaf435dedf89fe6714c48aca8df99592ae54b6","modified":1467175916000},{"_id":"themes/apollo/.git/refs/remotes/origin/HEAD","hash":"d9427cda09aba1cdde5c69c2b13c905bddb0bc51","modified":1467175916000},{"_id":"themes/apollo/.git/objects/pack/pack-73aa816875db6139c1f65abdc7a6e77ea74dcfb9.pack","hash":"13a17a78798c8f21404b2270a97e4f9ce5790253","modified":1467175916000},{"_id":"themes/apollo/.git/logs/refs/remotes/origin/HEAD","hash":"79eaf435dedf89fe6714c48aca8df99592ae54b6","modified":1467175916000}],"Category":[],"Data":[],"Page":[],"Post":[{"title":"Hello World","_content":"Welcome to [Hexo](https://hexo.io/)! This is your very first post. Check [documentation](https://hexo.io/docs/) for more info. If you get any problems when using Hexo, you can find the answer in [troubleshooting](https://hexo.io/docs/troubleshooting.html) or you can ask me on [GitHub](https://github.com/hexojs/hexo/issues).\n\n## Quick Start\n\n### Create a new post\n\n``` bash\n$ hexo new \"My New Post\"\n```\n\nMore info: [Writing](https://hexo.io/docs/writing.html)\n\n### Run server\n\n``` bash\n$ hexo server\n```\n\nMore info: [Server](https://hexo.io/docs/server.html)\n\n### Generate static files\n\n``` bash\n$ hexo generate\n```\n\nMore info: [Generating](https://hexo.io/docs/generating.html)\n\n### Deploy to remote sites\n\n``` bash\n$ hexo deploy\n```\n\nMore info: [Deployment](https://hexo.io/docs/deployment.html)\n","source":"_posts/hello-world.md","raw":"---\ntitle: Hello World\n---\nWelcome to [Hexo](https://hexo.io/)! This is your very first post. Check [documentation](https://hexo.io/docs/) for more info. If you get any problems when using Hexo, you can find the answer in [troubleshooting](https://hexo.io/docs/troubleshooting.html) or you can ask me on [GitHub](https://github.com/hexojs/hexo/issues).\n\n## Quick Start\n\n### Create a new post\n\n``` bash\n$ hexo new \"My New Post\"\n```\n\nMore info: [Writing](https://hexo.io/docs/writing.html)\n\n### Run server\n\n``` bash\n$ hexo server\n```\n\nMore info: [Server](https://hexo.io/docs/server.html)\n\n### Generate static files\n\n``` bash\n$ hexo generate\n```\n\nMore info: [Generating](https://hexo.io/docs/generating.html)\n\n### Deploy to remote sites\n\n``` bash\n$ hexo deploy\n```\n\nMore info: [Deployment](https://hexo.io/docs/deployment.html)\n","slug":"hello-world","published":1,"date":"2016-06-29T04:25:56.000Z","updated":"2016-06-29T04:25:56.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciq0dth2i00005yv7idzaet69","content":"<p>Welcome to <a href=\"https://hexo.io/\" target=\"_blank\" rel=\"external\">Hexo</a>! This is your very first post. Check <a href=\"https://hexo.io/docs/\" target=\"_blank\" rel=\"external\">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href=\"https://hexo.io/docs/troubleshooting.html\" target=\"_blank\" rel=\"external\">troubleshooting</a> or you can ask me on <a href=\"https://github.com/hexojs/hexo/issues\" target=\"_blank\" rel=\"external\">GitHub</a>.</p>\n<h2 id=\"Quick-Start\"><a href=\"#Quick-Start\" class=\"headerlink\" title=\"Quick Start\"></a>Quick Start</h2><h3 id=\"Create-a-new-post\"><a href=\"#Create-a-new-post\" class=\"headerlink\" title=\"Create a new post\"></a>Create a new post</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">$ hexo new <span class=\"string\">\"My New Post\"</span></div></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/writing.html\" target=\"_blank\" rel=\"external\">Writing</a></p>\n<h3 id=\"Run-server\"><a href=\"#Run-server\" class=\"headerlink\" title=\"Run server\"></a>Run server</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">$ hexo server</div></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/server.html\" target=\"_blank\" rel=\"external\">Server</a></p>\n<h3 id=\"Generate-static-files\"><a href=\"#Generate-static-files\" class=\"headerlink\" title=\"Generate static files\"></a>Generate static files</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">$ hexo generate</div></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/generating.html\" target=\"_blank\" rel=\"external\">Generating</a></p>\n<h3 id=\"Deploy-to-remote-sites\"><a href=\"#Deploy-to-remote-sites\" class=\"headerlink\" title=\"Deploy to remote sites\"></a>Deploy to remote sites</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">$ hexo deploy</div></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/deployment.html\" target=\"_blank\" rel=\"external\">Deployment</a></p>\n","excerpt":"","more":"<p>Welcome to <a href=\"https://hexo.io/\">Hexo</a>! This is your very first post. Check <a href=\"https://hexo.io/docs/\">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href=\"https://hexo.io/docs/troubleshooting.html\">troubleshooting</a> or you can ask me on <a href=\"https://github.com/hexojs/hexo/issues\">GitHub</a>.</p>\n<h2 id=\"Quick-Start\"><a href=\"#Quick-Start\" class=\"headerlink\" title=\"Quick Start\"></a>Quick Start</h2><h3 id=\"Create-a-new-post\"><a href=\"#Create-a-new-post\" class=\"headerlink\" title=\"Create a new post\"></a>Create a new post</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">$ hexo new <span class=\"string\">\"My New Post\"</span></div></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/writing.html\">Writing</a></p>\n<h3 id=\"Run-server\"><a href=\"#Run-server\" class=\"headerlink\" title=\"Run server\"></a>Run server</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">$ hexo server</div></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/server.html\">Server</a></p>\n<h3 id=\"Generate-static-files\"><a href=\"#Generate-static-files\" class=\"headerlink\" title=\"Generate static files\"></a>Generate static files</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">$ hexo generate</div></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/generating.html\">Generating</a></p>\n<h3 id=\"Deploy-to-remote-sites\"><a href=\"#Deploy-to-remote-sites\" class=\"headerlink\" title=\"Deploy to remote sites\"></a>Deploy to remote sites</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">$ hexo deploy</div></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/deployment.html\">Deployment</a></p>\n"},{"date":"2016-06-08T20:27:27.000Z","title":"NGS Workflows","_content":"\n*Next generation sequencing*. We all know that a fundamental practice in bioinformatics is the analysis of biological sequences. Similarities, functions, structures, associations, transcripts, proteins, RNA interference, regulation, interaction, DNA binding, the list goes on. Much can be hypothesized given some ATCGs (and some annotations). \n\nHowever, its **not plug and play**.  Various tools and algorithms exists for each step in NGS data pipelines. Each with their own advantages and disadvantages for a given set of data (e.g. bacterial vs. eukaryotic genomes). Their underlying algorithms can make assumptions which may not be true in all cases. New tools and methods are being developed and there are **rarely adopted standards**. Researchers today regularly construct hardcoded and unmaintanable scripts. I am not calling out these individuals on their coding practice, but rather positing that scripts without community maintained modular dependencies, with dependence on a specific environment configuration - let alone hardcoded absolute file references, are by their nature **unfit for providing reproducible NGS workflows to the community at large**. But hey, it [![works badge](https://cdn.rawgit.com/nikku/works-on-my-machine/v0.2.0/badge.svg)](https://github.com/nikku/works-on-my-machine) right?\n\nA well written **bash script** *can* be version controlled, and dependencies *can* be described, however the consumer *may* not be able to achieve an identical environment. At the very least, it will be a painful setup process.  Similarly, a **python script** is definitely more elegant and modular, but still suffers from issues such as pipeline reentrancy. One popular old tool is **make**, which can improve reentrancy by defining *rules* which have a *target* (output) and *input*s. However, the syntax is not newcomer friendly and file pattern matching can be confusing or limited. \n\nNot all hope is lost. There are have been many great efforts approaching this issue. One is **snakemake** which defines an elegant python-esque makefile with filename wildcarding, support for inline Python and R, and more. Another is **nextflow**, which goes a step further and describes pipelines through isolated (and containerizable) *process* blocks which communicate through channels. As well there are extras like galaxy, luigi, and bcbio.\n\nIn this blog post, I will define a simple variant calling pipeline. Then walk through the implementation of this pipeline using these four technologies:\n\n1. bash\n2. make\n3. snakemake\n4. nextflow\n\nThen discuss other alternatives in brief. Finally I will propose where Bionode and JavaScript can fit into this ecosystem, and which specific issues can be addressed.\n\n## Variant Calling Pipeline\n\nWe will be running a simple variant calling pipeline using a referenence genome and paired end genomic reads. For the sake of time when running the pipeline locally, we will use a small genome, *Salmonella enterica*, which has some paired end reads at about 100mb in size. With the reference genome at about 1.4mb, that provides about 70x coverage. \n\nTo follow along with this blog post, you will need to install: bionode-ncbi, sra-toolkit, bwa, samtools, bcftools, khmer, kmc, trimmomatic, snakemake, and nextflow.\n\nHa, I'm just kidding. Here is the Dockerfile: [polyglot-ngs-01](https://hub.docker.com/r/thejmazz/polyglot-ngs-01/).\n\n[bionode-ncbi][bionode-ncbi] will be used to download the reads from the NCBI SRA, or *sequence read archive*. The reference could be downloaded with `bionode-ncbi download assembly $specie` , but at the moment there is a [bug][bionode-ncbi-bug] where that downloads the `rna_from_genomic` rather than `genomic` file for some species.\n\nOnce we have the an `sra` for *Salmonella enterica*, the next step is to generate the two `fastq` files for the two ended reads. Paired ends have a 5' $\\rightarrow$ 3' set of reads and a 3' $\\rightarrow$ 5' set of reads. This allows for much more confident alignment and is generally preferred over a single set of reads. For this we can use `fastq-dump` from [sra tools][sra-tools]:\n\n```bash\nfastq-dump --split-files --skip-technical --gzip reads.sra\n```\n\nThis will produce `reads_1.fastq.gz` and `reads_2.fastq.gz`.\n\n> fastq is essentially a fasta file that also includes **quality** scores. Quality scores are produced by the **base calling** methods employed by the given NGS machine. See the wikipedia pages for the [fasta format][fasta format] and the [fastq format][fastq format].\n\nFiltering will be two step:\n\n1. trim reads adapters with [trimmomatic][trimmomatic]\n\n2. filter out bad [*k*-mers][k-mer] with \n\n   a) khmer\n\n   b) kmc\n\nThis allows us to illustrate how much the pipeline tools let us swap in and out different tools for the same step (in this instance, khmer vs kmc) and then compare results. \n\nThe filtering steps will complete by producing a `reads.filtered.fastq.gz` file. With other tools, it could have been a `reads_1.filtered.fastq.gz` and `reads_2.fastq.gz`.  It depends whether or not one of the filtering tools creates an *interleaved* file holding both read directions. It does not matter too much, as the next step can handle both cases. If we wanted to skip filtering, we could just use `reads_1.fastq.gz` and `reads_2.fastq.gz`.\n\nNow we want to align the reads to the reference using [bwa][bwa] which is a [Burrows-Wheeler transform][Burrows-Wheeler transform] based alignment tool. First, an [FM-index][FM-index] needs to be constructed for the reference genome:\n\n```bash\nbwa index reference.genomic.fna.gz\n```\n\nThen we align using the reads, producing a *sequence alignment map*:\n\n```bash\nbwa mem reference.genomic.fna.gz reads.filtered.fastq.gz > reads.sam\n```\n\nWe use [samtools][samtools] (also see [wiki/SAMtools][wiki/SAMtools]) to generate a *binary alignment map*:\n\n```bash\nsamtools view -bh reads.sam  > reads.unsorted.bam\n```\n\nand sort it:\n\n```bash\nsamtools sort reads.unsorted.bam -o reads.bam\n```\n\nWe could have piped the last three steps together, avoiding file writing/reading overhead:\n\n```bash\nbwa mem ref reads | samtools view -bh - | samtools sort - -o reads.bam\n```\n\nand then index it (creates `reads.bam.bai` - *binary alignment index*):\n\n```bash\nsamtools index reads.bam\n```\n\n\n\nAt this point, we have everything we need to call variants:\n\n```bash\nsamtools mpileup -uf reference.genomic.fna reads.bam | \\\nbcftools call -c - > reads.vcf\n```\n\n`mpileup` creates a BCF [pileup][pileup] file describing the base calls of aligned reads. Then `bcftools call` takes this and generates a [variant call format][vcf] file.\n\nSince `samtools` was complaining about the `reference.genomic.fna.gz` that comes from the NCBI Assemblies database - something to do with the compression format, I first decompressed it with:\n```bash\nbgzip -d reference.genomic.fna.gz\n```\n\n\nThats it! Thats how to get from SRA $\\rightarrow$ VCF using bionode, sra tools, trimming tools, filtering tools, bwa, samtools, and bcftools. The next sections will go over how to improve the reproducibility, reentrancy, ease of development, etc. of this workflow. \n\n## Topics\n\nFor each tool, we will inspect the following topics.\n\n### Basic Structure\n\n- quick intro to the tool, how it is written\n\n### Iterative Development\n\n- reentrancy\n- error debugging\n\n### Metrics\n\n- time for each task\n- resource usage\n- reports\n\n### Scaling\n\n- extend to many species\n- run on a cluster/cloud system\n- distribute/reproducibility\n\n## bash\n\nThe first obvious tool to use constructing a pipeline composed of a list of shell commands is a simple bash script. Essentially, we can take the commands from above and arrange them in a linear manner. You can see the full bash pipeline here: [basic-snp-calling.sh](https://github.com/bionode/gsoc16/blob/543ba66cbb42d1622089764bf01090e318307a57/pipelines/with-bash/basic-snp-calling.sh). \n\n### Basic Structure\n\nThe above section introduced the tools and their commands required to run a simple variant calling pipeline. However, all of the file names were hardcoded: `reference.genomic.fna.gz`, `reads.fastq.gz`, etc. In a real analysis, we would like to generalize so that we can provide paramaters defining the specie and reads to use. Then, for example, we can run the pipeline over many species and inspect related hypotheses. Within a bash script we can attempt to carry this out using *environment variables*. Depending on how these variables are defined, they may be accessible from different places. For example, the `PATH` variable (try `echo $PATH`) in your shell is available all the time. This is because it was defined using `export`. Without `export`, a variable is only available within in its current context. Consider:\n\n*main.sh*\n\n```bash\n#!/bin/bash\nREADS_ID=2492428 # no spaces around =!\n./downloadReads.sh\n```\n\n*downloadReads.sh*\n\n```bash\n#!/bin/bash\nbionode-ncbi download sra $READS_ID\n```\n\nIn this case, `READS_ID` will evaluate to blank. Alternatively you need to `export READS_ID=2492428` to have the variable available in other contexts. This is important to make note of, because complex shell scripts will likely be split across multiple files, in which case it is important to understand when and when not to use `export`. *Blindly using export everywhere is not recommended.*\n\nWith an understanding of environment variables out of the way, we can use them to define some initial settings for the script:\n\n```bash\n# Salmonella enterica\nREFERENCE_NAME='GCA_000006945.2_ASM694v2'\nREFERENCE=\"${REFERENCE_NAME}_genomic.fna.gz\"\nREFERENCE_URL=\"http://ftp.ncbi.nlm.nih.gov/genomes/all/$REFERENCE_NAME/$REFERENCE\"\nREADS='2492428'\nexport READSFQ='ERR1229296'\nFILTER_MODE='khmer' # 'kmc', 'none'\n```\n\n`READSFQ` is defined with `export` so it can be available from the filter scripts which have been separated out from the main file. Then files used as input and output can be described with these variables:\n\n```bash\necho \"START fastq-dump\"\nfastq-dump --split-files --skip-technical --gzip $READS/**.sra\necho \"END fastq-dump\"\n```\n\nI also notify the start and end of each task, *manually*. This is cumbersome and cluttering, and adding more useful features like time taken would be even more messy. Up until the filtering, the pipeline is completely linear. However, we would like to filter using two tools: `khmer` and `kmc`. One is significantly faster, but may produce more erroneous results. But that does not mean its useless, it can be useful to favour speed and number of species over specificity at the expense of less trials. Ideally, the swap from fast to slow should be a simple swap. As well, it can be useful to compare where exactly the two methods differ in their results. Enter the `if [ condtion ]; then  fi` block:\n\n```bash\nif [ $FILTER_MODE == 'kmc' ]; then\n  if [ ! -d $TMPDIR ]; then\n    mkdir -p $TMPDIR\n  fi\n  echo \"START filtering with kmc\"\n  export READS_TO_ALIGN=\"$READSFQ.trim.pe.kmc.fastq.gz\"\n  ./filter_kmc.sh\n  echo \"END filtering with kmc\"\nelif [ $FILTER_MODE == 'khmer' ]; then\n  echo \"Using khmer mode\"\n  export READS_TO_ALIGN=\"$READSFQ.trim.pe.khmer.fastq.gz\"\n  ./filter_khmer.sh\n  FINAL_OUTPUT=\"${READS}-khmer.vcf\"\nelse\n  echo \"No filter mode set. Continuing with reads with adapters trimmed.\"\n  READS_TO_ALIGN=\"$READSFQ.trim.pe.fastq.gz\"\nfi\n```\n\nAt this point, the pipeline branches into two modes. Currently, if we wanted to switch filter modes, we have to edit the definition of `FILTER_MODE` and restart the pipeline. If we are not checking for file existence before each command, this will be a big time waste. You can imagine how even deeper branching options can complicate it more. As well, it can be useful to change tool parameters and inspect the effects on final results. As an annoyance, changing these settings requires editing the file every time: this script would not scale to 100s of species well.\n\nAfter the filtering, the final section, alignment and calling, uses a variable set by the filtering option used to determine which reads to align with and the name of the final output. We can run the pipeline two times, wasting time, but at least the final results do not overwrite each other. \n\n### Iterative Development\n\nIterative development is the practice of developing a pipeline peice by peice. It is important because these pipelines can take a long time to run and simply running from scratch every time something needs to be changed is unnacceptable. In a bash script it can be handled by placing\n```bash\n#!/bin/bash\nset -e # exits script when an error is thrown\n\n...\n\nexit 1\n```\nthroughout the script. As well, I particulary enjoy [this][r-exec-atom] plugin, with it you can cmd+enter to send the current line or text selection to a terminal. However, there are still some remaining issues with this process. If you want to skip over previous steps, perhaps downloading reads or indexing, you will need to add explicit checks to your code. This clutters the code, and may still be error prone, as simply checking for a files existence might not be enough: it could be a 404 html document rather than a set of reads.\n\n### Metrics\n\nAny metrics you might want - time per task, cpu time per task, cpu and memory usage, would all have to be written manually. You could write a function to wrap commands and time them, etc, but this still clutters the code, obstructing the main message. Furthermore, \"in house\" metrics gathering is prone to error, especially when running on different machines. You may want to develop on a Mac and then deploy to a Linux server, where some commands may be different.\n\nAny sort of report on the pipeline performance, whether it be a text file or styled HTML document, would again require in house coding.\n\n### Scaling\n\nBash scripts are not scalable. If we were to port the script to handle multiple species, we *could use* bash arrays:\n\n```bash\nnums=(1 2 3 4 5); for i in $nums; do echo $i; done\n```\n\nThe above script might work for pipelines which only take one input parameter for a specie, but we also need the reference URL, SRA ID, etc. Going one step further, we can use array indices to gather related data. Yet the syntax is not the cleanest:\n\n```bash\nspecies=(Salmonella-enterica Staphylococcus-aureus)\nreadsID=(2492428 1274026)\n\nfor i in \"${!species[@]}\"; do \n    printf \"%s\\t%s\\t%s\\n\" $i ${species[$i]} ${readsID[$i]}\ndone\n```\n\nHowever, this would probably involve multiple code changes throughout the script. When deviating from the original purpose, bash scripts are likely to require relatively large codebase fixes. Changing species from a string to an array will break a lot of things.\n\nFurthermore, changing settings like maximum threads and memory usage will require manually editing the script when on the cluster, versus other tools which can provide this a command line option. Of course, *you could parse your own params*, but that is a lot of manual work.\n\nIn terms of reproducibility, there is not much. The script will likely assume all dependencies are installed. It may assume required binaries are in a specific place (in my script I used the `BIN` variable for this). It may also even fail on systems with different commands. It may also even assume being in a certain directory and directory structures of the system. \n\n## make\n\n`make` and `Makefile` are old, well known tools that are normally used for C/C++ compilation. The complete `Makefile` is [here](https://github.com/bionode/gsoc16/blob/543ba66cbb42d1622089764bf01090e318307a57/pipelines/with-make/Makefile). \n\n### Basic Structure\n\nWith `make`, the pipeline can be made a little more cleaner, particularly with respect to *input* and *output* from each command. A `makefile` is composed of rules:\n\n```make\nrule <target>: <prerequisites...>\n\t<command>\n```\n\nMake will run the rules that have targets for a given set of prerequisites. Consider:\n\n```bash\nrule all: genome\n\nrule chromosome_1:\n\techo ATCG > chromosome_1\n\nrule chromosome_2:\n\techo GATA > chromosome_2\n\nrule genome: chromosome_1 chromosome_2\n\tcat chromosome_1 chromsome_2 > genome\n```\n\nIf you trace the execution, it will first go into the first rule (which is named `all` by convention). The first rule states it requires `genome`. The third rule is capable of producing `genome` as that is its target, yet it requires two other prerequisites. So then rules that have `chromosome_*` in their target are used, these have no prereqs and can run right away. Then the program \"unwinds\" and everything works its way back to `rule all`. To learn more about `make` I highly recommend [this gist][make-gist] This technique, where pipelines are defined from the end-to-forwards, we will refer to as the **pull** method.\n\nWe can handle branching using set variables to enforce ignoring some rules:\n\n```bash\n# Toggle these variable declarations to switch between\n\n# 1. no trimming/filtering\n# PRECONDITION_TO_USE=fastq-dump.log\n# READS_1=reads_1.fastq.gz\n# READS_2=reads_2.fastq.gz\n\n# 2. kmc\n# PRECONDITION_TO_USE=reads.trim.pe.kmc.fastq.gz\n# READS_1=reads.trim.pe.kmc.fastq.gz\n\n# 3. khmer\nPRECONDITION_TO_USE=reads.trim.pe.khmer.fastq.gz\nREADS_1=reads.trim.pe.khmer.fastq.gz\n```\n\nAnd the corresponding rule which will invoke a given path in the branch:\n\n```bash\nreads.sam: bwa-index.log ${PRECONDITION_TO_USE}\n\t@echo \"=== Running $@ ===\"\n\t@export start=`date +%s`; \\\n\tbwa mem -t ${THREADS} reference.genomic.fna.gz ${READS_1} ${READS_2} > $@; \\\n\texport end=`date +%s`; \\\n\texport runtime=$$(($$end-$$start)); \\\n\techo \"Target $@ took $$runtime seconds\"\n```\n\n### Iterative Development\n\nIterative development in `make` is much improved over a `bash` script. This is because you can call a specific rule at invocation: `make reads.sam` for example. As well, it is common practice to include a clean rule. It is possible in `make` to define targets that don't actually create the target file. Since the target is not actually created (even though the command in that rule has ran and worked as expected), since make skips rules if the target already exists, it will unnecessarily rerun the rule. My general workflow writing a `make` pipeline was to update the prequisite of `rule all` as new rules were added, and each task from before would be skipped. One way to get around this issue is to create \"flag files\":\n\n```bash\ntrim.happened: fastq-dump.log\n\t@echo \"=== Running $@ ===\"\n\t@export start=`date +%s`; \\\n\tjava -jar ${TRIMMOMATIC} PE -phred33 \\\n\t  reads_1.fastq.gz reads_2.fastq.gz \\\n\t  reads_1.trim.pe.fastq.gz reads_1.trim.se.fastq.gz \\\n\t  reads_2.trim.pe.fastq.gz reads_2.trim.se.fastq.gz \\\n\t  ILLUMINACLIP:${ADAPTERS}/TruSeq3-PE.fa:2:30:10 \\\n\t  LEADING:3 TRAILING:3 SLIDINGWINDOW:4:20 MINLEN:36 > $@; \\\n\texport end=`date +%s`; \\\n\texport runtime=$$(($$end-$$start)); \\\n\tcat $@; \\\n\techo \"Target $@ took $$runtime seconds\"\n```\n\nHere, the stdout of trimmomatic is sent to `trim.happened`. Even if there is no stdout, an empty file will get made, and then the next run this rule can be skipped.\n\n### Metrics\n\nTo get time spent metrics, I wrapped each command in a simple delta time calculation:\n\n```bash\ntime:\n\t@echo \"=== Running $@ ===\"\n\t@export start=`date +%s`; \\\n\tsleep 2; \\\n\texport end=`date +%s`; \\\n\texport runtime=$$(($$end-$$start)); \\\n\techo \"Target $@ took $$runtime seconds\"\n```\n\nThis clutters the code fast, and is unmaintainable. It might be possible to do it cleaner, but I did not spend time trying obscure solutions. The point is that there is no \"out of the box\" way to get task metrics with `make`. You can imagine how tricky it might be to also get RAM/CPU usage for each task.\n\n### Scaling\n\nScaling with make is also difficult. There is no easy way to expand the pipeline to multiple species, and it is difficulty to \"variabilize\" outputs: targets need to strings.\n\n## snakemake\n\nThe `Makefile` was much more organized than the bash script, but still of \"a low level\".\n\nSnakemake was a refreshing take on the make style, but with many more features and powered by a high level language. Including Python integration everywhere, tasks to script in Python, Bash, and R, and some metrics out of the box. As well as some neat wildcarding. The complete Snakefile is [here](https://github.com/bionode/gsoc16/blob/543ba66cbb42d1622089764bf01090e318307a57/pipelines/with-snakemake/Snakefile).\n\n### Basic Structure\n\nYou start a Snakemake workflow very similar to a Makefile, defining a global rule and what you want it to create:\n\n```python\nrule all:\n    input: FINAL_FILES\n```\n\nSo what is `FINAL_FILES`? Since Snakemake has direct Python integration, we can actually compute it as a local variable. Heres the header for the `Snakefile` just before\n\n`rule all`:\n\n```python\nspecies = {\n    'Salmonella-enterica': {\n        'readsID': '2492428',\n        'reference_url': 'http://ftp.ncbi.nlm.nih.gov/genomes/all/GCA_000988525.2_ASM98852v2/GCA_000988525.2_ASM98852v2_genomic.fna.gz'\n    }\n}\n\nTHREADS=2\nTEMP='./tmp'\n\nFINAL_FILES = [specie+'.vcf' for specie in species]\n```\n\nThis will end up with `FINAL_FILES` being `['Salmonella-enterica.vcf']`.  See this rule which will be one of the first in the executed pipeline (here I hardcoded the specie name to better illustrate):\n\n```python\nrule all:\n    input: 'Salmonella-enterica.vcf'\n\nrule download_sra:\n    output: '{specie}.sra'\n    run:\n        readsID = species[wildcards.specie]['readsID']\n        shell('''\n            bionode-ncbi download sra {readsID};\n            cp {readsID}/*.sra {output} && rm -rf {readsID};\n        ''')       \n        \nrule call:\n    input: '{specie}.sra'\n    output: '{specie}.vcf'\n    shell: 'magic {input} > {output}'\n```\n\nThe first rule will be triggered and will be looking for another rule that has `Salmonella-enterica.vcf` in its `output`. It won't find *exactly* that, but because of Snakemake's wildcarding, `{specie}.vcf` will do (from `rule call`), and then within the `call` rule, the value of `wildcards.specie` will be `Salmonella-enterica`. Then it will move onto `download_sra`. This example also illustrates how you can use `run` and `shell()` to mix Python and shell code. You can also use `script` to run a Python or R script, and use the `R()` function to execute R code in a rule.\n\nAnother neat feature with Snakemake is the ability to drop in \"wrappers\":\n\n```python\nrule bwa_mem:\n    input:\n        ref = '{specie}.genomic.fna.gz',\n        sample = ['{specie}_'+num+'.fastq.gz' for num in ['1', '2']],\n        index_files = ['{specie}.genomic.fna.gz.'+suffix for suffix in ['amb', 'ann', 'bwt', 'pac', 'sa']]\n    log: 'logs/bwa_mem/{specie}.log'\n    output: '{specie}.sam'\n    threads: THREADS\n    wrapper: '0.0.8/bio/bwa_mem'\n```\n\nThese wrappers come from the [wrappers repository][wrappers-repository]. They run predefined commands using specific input variables. While this is great, there is some overhead in having to check the source to see what is actually happening, and by extension, you then need an internet connection to do so. There does not seem to be a huge list of wrappers, it is too bad it does not seem to have caught on extensively.\n\nStill have the problem of using log/flag files for task dependency, but less so, since you can define \"custom\" outputs, also has touch(flag) built in\n\nCould not figure out how to ecomical branching, wildcard regexes create ambiguity:\n\n![attempt-branching](https://raw.githubusercontent.com/bionode/gsoc16/fdf22b630e33dd11302ea8822c547ef9399c3ea4/pipelines/with-snakemake/dag.png)\n\nHere it unnecessarily redoes certain steps. This is because `Salmonella-enterica.trim.vcf` and `Salmonella-enterica.trim.vcf` both match `{specie}.vcf`. At first, Snakemake complained about ambiguous rules due to this conflict. I was able to generate the DAG above by using custom regexes for the wildcard in a few rules: `{specie,[a-zA-z-]+}`. However, since this then creates one set of rules with `wildcard.specie = Salmonella-enterica` and another with `wildcard.specie = Salmonella-enterica.trim`, and then that wildcard moves all the way to the first rule, and basically two pipelines are ran. I was not able to figure out how optimize this using Snakemake - if anyone can, let me know!\n\nMoreover - because the **pull** workflow style depends on *targets* and *prerequisites*, it can be difficult to achieve a branch-merge pipeline. We will see with Nextflow, which follows the dataflow paradigm, or **push**, it is natural to describe such pipelines.\n\nFor a more complicated (and real world) snakemake setup, take a look at [pachterlab/kallisto_paper_analysis/Snakefile](https://github.com/pachterlab/kallisto_paper_analysis/blob/nbt/Snakefile).\n\n### Iterative Development\n\nIterative development in Snakemake is about the same as with `make`. With wildcarding, the clean rule can be made more specific, for example:\n\n```python\nrule clean:\n    input: '{specie}.vcf'\n    shell: '''\n    \trm {specie}.sra {specie}.bam;\n    '''\n```\n\nAs opposed to `rm *.sra *.bam`. \n\n### Metrics\n\nYou can opt in to store rule benchmarks and logs:\n\n```python\nrule call:\n    input:\n        bam = '{specie}.bam',\n        alignment_index = '{specie}.bam.bai',\n        reference = '{specie}.genomic.fna'\n    output: '{specie}.vcf'\n    log: 'logs/call/{specie}.log'\n    benchmark: 'benchmarks/call/{specie}.txt'\n    shell: 'samtools mpileup -uf {input.reference} {input.bam} | bcftools call -c - > {output}'\n```\n\nHowever, you will need to make sure you name these files appropiately to avoid overwriting. An example benchmark file looks like:\n\n```\ns\th:m:s\n30.32477617263794\t0:00:30.324776\n```\n\nThere is no indication of which task the benchmark is for, meaning if you want a comprehensive report of your pipeline you will need to write a script that concatenates the benchmarks in the correct order.\n\nWith graphviz installed, snakemake can create a plot of the DAG for your pipeline:\n\n```bash\nsnakemake --dag | dot -Tpng > dag.png\n```\n\nYou can see an example of this above.\n\n### Scaling\n\nWith Snakemake it is easy to scale to multiple species. In my Snakefile, I simply just add more keys to the `species` dictionary. Since each output file is prefixed by the species name, there will be no file overlaps. You need to be vigilant with output names and ensure yourself there will be no conflicts.\n\nYou can specify clustering configuration by writing a `cluster.config` file and using its values in the call to Snakemake. The example [from the documentation](https://bitbucket.org/snakemake/snakemake/wiki/Documentation#markdown-header-cluster-configuration):\n\n*Snakefile*\n\n```python\nrule all:\n    input: \"input1.txt\", \"input2.txt\"\n\nrule compute1:\n    output: \"input1.txt\"\n    shell: \"touch input1.txt\"\n\nrule compute2:\n    output: \"input2.txt\"\n    shell: \"touch input2.txt\"\n```\n\n*cluster.json*\n\n```json\n{\n    \"__default__\" :\n    {\n        \"account\" : \"my account\",\n        \"time\" : \"00:15:00\",\n        \"n\" : 1,\n        \"partition\" : \"core\"\n    },\n    \"compute1\" :\n    {\n        \"time\" : \"00:20:00\"\n    }\n}\n```\n\nAnd the call to Snakemake:\n\n```\nsnakemake -j 999 --cluster-config cluster.json --cluster \"sbatch -A {cluster.account} -p {cluster.partition} -n {cluster.n}  -t {cluster.time}\"\n```\n\nWhile nextflow is providing a simple config based way to pass params into your clustering command, the actual command: `sbatch` is written manually. We will say with Nextflow, even this aspect of clustering is abstracted away (into \"executor\"). See also [job properties](https://bitbucket.org/snakemake/snakemake/wiki/Documentation#markdown-header-job-properties) to see how to write a generic job wrapper for clustering.\n\n## nextflow\n\nNextflow is a more recent tool, and approaches the workflow problem in a **push** sense. Similar to how Snakemake allows direct Python integration, Nextflow uses [Groovy](http://www.groovy-lang.org/). While Snakemake let you write rules with inline Python, R, and shell, in Nextflow, you *can use any scripting language*. See [main.nf](https://github.com/bionode/gsoc16/blob/543ba66cbb42d1622089764bf01090e318307a57/pipelines/with-nextflow/main.nf) for the complete pipeline.\n\nNextflow follows the [dataflow programming paradigm](https://en.wikipedia.org/wiki/Dataflow_programming) (push) which is also called [stream processing](https://en.wikipedia.org/wiki/Stream_processing) or [reactive programming](https://en.wikipedia.org/wiki/Reactive_programming). While in Snakemake you define `rule all`'s prerequisites, and then \"work backwards\" from there (pull), in the dataflow model you can think and write in the order that tasks will happen. Since the push model does not depend on inferring a complete dependency tree at the initialization, you can introduce dynamic structures into the pipeline. For example, choosing between two tasks to run *based on the output* of a previous task. One fallback with the push model is that since it can be dynamic, it is difficult to then perform a \"dry run\", or generate a DAG ahead of time.\n\n### Basic Structure\n\nNextflow uses **processes** as rules, and each process will occur in its own folder in `/work`. Since each process takes place in its own folder, using output from one task as input in another cannot be done the \"conventional way\". But that is alright, because Nextflow provides **channels** to communicate between processes. \n\nConsider:\n\n```groovy\n#!/usr/bin/env nextflow\n\nspecies = [\n  'Salmonella-enterica': [\n    'referenceURL': 'http://ftp.ncbi.nlm.nih.gov/genomes/all/GCA_000988525.2_ASM98852v2/GCA_000988525.2_ASM98852v2_genomic.fna.gz',\n    'readsID': '2492428'\n  ]\n]\n\nprocess downloadSRA {\n  container 'bionode/bionode-ncbi'\n\n  input: val readsID from species.collect { it.value.readsID }\n  output: file '**/*.sra' into reads\n\n  \"\"\"\n  bionode-ncbi download sra $readsID > tmp\n  \"\"\"\n}\n\nprocess extractSRA {\n  container 'inutano/sra-toolkit'\n\n  input: file read from reads\n  output: file '*.fastq.gz' into samples\n\n  \"\"\"\n  fastq-dump --split-files --skip-technical --gzip $read\n  \"\"\"\n}\n```\n\nSee the documentation on [processes] for more. As well, there are more types of channels than `val` and `file`, see [channels]. If you are new to [Groovy], and `species.collect { it.value.readsID }` confuses you, think of it as `species.map(specie => specie.value.readsID)`.  It is important to note that while channels may appear confusing at first (why not just use filenames?), they are an elegant solution to a problem we had with Snakemake: file name overlap. With Nextflow, you can be guarenteed no files will ever overwrite each other (unless you do so yourself in one process). As well, this abstracts away the filename, making commands appear generalized:\n\n```groovy\nprocess indexReference {\n  container 'biodckr/bwa'\n\n  input: file reference from referenceGenomeGz2\n  output: file '*.gz.*' into referenceIndexes\n\n  \"\"\"\n  bwa index $reference\n  \"\"\"\n}\n```\n\nThere is no mental overhead from managing `{specie}.genomic.fna.gz`, instead we can simply use the variable `$reference`. This makes the commands in our processes easy to read and comprehend. We can even \"hardcode\" output file names in an *extremely general way* without any fear of file overlap:\n\n```groovy\nprocess decompressReference {\n  container 'biodckrdev/htslib'\n\n  input: file referenceGenome from referenceGenomeGz1\n  output: file 'reference.genomic.fna' into referenceGenomes\n\n  \"\"\"\n  bgzip -d $referenceGenome --stdout > reference.genomic.fna\n  \"\"\"\n}\n```\n\nHere we output to `reference.genomic.fna`. A filename that would surely cause problems in any other worflow system.\n\nSomething else to discuss with Nextflow is how to handle a channel being consumed by multiple processes. Since a channel is a FIFO queue in Nextflow, once one process uses it, it is \"emptied\" and cannot be consumed by another process. The way to get around this is to create the original channel called `myOutput` and then \"clone\" it via `into` $n$ channels to be consumed by $n$ processes:\n\n```groovy\nprocess downloadReference {\n  container true\n\n  input: val referenceURL from species.collect { it.value.referenceURL }\n  output: file 'reference.genomic.fna.gz' into referenceGenomeGz\n\n  \"\"\"\n  appropriate/curl $referenceURL -o reference.genomic.fna.gz\n  \"\"\"\n}\n\n// fork reference genome into three other channels\n( referenceGenomeGz1,\n  referenceGenomeGz2,\n  referenceGenomeGz3 ) = referenceGenomeGz.into(3)\n```\n\nThis is also an example of an *executable container*, where the Dockerfile ends in `CMD [\"curl\"]`. \n\nHowever, I encountered some issues when trying to dockerize everything. TODO talk about dockerizing processes in a pipe.\n\nFinally, wherease Snakemake only allowed Python and R inside rules, with Nextflow you can use any scripting language.\n\n### Iterative Development\n\nIterative developmen with Nextflow is much improved over the other tools. I felt as though I could extend the pipeline, adding new processes, without having to worry about updating a `rule all` rule, or cleaning up old files. Nextflow seamlessly recognizes cached process' results when given the `-resume` flag.\n\nDebugging errors in processes works great. When an error occurs, you can `cd` into the relevant `/work` directory, and Nextflow provides log files, and a file that describes which command exactly was ran. This allows you to debug the error in the same environment as it will be ran, and then resume the workflow from that point.\n\n### Metrics\n\nNextflow can create timeline charts:\n\n![nextflow-timeline](https://github.com/bionode/gsoc16/raw/543ba66cbb42d1622089764bf01090e318307a57/pipelines/with-nextflow/timeline.png)\n\nand a DAG diagram of the pipeline (also notice I was able to create a \"forking\" pipeline unlike with Snakemake):\n\n![nextflow-workflow](https://github.com/bionode/gsoc16/raw/543ba66cbb42d1622089764bf01090e318307a57/pipelines/with-nextflow/workflow.png)\n\n### Scaling\n\nYou might have noticed the `container` [directive][nextflow-directive] in the above processes. Nextflow comes with built-in docker integration, which is great. *Each process can run in its own container*. This helps improve the portability and reproducibiltiy of the workflow. All a consumer needs installed on their system is Docker and Nextflow (and you can even run Nextflow in Docker). Each container can use a tagged version, ensuring when someone else runs the pipeline, they are using the *exact same version of each tool*. You could, of course use Docker in your Snakemake commands, but there would be overhead from volume mounting (mapping a local folder to a folder inside the container); Nextflow handles all that for you. As well, Nextflow has built in support for many cluster engines, which can be enabled by defining the `executor` in `nextflow.config`. Another feature to not is Nextflow's integration with GitHub. With a `main.nf` in your repo, you can run a pipeline with `nextflow run username/repo`. \n\nFinally, be sure to check out [awesome-nextflow](https://github.com/nextflow-io/awesome-nextflow)!\n\n## others\n\n[awesome-pipeline](https://github.com/pditommaso/awesome-pipeline) is a great list of many pipeline and workflow tools. Here are some I think are more relevant, and a few extras (though you should try to look through all of them, at least look at these):\n\n- [anduril](http://anduril.org/userguide/) - Component-based workflow framework for scientific data analysis\n- [antha](https://www.antha-lang.org/docs/concepts/flow-based-programming.html) - High-level language for biology\n\n\n- [bpipe](http://docs.bpipe.org/Examples/PairedEndAlignment/) - Tool for running and managing bioinformatics pipelines\n- [Conan2](https://github.com/tburdett/Conan2) - Light-weight workflow management application\n- [cosmos](https://github.com/LPM-HMS/COSMOS2) - Python library for massively parallel workflows\n- [drake](https://github.com/Factual/drake) - Robust DSL akin to Make, implemented in Clojure\n- [kronos](http://kronos.readthedocs.io/en/latest/) - Workflow assembler for cancer genome analytics and informatics\n- [loom](http://kronos.readthedocs.io/en/latest/) - Tool for running bioinformatics workflows locally or in the cloud\n- [moa](http://moa.readthedocs.io/en/latest/) - Lightweight workflows in bioinformatics\n- [OpenGE](https://github.com/adaptivegenome/openge) - Accelerated framework for manipulating and interpreting high-throughput sequencing data\n- [pipengine](https://github.com/fstrozzi/bioruby-pipengine#-the-pipeline-yaml-) - Ruby based launcher for complex biological pipelines\n- [ruffus](http://www.ruffus.org.uk/) - Computation Pipeline library for Python\n- [YAP](http://opensource.nibr.com/yap/) - Extensible parallel framework, written in Python using OpenMPI libraries\n\n\n- [clusterflow](http://clusterflow.io/examples/) - Command-line tool which uses common cluster managers to run bioinformatics pipelines\n\n\n- [hive](https://github.com/Ensembl/ensembl-hive) - System for creating and running pipelines on a distributed compute resource\n- [google cloud dataflow](https://cloud.google.com/dataflow/docs/) - unified programming model and amanaged servicefor developing and executing a wide range of data processing patterns including ETL, batch computation, and continuous computation\n- [makeflow](http://ccl.cse.nd.edu/software/makeflow/) - Workflow engine for executing large complex workflows on clusters\n- [scoop](https://github.com/soravux/scoop/) - Scalable Concurrent Operations in Python\n- [luigi](https://github.com/spotify/luigi) and [sciluigi](https://github.com/pharmbio/sciluigi) - Python module that helps you build complex pipelines of batch jobs, luigi wrapper for scientific workflows\n\n\n- [galaxy](https://usegalaxy.org/) - Web-based platform for biomedical research\n- [taverna](https://taverna.incubator.apache.org/introduction/) - Domain independent workflow system\n\n\n\n- [rabix](https://github.com/rabix/rabix) - implementation of CWL2\n\n\n- [cuneiform](https://github.com/joergen7/cuneiform) - Advanced functional workflow language and framework, implemented in Erlang\n- [mario](http://ccl.cse.nd.edu/software/makeflow/) - Scala library for defining data pipelines\n\n\n\n- [airflow](https://github.com/apache/incubator-airflow) - Python-based workflow system created by AirBnb\n- [pinball](https://github.com/pinterest/pinball) - Python based workflow engine by Pinterest\n\n\n\n- [AWS Data Pipeline](https://aws.amazon.com/datapipeline/) and [SWF](https://aws.amazon.com/swf/) - data workflow orchestration\n- [DRAY: Docker Workflow Engine](http://dray.it/) - UNIX pipes for Docker\n- [toil](https://github.com/BD2KGenomics/toil) - CWL3 and WDL support\n- [bcbio](https://github.com/chapmanb/bcbio-nextgen) - best-practice pipelines for automated analysis of high throughput sequencing data\n- [example gulpfile simple pipeline](https://github.com/pachterlab/kallisto/blob/master/gulpfile.js)\n- [scipipe](https://github.com/scipipe/scipipe) - workflow system in Go inspired by Flow-based programming\n- [node-datapumps](https://github.com/agmen-hu/node-datapumps) - Node.js ETL toolkit for easy data import, export or transfer between systems\n\nQuite a lot. \n\n## Conclusion\n\nOn a scale from 1-5, these are my ratings for each tool. Mostly as relative to each other, rather than absolutely.\n\n| Tool      | Structure | Iterative Dev. | Metrics | Scale | Reproducibility |\n| --------- | --------- | -------------- | ------- | ----- | --------------- |\n| bash      | 1         | 1              | 1       | 1     | 1               |\n| make      | 2         | 2              | 1       | 1     | 1               |\n| Snakemake | 4         | 3              | 3       | 3     | 4               |\n| Nextflow  | 4         | 5              | 5       | 5     | 5               |\n\nI have not investigated the \"others\" from above to warrant their ranking.\n\nA quick summary:\n\n**bash**\n\n- \"generic\" scripting\n- simple variable interpolation - `$foo`\n- no metrics, difficult to scale\n- no reentrancy, poor iterative development\n\n**make**\n\n- structure more pipeline oriented than bash - tasks with *input* (prerequisites) and *output* (target)\n- introduces some element of reentrancy by skipping rules whose target already exists = improved iterative development over bash\n- can \"fork\" by defining variables that change a rule's prerequisites, and running again\n- no metrics, difficult to scale\n\n**Snakemake**\n\n- enhanced scripting over bash/make with Python while using make's well known rules structure\n- improved `{input}`, `{output}` syntax and power (e.g. `{input.a}` or `{input[n]}`) over make\n- run Python, shell, or R in rule\n- enhanced wildcarding: easier to understand, more powerful than make's `%`\n- benchmarks are OK - need to do manual work to get a full pipeline report\n- DAG is nice\n- dry run is nice (and possible because of \"pull\" paradigm)\n- can scale to multiple species easily, just need to make sure file names do not overlap\n- cannot create forking paths (e.g. use same reference for two different filtering tools). Could do it by changing `FINAL_OUTPUT` and running again, but then thats basically the same as it was in `make`. (again, if you know how to do this without wildcarding ambiguity - let me know!)\n\n**Nextflow**\n\n- dataflow paradigm - \"push\"\n- as a consequence of push, no dry run\n- as a consequence of push, flexible forking and dynamic pipelines\n- scale to multiple species effortlessly with no overlapping file name concerns\n- timeline, DAG, benchmarks metrics are all very nice\n- Docker is nice - improves reproducibility and shareability\n- cannot handle large `stdout | stdin` pipes across channels (e.g. one process for `bwa mem` and another for `samtools view`)\n\n**bionode-waterwheel** *proposal*\n\n- no DSL to learn - just JavaScript. Functional, built around async, events, and streams.\n\n- streams resonate well with *push* paradigm\n\n- support for piping `stdin`, `stdout` around, e.g. `bwa.mem().pipe(samtools.view())` . This enhances modularity of pipeline, rather than having one rule or process doing `bwa mem | samtools view`, which is really two commands, each deserving their own input, output, parameter definitions. This can aid in improving reproducibility and modularity, as tools with defined input, output, params could be confidentally dropped into the pipeline.\n\n- moduler interoperation with web apps, native apps (Electron), services (Slack, email). Imagine pipeline logs being sent over a websocket to your browser.\n\n- interoperation with npm ecosystem\n\n- integrate with CWL spec\n\n- metrics will be easier to consume since output will be JSON to be consumed by d3, browser apps, etc\n\n- modular, specific, customizable waterwheel \"backend\" can be integrated into browser or native apps, bringing more power to pipeline GUIs than Galaxy for instance, where it is complicated to develop a custom pipeline (some labs hire someone to create a custom xml config for their specific pipeline so that the wet lab can \"click to play\" in Galaxy)\n\n- variable interpolation - like ES6 template literals, which can take any valid JS expression:\n\n  ```javascript\n  const PORT = config.PORT\n  const template = `Server listening on port ${PORT}`\n  ```\n\n  which can actually evaluate any JS expression:\n\n  ```javascript\n  [1, 2, 3, 4].map(num => `is even: ${num % 2 === 0 ? 'true' : 'false'}`)\n  ```\n\n\n\n- simple examples with small datasets can be browser compatibile (see: [nbind](https://github.com/charto/nbind)). Live browser examples are great for education - \"run your own NGS pipeline, from the browser\"\n- how to handle interative development is an open question. Can fork all streams into files for reentrancy while in a \"develop\" mode. \n\n\n\nA draft pipeline with **bionode-waterwheel**:\n\n```javascript\nconst ncbi = require('bionode-ncbi')\nconst wrapper = require('bionode-wrapper')\nconst waterwheel = require('bionode-waterwheel')\n\nconst { task, join, run } = waterwheel\nconst { stdout, stdin, file, directory } = waterwheel.types\n\n// Can be passed in from CLI\n// params for final pipeline call. params are things that do not\n// change how items are passed between processes, but decide output for the\n// pipeline as a whole. For example, species name and reads accession.\nconst pipelineParams = {\n  specie: 'Salmonella-enterica',\n  readsID: '2492428',\n  output: 'Salmonella-enterica.vcf'\n}\n\nconst sra = task({\n  output: stdout()\n}, ncbi.download('sra', '{params.readsID}'))\n\nconst reference = task({\n  output: stdout()\n}, ncbi.download('assembly', '{params.specie}'))\nconst bwa_index = task({\n  input: file(),\n  output: file()\n}, wrapper('bwa index {input}'))\n\n\n// some tools you cannot decide output file\n// so, tell waterwheel to stream a file as stdout\n// there is --stdout for fastq-dump to give a streamed fastq, but trimmomatic\n// wants reads_1 and reads_2\n// tools that are not bionode will need to wrapper()ed\nconst extract = task({\n  input: file('reads.sra'),\n  output: file([1, 2].map(n => `reads_${n}.fastq.gz`))\n}, wrapper('fastq-dump --split-files --skip-technical --gzip {input}'))\n\nconst trim = task({\n  input: file([1, 2].map(n => `reads_${n}.fastq.gz`)),\n  output: {\n    pe: file([1, 2].map(n => `reads_${n}.trim.pe.fastq.gz`),\n    se: file([1, 2].map(n => `reads_${n}.trim.se.fastq.gz`)\n  },\n  opts: {\n    adapters: '../adapters'\n  }\n}, wrapper('''\n  trimmomatic PE -phred33 \\\n  {input[0]} {input[1]} \\\n  {output.pe[0]} {output.se[0]} \\\n  {output.pe[1]} {output.se[1]} \\\n  ILLUMINACLIP:{opts.adapters}/TruSeq3-PE.fa:2:30:10 \\\n  LEADING:3 TRAILING:3 SLIDINGWINDOW:4:20 MINLEN:36 \\\n'''))\n\nconst merge = task({\n  input: file(),\n  output: stdout()\n}, wrapper('seqtk mergepe {input}'))\n\nconst gzip = task({\n  input: stdin(),\n  output: stdout()\n}, wrapper('gzip - > {output}'))\n\n// Branching: these two filtering types produce the same type of output\n// So we can pass an array, and define them both under the same input/output\nconst filter = task({\n  input: file()\n  output: file()\n  opts: {\n    tmpDir: directory()\n  }\n}, [wrapper('''\n  kmc -k{params.KMERSIZE} -m{params.MEMORYGB} -t{params.THREADS} {input} reads.trim.pe.kmc {opts.tmp} \\\n  kmc_tools filter reads.trim.pe.kmc -cx{params.MINCOVERAGE} {input} -ci0 -cx0 {output} \\\n'''), wrapper('''\n  load-into-counting.py -N 4 -k {params.KMERSIZE} -M {params.MEMORYGB}e9 -T {params.THREADS} reads.trim.pe.fastq.gz.kh {input} \\\n  abundance-dist.py reads.trim.pe.fastq.gz.kh {input} reads.trim.pe.fastq.gz.kh.hist \\\n  filter-abund.py -T {params.THREADS} -C ${MINCOVERAGE} reads.trim.pe.fastq.gz.kh -o {output} {input} \\\n''')])\n\n// file() dependencies will stop stream.\n// for example, need to wait on an index file to be made before aligning\nconst bwa_mem = task({\n  input: {\n    reference: file(),\n    index: file(), // check for reference indexing\n    sample: file() // output of filter\n  }\n  output: stdout()\n}, wrapper('bwa mem {input.reference} {input.sample}'))\n\n// This is where streams in Node can really show\n// In snakemake or Nextflow, this would be bwa mem | samtools view\n// Which is less modular, reproducible, containerizable\nconst samtools_view = task({\n  input: stdin(),\n  output: stdout()\n}, wrapper('samtools view -Sbh {input}'))\n\nconst samtools_sort = task({\n  input: stdin(),\n  output: stdout()\n}, wrapper('samtools sort {input}'))\n\nconst samtools_index = task({\n  input: stdin(),\n  output: stdout()\n}, wrapper('samtools index {input}'))\n\n// these will all get piped through each other:\n// bwa_mem().pipe(samtools_view()).pipe(samtools_sort()).pipe(samtools_index())\nconst align = join([bwa_mem, samtools_view, samtools_sort, samtools_index])\n\nconst samtools_mpileup = task({\n  input: {\n    bam: file(),\n    index: file(),\n    reference: file()\n  },\n  output: stdout()\n}, wrapper('samtools mpileup -uf {input.bam} {input.reference}'))\n\nconst bcftools_call = task({\n  input: stdin(),\n  output: stdout()\n}, wrapper('bcftools call -c {input}'))\n\nconst callVariants = join([samtools_mpileup, bcftools_call])\n\n// need a way to use output of another task as input for this one\nconst pipeline = join(\n  [sra, extract, trim, merge],\n  // this creates a branching\n  filter,\n  align,\n  callVariants\n)\n\n// Run the whole pipeline, passing in params\nrun(pipelineParams, pipeline).pipe(task(fs.createWriteStream('{output}')))\n```\n\n[bionode-ncbi]: https://github.com/bionode/bionode-ncbi\n[bionode-ncbi-bug]: https://github.com/bionode/bionode-ncbi/issues/19\n[sra-tools]: https://github.com/ncbi/sra-tools\n[fasta format]: https://en.wikipedia.org/wiki/FASTA_format\n[fastq format]: https://en.wikipedia.org/wiki/FASTQ_format\n[trimmomatic]: http://www.usadellab.org/cms/?page=trimmomatic\n[khmer]: http://khmer.readthedocs.io/en/v2.0/\n[k-mer]: https://en.wikipedia.org/wiki/K-mer\n[bwa]: https://github.com/lh3/bwa\n[FM-index]: https://en.wikipedia.org/wiki/FM-index\n[Burrows-Wheeler transform]: https://en.wikipedia.org/wiki/Burrows%E2%80%93Wheeler_transform\n[samtools]: https://github.com/samtools/samtools\n[wiki/SAMtools]: https://en.wikipedia.org/wiki/SAMtools\n[pileup]: https://en.wikipedia.org/wiki/Pileup_format\n[vcf]: https://en.wikipedia.org/wiki/Variant_Call_Format\n[atom-r-exec]: https://atom.io/packages/r-exec\n[make-gist]: https://gist.github.com/isaacs/62a2d1825d04437c6f08\n[nextflow-processes]: http://www.nextflow.io/docs/latest/process.html\n[nextflow-channels]: http://www.nextflow.io/docs/latest/channel.html\n[nextflow-groovy]: http://www.nextflow.io/docs/latest/script.html\n[nextflow-directive]: http://www.nextflow.io/docs/latest/process.html#directives\n[wrappers-repository]: https://bitbucket.org/snakemake/snakemake-wrappers\n","source":"_posts/NGS-Workflows.md","raw":"---\ndate: 2016-06-08T12:27:27-04:00\ntitle: NGS Workflows\ntags: \n---\n\n*Next generation sequencing*. We all know that a fundamental practice in bioinformatics is the analysis of biological sequences. Similarities, functions, structures, associations, transcripts, proteins, RNA interference, regulation, interaction, DNA binding, the list goes on. Much can be hypothesized given some ATCGs (and some annotations). \n\nHowever, its **not plug and play**.  Various tools and algorithms exists for each step in NGS data pipelines. Each with their own advantages and disadvantages for a given set of data (e.g. bacterial vs. eukaryotic genomes). Their underlying algorithms can make assumptions which may not be true in all cases. New tools and methods are being developed and there are **rarely adopted standards**. Researchers today regularly construct hardcoded and unmaintanable scripts. I am not calling out these individuals on their coding practice, but rather positing that scripts without community maintained modular dependencies, with dependence on a specific environment configuration - let alone hardcoded absolute file references, are by their nature **unfit for providing reproducible NGS workflows to the community at large**. But hey, it [![works badge](https://cdn.rawgit.com/nikku/works-on-my-machine/v0.2.0/badge.svg)](https://github.com/nikku/works-on-my-machine) right?\n\nA well written **bash script** *can* be version controlled, and dependencies *can* be described, however the consumer *may* not be able to achieve an identical environment. At the very least, it will be a painful setup process.  Similarly, a **python script** is definitely more elegant and modular, but still suffers from issues such as pipeline reentrancy. One popular old tool is **make**, which can improve reentrancy by defining *rules* which have a *target* (output) and *input*s. However, the syntax is not newcomer friendly and file pattern matching can be confusing or limited. \n\nNot all hope is lost. There are have been many great efforts approaching this issue. One is **snakemake** which defines an elegant python-esque makefile with filename wildcarding, support for inline Python and R, and more. Another is **nextflow**, which goes a step further and describes pipelines through isolated (and containerizable) *process* blocks which communicate through channels. As well there are extras like galaxy, luigi, and bcbio.\n\nIn this blog post, I will define a simple variant calling pipeline. Then walk through the implementation of this pipeline using these four technologies:\n\n1. bash\n2. make\n3. snakemake\n4. nextflow\n\nThen discuss other alternatives in brief. Finally I will propose where Bionode and JavaScript can fit into this ecosystem, and which specific issues can be addressed.\n\n## Variant Calling Pipeline\n\nWe will be running a simple variant calling pipeline using a referenence genome and paired end genomic reads. For the sake of time when running the pipeline locally, we will use a small genome, *Salmonella enterica*, which has some paired end reads at about 100mb in size. With the reference genome at about 1.4mb, that provides about 70x coverage. \n\nTo follow along with this blog post, you will need to install: bionode-ncbi, sra-toolkit, bwa, samtools, bcftools, khmer, kmc, trimmomatic, snakemake, and nextflow.\n\nHa, I'm just kidding. Here is the Dockerfile: [polyglot-ngs-01](https://hub.docker.com/r/thejmazz/polyglot-ngs-01/).\n\n[bionode-ncbi][bionode-ncbi] will be used to download the reads from the NCBI SRA, or *sequence read archive*. The reference could be downloaded with `bionode-ncbi download assembly $specie` , but at the moment there is a [bug][bionode-ncbi-bug] where that downloads the `rna_from_genomic` rather than `genomic` file for some species.\n\nOnce we have the an `sra` for *Salmonella enterica*, the next step is to generate the two `fastq` files for the two ended reads. Paired ends have a 5' $\\rightarrow$ 3' set of reads and a 3' $\\rightarrow$ 5' set of reads. This allows for much more confident alignment and is generally preferred over a single set of reads. For this we can use `fastq-dump` from [sra tools][sra-tools]:\n\n```bash\nfastq-dump --split-files --skip-technical --gzip reads.sra\n```\n\nThis will produce `reads_1.fastq.gz` and `reads_2.fastq.gz`.\n\n> fastq is essentially a fasta file that also includes **quality** scores. Quality scores are produced by the **base calling** methods employed by the given NGS machine. See the wikipedia pages for the [fasta format][fasta format] and the [fastq format][fastq format].\n\nFiltering will be two step:\n\n1. trim reads adapters with [trimmomatic][trimmomatic]\n\n2. filter out bad [*k*-mers][k-mer] with \n\n   a) khmer\n\n   b) kmc\n\nThis allows us to illustrate how much the pipeline tools let us swap in and out different tools for the same step (in this instance, khmer vs kmc) and then compare results. \n\nThe filtering steps will complete by producing a `reads.filtered.fastq.gz` file. With other tools, it could have been a `reads_1.filtered.fastq.gz` and `reads_2.fastq.gz`.  It depends whether or not one of the filtering tools creates an *interleaved* file holding both read directions. It does not matter too much, as the next step can handle both cases. If we wanted to skip filtering, we could just use `reads_1.fastq.gz` and `reads_2.fastq.gz`.\n\nNow we want to align the reads to the reference using [bwa][bwa] which is a [Burrows-Wheeler transform][Burrows-Wheeler transform] based alignment tool. First, an [FM-index][FM-index] needs to be constructed for the reference genome:\n\n```bash\nbwa index reference.genomic.fna.gz\n```\n\nThen we align using the reads, producing a *sequence alignment map*:\n\n```bash\nbwa mem reference.genomic.fna.gz reads.filtered.fastq.gz > reads.sam\n```\n\nWe use [samtools][samtools] (also see [wiki/SAMtools][wiki/SAMtools]) to generate a *binary alignment map*:\n\n```bash\nsamtools view -bh reads.sam  > reads.unsorted.bam\n```\n\nand sort it:\n\n```bash\nsamtools sort reads.unsorted.bam -o reads.bam\n```\n\nWe could have piped the last three steps together, avoiding file writing/reading overhead:\n\n```bash\nbwa mem ref reads | samtools view -bh - | samtools sort - -o reads.bam\n```\n\nand then index it (creates `reads.bam.bai` - *binary alignment index*):\n\n```bash\nsamtools index reads.bam\n```\n\n\n\nAt this point, we have everything we need to call variants:\n\n```bash\nsamtools mpileup -uf reference.genomic.fna reads.bam | \\\nbcftools call -c - > reads.vcf\n```\n\n`mpileup` creates a BCF [pileup][pileup] file describing the base calls of aligned reads. Then `bcftools call` takes this and generates a [variant call format][vcf] file.\n\nSince `samtools` was complaining about the `reference.genomic.fna.gz` that comes from the NCBI Assemblies database - something to do with the compression format, I first decompressed it with:\n```bash\nbgzip -d reference.genomic.fna.gz\n```\n\n\nThats it! Thats how to get from SRA $\\rightarrow$ VCF using bionode, sra tools, trimming tools, filtering tools, bwa, samtools, and bcftools. The next sections will go over how to improve the reproducibility, reentrancy, ease of development, etc. of this workflow. \n\n## Topics\n\nFor each tool, we will inspect the following topics.\n\n### Basic Structure\n\n- quick intro to the tool, how it is written\n\n### Iterative Development\n\n- reentrancy\n- error debugging\n\n### Metrics\n\n- time for each task\n- resource usage\n- reports\n\n### Scaling\n\n- extend to many species\n- run on a cluster/cloud system\n- distribute/reproducibility\n\n## bash\n\nThe first obvious tool to use constructing a pipeline composed of a list of shell commands is a simple bash script. Essentially, we can take the commands from above and arrange them in a linear manner. You can see the full bash pipeline here: [basic-snp-calling.sh](https://github.com/bionode/gsoc16/blob/543ba66cbb42d1622089764bf01090e318307a57/pipelines/with-bash/basic-snp-calling.sh). \n\n### Basic Structure\n\nThe above section introduced the tools and their commands required to run a simple variant calling pipeline. However, all of the file names were hardcoded: `reference.genomic.fna.gz`, `reads.fastq.gz`, etc. In a real analysis, we would like to generalize so that we can provide paramaters defining the specie and reads to use. Then, for example, we can run the pipeline over many species and inspect related hypotheses. Within a bash script we can attempt to carry this out using *environment variables*. Depending on how these variables are defined, they may be accessible from different places. For example, the `PATH` variable (try `echo $PATH`) in your shell is available all the time. This is because it was defined using `export`. Without `export`, a variable is only available within in its current context. Consider:\n\n*main.sh*\n\n```bash\n#!/bin/bash\nREADS_ID=2492428 # no spaces around =!\n./downloadReads.sh\n```\n\n*downloadReads.sh*\n\n```bash\n#!/bin/bash\nbionode-ncbi download sra $READS_ID\n```\n\nIn this case, `READS_ID` will evaluate to blank. Alternatively you need to `export READS_ID=2492428` to have the variable available in other contexts. This is important to make note of, because complex shell scripts will likely be split across multiple files, in which case it is important to understand when and when not to use `export`. *Blindly using export everywhere is not recommended.*\n\nWith an understanding of environment variables out of the way, we can use them to define some initial settings for the script:\n\n```bash\n# Salmonella enterica\nREFERENCE_NAME='GCA_000006945.2_ASM694v2'\nREFERENCE=\"${REFERENCE_NAME}_genomic.fna.gz\"\nREFERENCE_URL=\"http://ftp.ncbi.nlm.nih.gov/genomes/all/$REFERENCE_NAME/$REFERENCE\"\nREADS='2492428'\nexport READSFQ='ERR1229296'\nFILTER_MODE='khmer' # 'kmc', 'none'\n```\n\n`READSFQ` is defined with `export` so it can be available from the filter scripts which have been separated out from the main file. Then files used as input and output can be described with these variables:\n\n```bash\necho \"START fastq-dump\"\nfastq-dump --split-files --skip-technical --gzip $READS/**.sra\necho \"END fastq-dump\"\n```\n\nI also notify the start and end of each task, *manually*. This is cumbersome and cluttering, and adding more useful features like time taken would be even more messy. Up until the filtering, the pipeline is completely linear. However, we would like to filter using two tools: `khmer` and `kmc`. One is significantly faster, but may produce more erroneous results. But that does not mean its useless, it can be useful to favour speed and number of species over specificity at the expense of less trials. Ideally, the swap from fast to slow should be a simple swap. As well, it can be useful to compare where exactly the two methods differ in their results. Enter the `if [ condtion ]; then  fi` block:\n\n```bash\nif [ $FILTER_MODE == 'kmc' ]; then\n  if [ ! -d $TMPDIR ]; then\n    mkdir -p $TMPDIR\n  fi\n  echo \"START filtering with kmc\"\n  export READS_TO_ALIGN=\"$READSFQ.trim.pe.kmc.fastq.gz\"\n  ./filter_kmc.sh\n  echo \"END filtering with kmc\"\nelif [ $FILTER_MODE == 'khmer' ]; then\n  echo \"Using khmer mode\"\n  export READS_TO_ALIGN=\"$READSFQ.trim.pe.khmer.fastq.gz\"\n  ./filter_khmer.sh\n  FINAL_OUTPUT=\"${READS}-khmer.vcf\"\nelse\n  echo \"No filter mode set. Continuing with reads with adapters trimmed.\"\n  READS_TO_ALIGN=\"$READSFQ.trim.pe.fastq.gz\"\nfi\n```\n\nAt this point, the pipeline branches into two modes. Currently, if we wanted to switch filter modes, we have to edit the definition of `FILTER_MODE` and restart the pipeline. If we are not checking for file existence before each command, this will be a big time waste. You can imagine how even deeper branching options can complicate it more. As well, it can be useful to change tool parameters and inspect the effects on final results. As an annoyance, changing these settings requires editing the file every time: this script would not scale to 100s of species well.\n\nAfter the filtering, the final section, alignment and calling, uses a variable set by the filtering option used to determine which reads to align with and the name of the final output. We can run the pipeline two times, wasting time, but at least the final results do not overwrite each other. \n\n### Iterative Development\n\nIterative development is the practice of developing a pipeline peice by peice. It is important because these pipelines can take a long time to run and simply running from scratch every time something needs to be changed is unnacceptable. In a bash script it can be handled by placing\n```bash\n#!/bin/bash\nset -e # exits script when an error is thrown\n\n...\n\nexit 1\n```\nthroughout the script. As well, I particulary enjoy [this][r-exec-atom] plugin, with it you can cmd+enter to send the current line or text selection to a terminal. However, there are still some remaining issues with this process. If you want to skip over previous steps, perhaps downloading reads or indexing, you will need to add explicit checks to your code. This clutters the code, and may still be error prone, as simply checking for a files existence might not be enough: it could be a 404 html document rather than a set of reads.\n\n### Metrics\n\nAny metrics you might want - time per task, cpu time per task, cpu and memory usage, would all have to be written manually. You could write a function to wrap commands and time them, etc, but this still clutters the code, obstructing the main message. Furthermore, \"in house\" metrics gathering is prone to error, especially when running on different machines. You may want to develop on a Mac and then deploy to a Linux server, where some commands may be different.\n\nAny sort of report on the pipeline performance, whether it be a text file or styled HTML document, would again require in house coding.\n\n### Scaling\n\nBash scripts are not scalable. If we were to port the script to handle multiple species, we *could use* bash arrays:\n\n```bash\nnums=(1 2 3 4 5); for i in $nums; do echo $i; done\n```\n\nThe above script might work for pipelines which only take one input parameter for a specie, but we also need the reference URL, SRA ID, etc. Going one step further, we can use array indices to gather related data. Yet the syntax is not the cleanest:\n\n```bash\nspecies=(Salmonella-enterica Staphylococcus-aureus)\nreadsID=(2492428 1274026)\n\nfor i in \"${!species[@]}\"; do \n    printf \"%s\\t%s\\t%s\\n\" $i ${species[$i]} ${readsID[$i]}\ndone\n```\n\nHowever, this would probably involve multiple code changes throughout the script. When deviating from the original purpose, bash scripts are likely to require relatively large codebase fixes. Changing species from a string to an array will break a lot of things.\n\nFurthermore, changing settings like maximum threads and memory usage will require manually editing the script when on the cluster, versus other tools which can provide this a command line option. Of course, *you could parse your own params*, but that is a lot of manual work.\n\nIn terms of reproducibility, there is not much. The script will likely assume all dependencies are installed. It may assume required binaries are in a specific place (in my script I used the `BIN` variable for this). It may also even fail on systems with different commands. It may also even assume being in a certain directory and directory structures of the system. \n\n## make\n\n`make` and `Makefile` are old, well known tools that are normally used for C/C++ compilation. The complete `Makefile` is [here](https://github.com/bionode/gsoc16/blob/543ba66cbb42d1622089764bf01090e318307a57/pipelines/with-make/Makefile). \n\n### Basic Structure\n\nWith `make`, the pipeline can be made a little more cleaner, particularly with respect to *input* and *output* from each command. A `makefile` is composed of rules:\n\n```make\nrule <target>: <prerequisites...>\n\t<command>\n```\n\nMake will run the rules that have targets for a given set of prerequisites. Consider:\n\n```bash\nrule all: genome\n\nrule chromosome_1:\n\techo ATCG > chromosome_1\n\nrule chromosome_2:\n\techo GATA > chromosome_2\n\nrule genome: chromosome_1 chromosome_2\n\tcat chromosome_1 chromsome_2 > genome\n```\n\nIf you trace the execution, it will first go into the first rule (which is named `all` by convention). The first rule states it requires `genome`. The third rule is capable of producing `genome` as that is its target, yet it requires two other prerequisites. So then rules that have `chromosome_*` in their target are used, these have no prereqs and can run right away. Then the program \"unwinds\" and everything works its way back to `rule all`. To learn more about `make` I highly recommend [this gist][make-gist] This technique, where pipelines are defined from the end-to-forwards, we will refer to as the **pull** method.\n\nWe can handle branching using set variables to enforce ignoring some rules:\n\n```bash\n# Toggle these variable declarations to switch between\n\n# 1. no trimming/filtering\n# PRECONDITION_TO_USE=fastq-dump.log\n# READS_1=reads_1.fastq.gz\n# READS_2=reads_2.fastq.gz\n\n# 2. kmc\n# PRECONDITION_TO_USE=reads.trim.pe.kmc.fastq.gz\n# READS_1=reads.trim.pe.kmc.fastq.gz\n\n# 3. khmer\nPRECONDITION_TO_USE=reads.trim.pe.khmer.fastq.gz\nREADS_1=reads.trim.pe.khmer.fastq.gz\n```\n\nAnd the corresponding rule which will invoke a given path in the branch:\n\n```bash\nreads.sam: bwa-index.log ${PRECONDITION_TO_USE}\n\t@echo \"=== Running $@ ===\"\n\t@export start=`date +%s`; \\\n\tbwa mem -t ${THREADS} reference.genomic.fna.gz ${READS_1} ${READS_2} > $@; \\\n\texport end=`date +%s`; \\\n\texport runtime=$$(($$end-$$start)); \\\n\techo \"Target $@ took $$runtime seconds\"\n```\n\n### Iterative Development\n\nIterative development in `make` is much improved over a `bash` script. This is because you can call a specific rule at invocation: `make reads.sam` for example. As well, it is common practice to include a clean rule. It is possible in `make` to define targets that don't actually create the target file. Since the target is not actually created (even though the command in that rule has ran and worked as expected), since make skips rules if the target already exists, it will unnecessarily rerun the rule. My general workflow writing a `make` pipeline was to update the prequisite of `rule all` as new rules were added, and each task from before would be skipped. One way to get around this issue is to create \"flag files\":\n\n```bash\ntrim.happened: fastq-dump.log\n\t@echo \"=== Running $@ ===\"\n\t@export start=`date +%s`; \\\n\tjava -jar ${TRIMMOMATIC} PE -phred33 \\\n\t  reads_1.fastq.gz reads_2.fastq.gz \\\n\t  reads_1.trim.pe.fastq.gz reads_1.trim.se.fastq.gz \\\n\t  reads_2.trim.pe.fastq.gz reads_2.trim.se.fastq.gz \\\n\t  ILLUMINACLIP:${ADAPTERS}/TruSeq3-PE.fa:2:30:10 \\\n\t  LEADING:3 TRAILING:3 SLIDINGWINDOW:4:20 MINLEN:36 > $@; \\\n\texport end=`date +%s`; \\\n\texport runtime=$$(($$end-$$start)); \\\n\tcat $@; \\\n\techo \"Target $@ took $$runtime seconds\"\n```\n\nHere, the stdout of trimmomatic is sent to `trim.happened`. Even if there is no stdout, an empty file will get made, and then the next run this rule can be skipped.\n\n### Metrics\n\nTo get time spent metrics, I wrapped each command in a simple delta time calculation:\n\n```bash\ntime:\n\t@echo \"=== Running $@ ===\"\n\t@export start=`date +%s`; \\\n\tsleep 2; \\\n\texport end=`date +%s`; \\\n\texport runtime=$$(($$end-$$start)); \\\n\techo \"Target $@ took $$runtime seconds\"\n```\n\nThis clutters the code fast, and is unmaintainable. It might be possible to do it cleaner, but I did not spend time trying obscure solutions. The point is that there is no \"out of the box\" way to get task metrics with `make`. You can imagine how tricky it might be to also get RAM/CPU usage for each task.\n\n### Scaling\n\nScaling with make is also difficult. There is no easy way to expand the pipeline to multiple species, and it is difficulty to \"variabilize\" outputs: targets need to strings.\n\n## snakemake\n\nThe `Makefile` was much more organized than the bash script, but still of \"a low level\".\n\nSnakemake was a refreshing take on the make style, but with many more features and powered by a high level language. Including Python integration everywhere, tasks to script in Python, Bash, and R, and some metrics out of the box. As well as some neat wildcarding. The complete Snakefile is [here](https://github.com/bionode/gsoc16/blob/543ba66cbb42d1622089764bf01090e318307a57/pipelines/with-snakemake/Snakefile).\n\n### Basic Structure\n\nYou start a Snakemake workflow very similar to a Makefile, defining a global rule and what you want it to create:\n\n```python\nrule all:\n    input: FINAL_FILES\n```\n\nSo what is `FINAL_FILES`? Since Snakemake has direct Python integration, we can actually compute it as a local variable. Heres the header for the `Snakefile` just before\n\n`rule all`:\n\n```python\nspecies = {\n    'Salmonella-enterica': {\n        'readsID': '2492428',\n        'reference_url': 'http://ftp.ncbi.nlm.nih.gov/genomes/all/GCA_000988525.2_ASM98852v2/GCA_000988525.2_ASM98852v2_genomic.fna.gz'\n    }\n}\n\nTHREADS=2\nTEMP='./tmp'\n\nFINAL_FILES = [specie+'.vcf' for specie in species]\n```\n\nThis will end up with `FINAL_FILES` being `['Salmonella-enterica.vcf']`.  See this rule which will be one of the first in the executed pipeline (here I hardcoded the specie name to better illustrate):\n\n```python\nrule all:\n    input: 'Salmonella-enterica.vcf'\n\nrule download_sra:\n    output: '{specie}.sra'\n    run:\n        readsID = species[wildcards.specie]['readsID']\n        shell('''\n            bionode-ncbi download sra {readsID};\n            cp {readsID}/*.sra {output} && rm -rf {readsID};\n        ''')       \n        \nrule call:\n    input: '{specie}.sra'\n    output: '{specie}.vcf'\n    shell: 'magic {input} > {output}'\n```\n\nThe first rule will be triggered and will be looking for another rule that has `Salmonella-enterica.vcf` in its `output`. It won't find *exactly* that, but because of Snakemake's wildcarding, `{specie}.vcf` will do (from `rule call`), and then within the `call` rule, the value of `wildcards.specie` will be `Salmonella-enterica`. Then it will move onto `download_sra`. This example also illustrates how you can use `run` and `shell()` to mix Python and shell code. You can also use `script` to run a Python or R script, and use the `R()` function to execute R code in a rule.\n\nAnother neat feature with Snakemake is the ability to drop in \"wrappers\":\n\n```python\nrule bwa_mem:\n    input:\n        ref = '{specie}.genomic.fna.gz',\n        sample = ['{specie}_'+num+'.fastq.gz' for num in ['1', '2']],\n        index_files = ['{specie}.genomic.fna.gz.'+suffix for suffix in ['amb', 'ann', 'bwt', 'pac', 'sa']]\n    log: 'logs/bwa_mem/{specie}.log'\n    output: '{specie}.sam'\n    threads: THREADS\n    wrapper: '0.0.8/bio/bwa_mem'\n```\n\nThese wrappers come from the [wrappers repository][wrappers-repository]. They run predefined commands using specific input variables. While this is great, there is some overhead in having to check the source to see what is actually happening, and by extension, you then need an internet connection to do so. There does not seem to be a huge list of wrappers, it is too bad it does not seem to have caught on extensively.\n\nStill have the problem of using log/flag files for task dependency, but less so, since you can define \"custom\" outputs, also has touch(flag) built in\n\nCould not figure out how to ecomical branching, wildcard regexes create ambiguity:\n\n![attempt-branching](https://raw.githubusercontent.com/bionode/gsoc16/fdf22b630e33dd11302ea8822c547ef9399c3ea4/pipelines/with-snakemake/dag.png)\n\nHere it unnecessarily redoes certain steps. This is because `Salmonella-enterica.trim.vcf` and `Salmonella-enterica.trim.vcf` both match `{specie}.vcf`. At first, Snakemake complained about ambiguous rules due to this conflict. I was able to generate the DAG above by using custom regexes for the wildcard in a few rules: `{specie,[a-zA-z-]+}`. However, since this then creates one set of rules with `wildcard.specie = Salmonella-enterica` and another with `wildcard.specie = Salmonella-enterica.trim`, and then that wildcard moves all the way to the first rule, and basically two pipelines are ran. I was not able to figure out how optimize this using Snakemake - if anyone can, let me know!\n\nMoreover - because the **pull** workflow style depends on *targets* and *prerequisites*, it can be difficult to achieve a branch-merge pipeline. We will see with Nextflow, which follows the dataflow paradigm, or **push**, it is natural to describe such pipelines.\n\nFor a more complicated (and real world) snakemake setup, take a look at [pachterlab/kallisto_paper_analysis/Snakefile](https://github.com/pachterlab/kallisto_paper_analysis/blob/nbt/Snakefile).\n\n### Iterative Development\n\nIterative development in Snakemake is about the same as with `make`. With wildcarding, the clean rule can be made more specific, for example:\n\n```python\nrule clean:\n    input: '{specie}.vcf'\n    shell: '''\n    \trm {specie}.sra {specie}.bam;\n    '''\n```\n\nAs opposed to `rm *.sra *.bam`. \n\n### Metrics\n\nYou can opt in to store rule benchmarks and logs:\n\n```python\nrule call:\n    input:\n        bam = '{specie}.bam',\n        alignment_index = '{specie}.bam.bai',\n        reference = '{specie}.genomic.fna'\n    output: '{specie}.vcf'\n    log: 'logs/call/{specie}.log'\n    benchmark: 'benchmarks/call/{specie}.txt'\n    shell: 'samtools mpileup -uf {input.reference} {input.bam} | bcftools call -c - > {output}'\n```\n\nHowever, you will need to make sure you name these files appropiately to avoid overwriting. An example benchmark file looks like:\n\n```\ns\th:m:s\n30.32477617263794\t0:00:30.324776\n```\n\nThere is no indication of which task the benchmark is for, meaning if you want a comprehensive report of your pipeline you will need to write a script that concatenates the benchmarks in the correct order.\n\nWith graphviz installed, snakemake can create a plot of the DAG for your pipeline:\n\n```bash\nsnakemake --dag | dot -Tpng > dag.png\n```\n\nYou can see an example of this above.\n\n### Scaling\n\nWith Snakemake it is easy to scale to multiple species. In my Snakefile, I simply just add more keys to the `species` dictionary. Since each output file is prefixed by the species name, there will be no file overlaps. You need to be vigilant with output names and ensure yourself there will be no conflicts.\n\nYou can specify clustering configuration by writing a `cluster.config` file and using its values in the call to Snakemake. The example [from the documentation](https://bitbucket.org/snakemake/snakemake/wiki/Documentation#markdown-header-cluster-configuration):\n\n*Snakefile*\n\n```python\nrule all:\n    input: \"input1.txt\", \"input2.txt\"\n\nrule compute1:\n    output: \"input1.txt\"\n    shell: \"touch input1.txt\"\n\nrule compute2:\n    output: \"input2.txt\"\n    shell: \"touch input2.txt\"\n```\n\n*cluster.json*\n\n```json\n{\n    \"__default__\" :\n    {\n        \"account\" : \"my account\",\n        \"time\" : \"00:15:00\",\n        \"n\" : 1,\n        \"partition\" : \"core\"\n    },\n    \"compute1\" :\n    {\n        \"time\" : \"00:20:00\"\n    }\n}\n```\n\nAnd the call to Snakemake:\n\n```\nsnakemake -j 999 --cluster-config cluster.json --cluster \"sbatch -A {cluster.account} -p {cluster.partition} -n {cluster.n}  -t {cluster.time}\"\n```\n\nWhile nextflow is providing a simple config based way to pass params into your clustering command, the actual command: `sbatch` is written manually. We will say with Nextflow, even this aspect of clustering is abstracted away (into \"executor\"). See also [job properties](https://bitbucket.org/snakemake/snakemake/wiki/Documentation#markdown-header-job-properties) to see how to write a generic job wrapper for clustering.\n\n## nextflow\n\nNextflow is a more recent tool, and approaches the workflow problem in a **push** sense. Similar to how Snakemake allows direct Python integration, Nextflow uses [Groovy](http://www.groovy-lang.org/). While Snakemake let you write rules with inline Python, R, and shell, in Nextflow, you *can use any scripting language*. See [main.nf](https://github.com/bionode/gsoc16/blob/543ba66cbb42d1622089764bf01090e318307a57/pipelines/with-nextflow/main.nf) for the complete pipeline.\n\nNextflow follows the [dataflow programming paradigm](https://en.wikipedia.org/wiki/Dataflow_programming) (push) which is also called [stream processing](https://en.wikipedia.org/wiki/Stream_processing) or [reactive programming](https://en.wikipedia.org/wiki/Reactive_programming). While in Snakemake you define `rule all`'s prerequisites, and then \"work backwards\" from there (pull), in the dataflow model you can think and write in the order that tasks will happen. Since the push model does not depend on inferring a complete dependency tree at the initialization, you can introduce dynamic structures into the pipeline. For example, choosing between two tasks to run *based on the output* of a previous task. One fallback with the push model is that since it can be dynamic, it is difficult to then perform a \"dry run\", or generate a DAG ahead of time.\n\n### Basic Structure\n\nNextflow uses **processes** as rules, and each process will occur in its own folder in `/work`. Since each process takes place in its own folder, using output from one task as input in another cannot be done the \"conventional way\". But that is alright, because Nextflow provides **channels** to communicate between processes. \n\nConsider:\n\n```groovy\n#!/usr/bin/env nextflow\n\nspecies = [\n  'Salmonella-enterica': [\n    'referenceURL': 'http://ftp.ncbi.nlm.nih.gov/genomes/all/GCA_000988525.2_ASM98852v2/GCA_000988525.2_ASM98852v2_genomic.fna.gz',\n    'readsID': '2492428'\n  ]\n]\n\nprocess downloadSRA {\n  container 'bionode/bionode-ncbi'\n\n  input: val readsID from species.collect { it.value.readsID }\n  output: file '**/*.sra' into reads\n\n  \"\"\"\n  bionode-ncbi download sra $readsID > tmp\n  \"\"\"\n}\n\nprocess extractSRA {\n  container 'inutano/sra-toolkit'\n\n  input: file read from reads\n  output: file '*.fastq.gz' into samples\n\n  \"\"\"\n  fastq-dump --split-files --skip-technical --gzip $read\n  \"\"\"\n}\n```\n\nSee the documentation on [processes] for more. As well, there are more types of channels than `val` and `file`, see [channels]. If you are new to [Groovy], and `species.collect { it.value.readsID }` confuses you, think of it as `species.map(specie => specie.value.readsID)`.  It is important to note that while channels may appear confusing at first (why not just use filenames?), they are an elegant solution to a problem we had with Snakemake: file name overlap. With Nextflow, you can be guarenteed no files will ever overwrite each other (unless you do so yourself in one process). As well, this abstracts away the filename, making commands appear generalized:\n\n```groovy\nprocess indexReference {\n  container 'biodckr/bwa'\n\n  input: file reference from referenceGenomeGz2\n  output: file '*.gz.*' into referenceIndexes\n\n  \"\"\"\n  bwa index $reference\n  \"\"\"\n}\n```\n\nThere is no mental overhead from managing `{specie}.genomic.fna.gz`, instead we can simply use the variable `$reference`. This makes the commands in our processes easy to read and comprehend. We can even \"hardcode\" output file names in an *extremely general way* without any fear of file overlap:\n\n```groovy\nprocess decompressReference {\n  container 'biodckrdev/htslib'\n\n  input: file referenceGenome from referenceGenomeGz1\n  output: file 'reference.genomic.fna' into referenceGenomes\n\n  \"\"\"\n  bgzip -d $referenceGenome --stdout > reference.genomic.fna\n  \"\"\"\n}\n```\n\nHere we output to `reference.genomic.fna`. A filename that would surely cause problems in any other worflow system.\n\nSomething else to discuss with Nextflow is how to handle a channel being consumed by multiple processes. Since a channel is a FIFO queue in Nextflow, once one process uses it, it is \"emptied\" and cannot be consumed by another process. The way to get around this is to create the original channel called `myOutput` and then \"clone\" it via `into` $n$ channels to be consumed by $n$ processes:\n\n```groovy\nprocess downloadReference {\n  container true\n\n  input: val referenceURL from species.collect { it.value.referenceURL }\n  output: file 'reference.genomic.fna.gz' into referenceGenomeGz\n\n  \"\"\"\n  appropriate/curl $referenceURL -o reference.genomic.fna.gz\n  \"\"\"\n}\n\n// fork reference genome into three other channels\n( referenceGenomeGz1,\n  referenceGenomeGz2,\n  referenceGenomeGz3 ) = referenceGenomeGz.into(3)\n```\n\nThis is also an example of an *executable container*, where the Dockerfile ends in `CMD [\"curl\"]`. \n\nHowever, I encountered some issues when trying to dockerize everything. TODO talk about dockerizing processes in a pipe.\n\nFinally, wherease Snakemake only allowed Python and R inside rules, with Nextflow you can use any scripting language.\n\n### Iterative Development\n\nIterative developmen with Nextflow is much improved over the other tools. I felt as though I could extend the pipeline, adding new processes, without having to worry about updating a `rule all` rule, or cleaning up old files. Nextflow seamlessly recognizes cached process' results when given the `-resume` flag.\n\nDebugging errors in processes works great. When an error occurs, you can `cd` into the relevant `/work` directory, and Nextflow provides log files, and a file that describes which command exactly was ran. This allows you to debug the error in the same environment as it will be ran, and then resume the workflow from that point.\n\n### Metrics\n\nNextflow can create timeline charts:\n\n![nextflow-timeline](https://github.com/bionode/gsoc16/raw/543ba66cbb42d1622089764bf01090e318307a57/pipelines/with-nextflow/timeline.png)\n\nand a DAG diagram of the pipeline (also notice I was able to create a \"forking\" pipeline unlike with Snakemake):\n\n![nextflow-workflow](https://github.com/bionode/gsoc16/raw/543ba66cbb42d1622089764bf01090e318307a57/pipelines/with-nextflow/workflow.png)\n\n### Scaling\n\nYou might have noticed the `container` [directive][nextflow-directive] in the above processes. Nextflow comes with built-in docker integration, which is great. *Each process can run in its own container*. This helps improve the portability and reproducibiltiy of the workflow. All a consumer needs installed on their system is Docker and Nextflow (and you can even run Nextflow in Docker). Each container can use a tagged version, ensuring when someone else runs the pipeline, they are using the *exact same version of each tool*. You could, of course use Docker in your Snakemake commands, but there would be overhead from volume mounting (mapping a local folder to a folder inside the container); Nextflow handles all that for you. As well, Nextflow has built in support for many cluster engines, which can be enabled by defining the `executor` in `nextflow.config`. Another feature to not is Nextflow's integration with GitHub. With a `main.nf` in your repo, you can run a pipeline with `nextflow run username/repo`. \n\nFinally, be sure to check out [awesome-nextflow](https://github.com/nextflow-io/awesome-nextflow)!\n\n## others\n\n[awesome-pipeline](https://github.com/pditommaso/awesome-pipeline) is a great list of many pipeline and workflow tools. Here are some I think are more relevant, and a few extras (though you should try to look through all of them, at least look at these):\n\n- [anduril](http://anduril.org/userguide/) - Component-based workflow framework for scientific data analysis\n- [antha](https://www.antha-lang.org/docs/concepts/flow-based-programming.html) - High-level language for biology\n\n\n- [bpipe](http://docs.bpipe.org/Examples/PairedEndAlignment/) - Tool for running and managing bioinformatics pipelines\n- [Conan2](https://github.com/tburdett/Conan2) - Light-weight workflow management application\n- [cosmos](https://github.com/LPM-HMS/COSMOS2) - Python library for massively parallel workflows\n- [drake](https://github.com/Factual/drake) - Robust DSL akin to Make, implemented in Clojure\n- [kronos](http://kronos.readthedocs.io/en/latest/) - Workflow assembler for cancer genome analytics and informatics\n- [loom](http://kronos.readthedocs.io/en/latest/) - Tool for running bioinformatics workflows locally or in the cloud\n- [moa](http://moa.readthedocs.io/en/latest/) - Lightweight workflows in bioinformatics\n- [OpenGE](https://github.com/adaptivegenome/openge) - Accelerated framework for manipulating and interpreting high-throughput sequencing data\n- [pipengine](https://github.com/fstrozzi/bioruby-pipengine#-the-pipeline-yaml-) - Ruby based launcher for complex biological pipelines\n- [ruffus](http://www.ruffus.org.uk/) - Computation Pipeline library for Python\n- [YAP](http://opensource.nibr.com/yap/) - Extensible parallel framework, written in Python using OpenMPI libraries\n\n\n- [clusterflow](http://clusterflow.io/examples/) - Command-line tool which uses common cluster managers to run bioinformatics pipelines\n\n\n- [hive](https://github.com/Ensembl/ensembl-hive) - System for creating and running pipelines on a distributed compute resource\n- [google cloud dataflow](https://cloud.google.com/dataflow/docs/) - unified programming model and amanaged servicefor developing and executing a wide range of data processing patterns including ETL, batch computation, and continuous computation\n- [makeflow](http://ccl.cse.nd.edu/software/makeflow/) - Workflow engine for executing large complex workflows on clusters\n- [scoop](https://github.com/soravux/scoop/) - Scalable Concurrent Operations in Python\n- [luigi](https://github.com/spotify/luigi) and [sciluigi](https://github.com/pharmbio/sciluigi) - Python module that helps you build complex pipelines of batch jobs, luigi wrapper for scientific workflows\n\n\n- [galaxy](https://usegalaxy.org/) - Web-based platform for biomedical research\n- [taverna](https://taverna.incubator.apache.org/introduction/) - Domain independent workflow system\n\n\n\n- [rabix](https://github.com/rabix/rabix) - implementation of CWL2\n\n\n- [cuneiform](https://github.com/joergen7/cuneiform) - Advanced functional workflow language and framework, implemented in Erlang\n- [mario](http://ccl.cse.nd.edu/software/makeflow/) - Scala library for defining data pipelines\n\n\n\n- [airflow](https://github.com/apache/incubator-airflow) - Python-based workflow system created by AirBnb\n- [pinball](https://github.com/pinterest/pinball) - Python based workflow engine by Pinterest\n\n\n\n- [AWS Data Pipeline](https://aws.amazon.com/datapipeline/) and [SWF](https://aws.amazon.com/swf/) - data workflow orchestration\n- [DRAY: Docker Workflow Engine](http://dray.it/) - UNIX pipes for Docker\n- [toil](https://github.com/BD2KGenomics/toil) - CWL3 and WDL support\n- [bcbio](https://github.com/chapmanb/bcbio-nextgen) - best-practice pipelines for automated analysis of high throughput sequencing data\n- [example gulpfile simple pipeline](https://github.com/pachterlab/kallisto/blob/master/gulpfile.js)\n- [scipipe](https://github.com/scipipe/scipipe) - workflow system in Go inspired by Flow-based programming\n- [node-datapumps](https://github.com/agmen-hu/node-datapumps) - Node.js ETL toolkit for easy data import, export or transfer between systems\n\nQuite a lot. \n\n## Conclusion\n\nOn a scale from 1-5, these are my ratings for each tool. Mostly as relative to each other, rather than absolutely.\n\n| Tool      | Structure | Iterative Dev. | Metrics | Scale | Reproducibility |\n| --------- | --------- | -------------- | ------- | ----- | --------------- |\n| bash      | 1         | 1              | 1       | 1     | 1               |\n| make      | 2         | 2              | 1       | 1     | 1               |\n| Snakemake | 4         | 3              | 3       | 3     | 4               |\n| Nextflow  | 4         | 5              | 5       | 5     | 5               |\n\nI have not investigated the \"others\" from above to warrant their ranking.\n\nA quick summary:\n\n**bash**\n\n- \"generic\" scripting\n- simple variable interpolation - `$foo`\n- no metrics, difficult to scale\n- no reentrancy, poor iterative development\n\n**make**\n\n- structure more pipeline oriented than bash - tasks with *input* (prerequisites) and *output* (target)\n- introduces some element of reentrancy by skipping rules whose target already exists = improved iterative development over bash\n- can \"fork\" by defining variables that change a rule's prerequisites, and running again\n- no metrics, difficult to scale\n\n**Snakemake**\n\n- enhanced scripting over bash/make with Python while using make's well known rules structure\n- improved `{input}`, `{output}` syntax and power (e.g. `{input.a}` or `{input[n]}`) over make\n- run Python, shell, or R in rule\n- enhanced wildcarding: easier to understand, more powerful than make's `%`\n- benchmarks are OK - need to do manual work to get a full pipeline report\n- DAG is nice\n- dry run is nice (and possible because of \"pull\" paradigm)\n- can scale to multiple species easily, just need to make sure file names do not overlap\n- cannot create forking paths (e.g. use same reference for two different filtering tools). Could do it by changing `FINAL_OUTPUT` and running again, but then thats basically the same as it was in `make`. (again, if you know how to do this without wildcarding ambiguity - let me know!)\n\n**Nextflow**\n\n- dataflow paradigm - \"push\"\n- as a consequence of push, no dry run\n- as a consequence of push, flexible forking and dynamic pipelines\n- scale to multiple species effortlessly with no overlapping file name concerns\n- timeline, DAG, benchmarks metrics are all very nice\n- Docker is nice - improves reproducibility and shareability\n- cannot handle large `stdout | stdin` pipes across channels (e.g. one process for `bwa mem` and another for `samtools view`)\n\n**bionode-waterwheel** *proposal*\n\n- no DSL to learn - just JavaScript. Functional, built around async, events, and streams.\n\n- streams resonate well with *push* paradigm\n\n- support for piping `stdin`, `stdout` around, e.g. `bwa.mem().pipe(samtools.view())` . This enhances modularity of pipeline, rather than having one rule or process doing `bwa mem | samtools view`, which is really two commands, each deserving their own input, output, parameter definitions. This can aid in improving reproducibility and modularity, as tools with defined input, output, params could be confidentally dropped into the pipeline.\n\n- moduler interoperation with web apps, native apps (Electron), services (Slack, email). Imagine pipeline logs being sent over a websocket to your browser.\n\n- interoperation with npm ecosystem\n\n- integrate with CWL spec\n\n- metrics will be easier to consume since output will be JSON to be consumed by d3, browser apps, etc\n\n- modular, specific, customizable waterwheel \"backend\" can be integrated into browser or native apps, bringing more power to pipeline GUIs than Galaxy for instance, where it is complicated to develop a custom pipeline (some labs hire someone to create a custom xml config for their specific pipeline so that the wet lab can \"click to play\" in Galaxy)\n\n- variable interpolation - like ES6 template literals, which can take any valid JS expression:\n\n  ```javascript\n  const PORT = config.PORT\n  const template = `Server listening on port ${PORT}`\n  ```\n\n  which can actually evaluate any JS expression:\n\n  ```javascript\n  [1, 2, 3, 4].map(num => `is even: ${num % 2 === 0 ? 'true' : 'false'}`)\n  ```\n\n\n\n- simple examples with small datasets can be browser compatibile (see: [nbind](https://github.com/charto/nbind)). Live browser examples are great for education - \"run your own NGS pipeline, from the browser\"\n- how to handle interative development is an open question. Can fork all streams into files for reentrancy while in a \"develop\" mode. \n\n\n\nA draft pipeline with **bionode-waterwheel**:\n\n```javascript\nconst ncbi = require('bionode-ncbi')\nconst wrapper = require('bionode-wrapper')\nconst waterwheel = require('bionode-waterwheel')\n\nconst { task, join, run } = waterwheel\nconst { stdout, stdin, file, directory } = waterwheel.types\n\n// Can be passed in from CLI\n// params for final pipeline call. params are things that do not\n// change how items are passed between processes, but decide output for the\n// pipeline as a whole. For example, species name and reads accession.\nconst pipelineParams = {\n  specie: 'Salmonella-enterica',\n  readsID: '2492428',\n  output: 'Salmonella-enterica.vcf'\n}\n\nconst sra = task({\n  output: stdout()\n}, ncbi.download('sra', '{params.readsID}'))\n\nconst reference = task({\n  output: stdout()\n}, ncbi.download('assembly', '{params.specie}'))\nconst bwa_index = task({\n  input: file(),\n  output: file()\n}, wrapper('bwa index {input}'))\n\n\n// some tools you cannot decide output file\n// so, tell waterwheel to stream a file as stdout\n// there is --stdout for fastq-dump to give a streamed fastq, but trimmomatic\n// wants reads_1 and reads_2\n// tools that are not bionode will need to wrapper()ed\nconst extract = task({\n  input: file('reads.sra'),\n  output: file([1, 2].map(n => `reads_${n}.fastq.gz`))\n}, wrapper('fastq-dump --split-files --skip-technical --gzip {input}'))\n\nconst trim = task({\n  input: file([1, 2].map(n => `reads_${n}.fastq.gz`)),\n  output: {\n    pe: file([1, 2].map(n => `reads_${n}.trim.pe.fastq.gz`),\n    se: file([1, 2].map(n => `reads_${n}.trim.se.fastq.gz`)\n  },\n  opts: {\n    adapters: '../adapters'\n  }\n}, wrapper('''\n  trimmomatic PE -phred33 \\\n  {input[0]} {input[1]} \\\n  {output.pe[0]} {output.se[0]} \\\n  {output.pe[1]} {output.se[1]} \\\n  ILLUMINACLIP:{opts.adapters}/TruSeq3-PE.fa:2:30:10 \\\n  LEADING:3 TRAILING:3 SLIDINGWINDOW:4:20 MINLEN:36 \\\n'''))\n\nconst merge = task({\n  input: file(),\n  output: stdout()\n}, wrapper('seqtk mergepe {input}'))\n\nconst gzip = task({\n  input: stdin(),\n  output: stdout()\n}, wrapper('gzip - > {output}'))\n\n// Branching: these two filtering types produce the same type of output\n// So we can pass an array, and define them both under the same input/output\nconst filter = task({\n  input: file()\n  output: file()\n  opts: {\n    tmpDir: directory()\n  }\n}, [wrapper('''\n  kmc -k{params.KMERSIZE} -m{params.MEMORYGB} -t{params.THREADS} {input} reads.trim.pe.kmc {opts.tmp} \\\n  kmc_tools filter reads.trim.pe.kmc -cx{params.MINCOVERAGE} {input} -ci0 -cx0 {output} \\\n'''), wrapper('''\n  load-into-counting.py -N 4 -k {params.KMERSIZE} -M {params.MEMORYGB}e9 -T {params.THREADS} reads.trim.pe.fastq.gz.kh {input} \\\n  abundance-dist.py reads.trim.pe.fastq.gz.kh {input} reads.trim.pe.fastq.gz.kh.hist \\\n  filter-abund.py -T {params.THREADS} -C ${MINCOVERAGE} reads.trim.pe.fastq.gz.kh -o {output} {input} \\\n''')])\n\n// file() dependencies will stop stream.\n// for example, need to wait on an index file to be made before aligning\nconst bwa_mem = task({\n  input: {\n    reference: file(),\n    index: file(), // check for reference indexing\n    sample: file() // output of filter\n  }\n  output: stdout()\n}, wrapper('bwa mem {input.reference} {input.sample}'))\n\n// This is where streams in Node can really show\n// In snakemake or Nextflow, this would be bwa mem | samtools view\n// Which is less modular, reproducible, containerizable\nconst samtools_view = task({\n  input: stdin(),\n  output: stdout()\n}, wrapper('samtools view -Sbh {input}'))\n\nconst samtools_sort = task({\n  input: stdin(),\n  output: stdout()\n}, wrapper('samtools sort {input}'))\n\nconst samtools_index = task({\n  input: stdin(),\n  output: stdout()\n}, wrapper('samtools index {input}'))\n\n// these will all get piped through each other:\n// bwa_mem().pipe(samtools_view()).pipe(samtools_sort()).pipe(samtools_index())\nconst align = join([bwa_mem, samtools_view, samtools_sort, samtools_index])\n\nconst samtools_mpileup = task({\n  input: {\n    bam: file(),\n    index: file(),\n    reference: file()\n  },\n  output: stdout()\n}, wrapper('samtools mpileup -uf {input.bam} {input.reference}'))\n\nconst bcftools_call = task({\n  input: stdin(),\n  output: stdout()\n}, wrapper('bcftools call -c {input}'))\n\nconst callVariants = join([samtools_mpileup, bcftools_call])\n\n// need a way to use output of another task as input for this one\nconst pipeline = join(\n  [sra, extract, trim, merge],\n  // this creates a branching\n  filter,\n  align,\n  callVariants\n)\n\n// Run the whole pipeline, passing in params\nrun(pipelineParams, pipeline).pipe(task(fs.createWriteStream('{output}')))\n```\n\n[bionode-ncbi]: https://github.com/bionode/bionode-ncbi\n[bionode-ncbi-bug]: https://github.com/bionode/bionode-ncbi/issues/19\n[sra-tools]: https://github.com/ncbi/sra-tools\n[fasta format]: https://en.wikipedia.org/wiki/FASTA_format\n[fastq format]: https://en.wikipedia.org/wiki/FASTQ_format\n[trimmomatic]: http://www.usadellab.org/cms/?page=trimmomatic\n[khmer]: http://khmer.readthedocs.io/en/v2.0/\n[k-mer]: https://en.wikipedia.org/wiki/K-mer\n[bwa]: https://github.com/lh3/bwa\n[FM-index]: https://en.wikipedia.org/wiki/FM-index\n[Burrows-Wheeler transform]: https://en.wikipedia.org/wiki/Burrows%E2%80%93Wheeler_transform\n[samtools]: https://github.com/samtools/samtools\n[wiki/SAMtools]: https://en.wikipedia.org/wiki/SAMtools\n[pileup]: https://en.wikipedia.org/wiki/Pileup_format\n[vcf]: https://en.wikipedia.org/wiki/Variant_Call_Format\n[atom-r-exec]: https://atom.io/packages/r-exec\n[make-gist]: https://gist.github.com/isaacs/62a2d1825d04437c6f08\n[nextflow-processes]: http://www.nextflow.io/docs/latest/process.html\n[nextflow-channels]: http://www.nextflow.io/docs/latest/channel.html\n[nextflow-groovy]: http://www.nextflow.io/docs/latest/script.html\n[nextflow-directive]: http://www.nextflow.io/docs/latest/process.html#directives\n[wrappers-repository]: https://bitbucket.org/snakemake/snakemake-wrappers\n","slug":"NGS-Workflows","published":1,"updated":"2016-06-29T04:33:13.000Z","_id":"ciq0dumpo00015yv7apdtha5u","comments":1,"layout":"post","photos":[],"link":"","content":"<p><em>Next generation sequencing</em>. We all know that a fundamental practice in bioinformatics is the analysis of biological sequences. Similarities, functions, structures, associations, transcripts, proteins, RNA interference, regulation, interaction, DNA binding, the list goes on. Much can be hypothesized given some ATCGs (and some annotations). </p>\n<p>However, its <strong>not plug and play</strong>.  Various tools and algorithms exists for each step in NGS data pipelines. Each with their own advantages and disadvantages for a given set of data (e.g. bacterial vs. eukaryotic genomes). Their underlying algorithms can make assumptions which may not be true in all cases. New tools and methods are being developed and there are <strong>rarely adopted standards</strong>. Researchers today regularly construct hardcoded and unmaintanable scripts. I am not calling out these individuals on their coding practice, but rather positing that scripts without community maintained modular dependencies, with dependence on a specific environment configuration - let alone hardcoded absolute file references, are by their nature <strong>unfit for providing reproducible NGS workflows to the community at large</strong>. But hey, it <a href=\"https://github.com/nikku/works-on-my-machine\" target=\"_blank\" rel=\"external\"><img src=\"https://cdn.rawgit.com/nikku/works-on-my-machine/v0.2.0/badge.svg\" alt=\"works badge\"></a> right?</p>\n<p>A well written <strong>bash script</strong> <em>can</em> be version controlled, and dependencies <em>can</em> be described, however the consumer <em>may</em> not be able to achieve an identical environment. At the very least, it will be a painful setup process.  Similarly, a <strong>python script</strong> is definitely more elegant and modular, but still suffers from issues such as pipeline reentrancy. One popular old tool is <strong>make</strong>, which can improve reentrancy by defining <em>rules</em> which have a <em>target</em> (output) and <em>input</em>s. However, the syntax is not newcomer friendly and file pattern matching can be confusing or limited. </p>\n<p>Not all hope is lost. There are have been many great efforts approaching this issue. One is <strong>snakemake</strong> which defines an elegant python-esque makefile with filename wildcarding, support for inline Python and R, and more. Another is <strong>nextflow</strong>, which goes a step further and describes pipelines through isolated (and containerizable) <em>process</em> blocks which communicate through channels. As well there are extras like galaxy, luigi, and bcbio.</p>\n<p>In this blog post, I will define a simple variant calling pipeline. Then walk through the implementation of this pipeline using these four technologies:</p>\n<ol>\n<li>bash</li>\n<li>make</li>\n<li>snakemake</li>\n<li>nextflow</li>\n</ol>\n<p>Then discuss other alternatives in brief. Finally I will propose where Bionode and JavaScript can fit into this ecosystem, and which specific issues can be addressed.</p>\n<h2 id=\"Variant-Calling-Pipeline\"><a href=\"#Variant-Calling-Pipeline\" class=\"headerlink\" title=\"Variant Calling Pipeline\"></a>Variant Calling Pipeline</h2><p>We will be running a simple variant calling pipeline using a referenence genome and paired end genomic reads. For the sake of time when running the pipeline locally, we will use a small genome, <em>Salmonella enterica</em>, which has some paired end reads at about 100mb in size. With the reference genome at about 1.4mb, that provides about 70x coverage. </p>\n<p>To follow along with this blog post, you will need to install: bionode-ncbi, sra-toolkit, bwa, samtools, bcftools, khmer, kmc, trimmomatic, snakemake, and nextflow.</p>\n<p>Ha, Im just kidding. Here is the Dockerfile: <a href=\"https://hub.docker.com/r/thejmazz/polyglot-ngs-01/\" target=\"_blank\" rel=\"external\">polyglot-ngs-01</a>.</p>\n<p><a href=\"https://github.com/bionode/bionode-ncbi\" target=\"_blank\" rel=\"external\">bionode-ncbi</a> will be used to download the reads from the NCBI SRA, or <em>sequence read archive</em>. The reference could be downloaded with <code>bionode-ncbi download assembly $specie</code> , but at the moment there is a <a href=\"https://github.com/bionode/bionode-ncbi/issues/19\" target=\"_blank\" rel=\"external\">bug</a> where that downloads the <code>rna_from_genomic</code> rather than <code>genomic</code> file for some species.</p>\n<p>Once we have the an <code>sra</code> for <em>Salmonella enterica</em>, the next step is to generate the two <code>fastq</code> files for the two ended reads. Paired ends have a 5 $\\rightarrow$ 3 set of reads and a 3 $\\rightarrow$ 5 set of reads. This allows for much more confident alignment and is generally preferred over a single set of reads. For this we can use <code>fastq-dump</code> from <a href=\"https://github.com/ncbi/sra-tools\" target=\"_blank\" rel=\"external\">sra tools</a>:</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">fastq-dump --split-files --skip-technical --gzip reads.sra</div></pre></td></tr></table></figure>\n<p>This will produce <code>reads_1.fastq.gz</code> and <code>reads_2.fastq.gz</code>.</p>\n<blockquote>\n<p>fastq is essentially a fasta file that also includes <strong>quality</strong> scores. Quality scores are produced by the <strong>base calling</strong> methods employed by the given NGS machine. See the wikipedia pages for the <a href=\"https://en.wikipedia.org/wiki/FASTA_format\" target=\"_blank\" rel=\"external\">fasta format</a> and the <a href=\"https://en.wikipedia.org/wiki/FASTQ_format\" target=\"_blank\" rel=\"external\">fastq format</a>.</p>\n</blockquote>\n<p>Filtering will be two step:</p>\n<ol>\n<li><p>trim reads adapters with <a href=\"http://www.usadellab.org/cms/?page=trimmomatic\" target=\"_blank\" rel=\"external\">trimmomatic</a></p>\n</li>\n<li><p>filter out bad <a href=\"https://en.wikipedia.org/wiki/K-mer\" target=\"_blank\" rel=\"external\"><em>k</em>-mers</a> with </p>\n<p>a) khmer</p>\n<p>b) kmc</p>\n</li>\n</ol>\n<p>This allows us to illustrate how much the pipeline tools let us swap in and out different tools for the same step (in this instance, khmer vs kmc) and then compare results. </p>\n<p>The filtering steps will complete by producing a <code>reads.filtered.fastq.gz</code> file. With other tools, it could have been a <code>reads_1.filtered.fastq.gz</code> and <code>reads_2.fastq.gz</code>.  It depends whether or not one of the filtering tools creates an <em>interleaved</em> file holding both read directions. It does not matter too much, as the next step can handle both cases. If we wanted to skip filtering, we could just use <code>reads_1.fastq.gz</code> and <code>reads_2.fastq.gz</code>.</p>\n<p>Now we want to align the reads to the reference using <a href=\"https://github.com/lh3/bwa\" target=\"_blank\" rel=\"external\">bwa</a> which is a <a href=\"https://en.wikipedia.org/wiki/Burrows%E2%80%93Wheeler_transform\" target=\"_blank\" rel=\"external\">Burrows-Wheeler transform</a> based alignment tool. First, an <a href=\"https://en.wikipedia.org/wiki/FM-index\" target=\"_blank\" rel=\"external\">FM-index</a> needs to be constructed for the reference genome:</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">bwa index reference.genomic.fna.gz</div></pre></td></tr></table></figure>\n<p>Then we align using the reads, producing a <em>sequence alignment map</em>:</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">bwa mem reference.genomic.fna.gz reads.filtered.fastq.gz &gt; reads.sam</div></pre></td></tr></table></figure>\n<p>We use <a href=\"https://github.com/samtools/samtools\" target=\"_blank\" rel=\"external\">samtools</a> (also see <a href=\"https://en.wikipedia.org/wiki/SAMtools\" target=\"_blank\" rel=\"external\">wiki/SAMtools</a>) to generate a <em>binary alignment map</em>:</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">samtools view -bh reads.sam  &gt; reads.unsorted.bam</div></pre></td></tr></table></figure>\n<p>and sort it:</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">samtools sort reads.unsorted.bam -o reads.bam</div></pre></td></tr></table></figure>\n<p>We could have piped the last three steps together, avoiding file writing/reading overhead:</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">bwa mem ref reads | samtools view -bh - | samtools sort - -o reads.bam</div></pre></td></tr></table></figure>\n<p>and then index it (creates <code>reads.bam.bai</code> - <em>binary alignment index</em>):</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">samtools index reads.bam</div></pre></td></tr></table></figure>\n<p>At this point, we have everything we need to call variants:</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">samtools mpileup -uf reference.genomic.fna reads.bam | \\</div><div class=\"line\">bcftools call -c - &gt; reads.vcf</div></pre></td></tr></table></figure>\n<p><code>mpileup</code> creates a BCF <a href=\"https://en.wikipedia.org/wiki/Pileup_format\" target=\"_blank\" rel=\"external\">pileup</a> file describing the base calls of aligned reads. Then <code>bcftools call</code> takes this and generates a <a href=\"https://en.wikipedia.org/wiki/Variant_Call_Format\" target=\"_blank\" rel=\"external\">variant call format</a> file.</p>\n<p>Since <code>samtools</code> was complaining about the <code>reference.genomic.fna.gz</code> that comes from the NCBI Assemblies database - something to do with the compression format, I first decompressed it with:<br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">bgzip <span class=\"_\">-d</span> reference.genomic.fna.gz</div></pre></td></tr></table></figure></p>\n<p>Thats it! Thats how to get from SRA $\\rightarrow$ VCF using bionode, sra tools, trimming tools, filtering tools, bwa, samtools, and bcftools. The next sections will go over how to improve the reproducibility, reentrancy, ease of development, etc. of this workflow. </p>\n<h2 id=\"Topics\"><a href=\"#Topics\" class=\"headerlink\" title=\"Topics\"></a>Topics</h2><p>For each tool, we will inspect the following topics.</p>\n<h3 id=\"Basic-Structure\"><a href=\"#Basic-Structure\" class=\"headerlink\" title=\"Basic Structure\"></a>Basic Structure</h3><ul>\n<li>quick intro to the tool, how it is written</li>\n</ul>\n<h3 id=\"Iterative-Development\"><a href=\"#Iterative-Development\" class=\"headerlink\" title=\"Iterative Development\"></a>Iterative Development</h3><ul>\n<li>reentrancy</li>\n<li>error debugging</li>\n</ul>\n<h3 id=\"Metrics\"><a href=\"#Metrics\" class=\"headerlink\" title=\"Metrics\"></a>Metrics</h3><ul>\n<li>time for each task</li>\n<li>resource usage</li>\n<li>reports</li>\n</ul>\n<h3 id=\"Scaling\"><a href=\"#Scaling\" class=\"headerlink\" title=\"Scaling\"></a>Scaling</h3><ul>\n<li>extend to many species</li>\n<li>run on a cluster/cloud system</li>\n<li>distribute/reproducibility</li>\n</ul>\n<h2 id=\"bash\"><a href=\"#bash\" class=\"headerlink\" title=\"bash\"></a>bash</h2><p>The first obvious tool to use constructing a pipeline composed of a list of shell commands is a simple bash script. Essentially, we can take the commands from above and arrange them in a linear manner. You can see the full bash pipeline here: <a href=\"https://github.com/bionode/gsoc16/blob/543ba66cbb42d1622089764bf01090e318307a57/pipelines/with-bash/basic-snp-calling.sh\" target=\"_blank\" rel=\"external\">basic-snp-calling.sh</a>. </p>\n<h3 id=\"Basic-Structure-1\"><a href=\"#Basic-Structure-1\" class=\"headerlink\" title=\"Basic Structure\"></a>Basic Structure</h3><p>The above section introduced the tools and their commands required to run a simple variant calling pipeline. However, all of the file names were hardcoded: <code>reference.genomic.fna.gz</code>, <code>reads.fastq.gz</code>, etc. In a real analysis, we would like to generalize so that we can provide paramaters defining the specie and reads to use. Then, for example, we can run the pipeline over many species and inspect related hypotheses. Within a bash script we can attempt to carry this out using <em>environment variables</em>. Depending on how these variables are defined, they may be accessible from different places. For example, the <code>PATH</code> variable (try <code>echo $PATH</code>) in your shell is available all the time. This is because it was defined using <code>export</code>. Without <code>export</code>, a variable is only available within in its current context. Consider:</p>\n<p><em>main.sh</em></p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"meta\">#!/bin/bash</span></div><div class=\"line\">READS_ID=2492428 <span class=\"comment\"># no spaces around =!</span></div><div class=\"line\">./downloadReads.sh</div></pre></td></tr></table></figure>\n<p><em>downloadReads.sh</em></p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"meta\">#!/bin/bash</span></div><div class=\"line\">bionode-ncbi download sra <span class=\"variable\">$READS_ID</span></div></pre></td></tr></table></figure>\n<p>In this case, <code>READS_ID</code> will evaluate to blank. Alternatively you need to <code>export READS_ID=2492428</code> to have the variable available in other contexts. This is important to make note of, because complex shell scripts will likely be split across multiple files, in which case it is important to understand when and when not to use <code>export</code>. <em>Blindly using export everywhere is not recommended.</em></p>\n<p>With an understanding of environment variables out of the way, we can use them to define some initial settings for the script:</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\"># Salmonella enterica</span></div><div class=\"line\">REFERENCE_NAME=<span class=\"string\">'GCA_000006945.2_ASM694v2'</span></div><div class=\"line\">REFERENCE=<span class=\"string\">\"<span class=\"variable\">$&#123;REFERENCE_NAME&#125;</span>_genomic.fna.gz\"</span></div><div class=\"line\">REFERENCE_URL=<span class=\"string\">\"http://ftp.ncbi.nlm.nih.gov/genomes/all/<span class=\"variable\">$REFERENCE_NAME</span>/<span class=\"variable\">$REFERENCE</span>\"</span></div><div class=\"line\">READS=<span class=\"string\">'2492428'</span></div><div class=\"line\"><span class=\"built_in\">export</span> READSFQ=<span class=\"string\">'ERR1229296'</span></div><div class=\"line\">FILTER_MODE=<span class=\"string\">'khmer'</span> <span class=\"comment\"># 'kmc', 'none'</span></div></pre></td></tr></table></figure>\n<p><code>READSFQ</code> is defined with <code>export</code> so it can be available from the filter scripts which have been separated out from the main file. Then files used as input and output can be described with these variables:</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"built_in\">echo</span> <span class=\"string\">\"START fastq-dump\"</span></div><div class=\"line\">fastq-dump --split-files --skip-technical --gzip <span class=\"variable\">$READS</span>/**.sra</div><div class=\"line\"><span class=\"built_in\">echo</span> <span class=\"string\">\"END fastq-dump\"</span></div></pre></td></tr></table></figure>\n<p>I also notify the start and end of each task, <em>manually</em>. This is cumbersome and cluttering, and adding more useful features like time taken would be even more messy. Up until the filtering, the pipeline is completely linear. However, we would like to filter using two tools: <code>khmer</code> and <code>kmc</code>. One is significantly faster, but may produce more erroneous results. But that does not mean its useless, it can be useful to favour speed and number of species over specificity at the expense of less trials. Ideally, the swap from fast to slow should be a simple swap. As well, it can be useful to compare where exactly the two methods differ in their results. Enter the <code>if [ condtion ]; then  fi</code> block:</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">if</span> [ <span class=\"variable\">$FILTER_MODE</span> == <span class=\"string\">'kmc'</span> ]; <span class=\"keyword\">then</span></div><div class=\"line\">  <span class=\"keyword\">if</span> [ ! <span class=\"_\">-d</span> <span class=\"variable\">$TMPDIR</span> ]; <span class=\"keyword\">then</span></div><div class=\"line\">    mkdir -p <span class=\"variable\">$TMPDIR</span></div><div class=\"line\">  <span class=\"keyword\">fi</span></div><div class=\"line\">  <span class=\"built_in\">echo</span> <span class=\"string\">\"START filtering with kmc\"</span></div><div class=\"line\">  <span class=\"built_in\">export</span> READS_TO_ALIGN=<span class=\"string\">\"<span class=\"variable\">$READSFQ</span>.trim.pe.kmc.fastq.gz\"</span></div><div class=\"line\">  ./filter_kmc.sh</div><div class=\"line\">  <span class=\"built_in\">echo</span> <span class=\"string\">\"END filtering with kmc\"</span></div><div class=\"line\"><span class=\"keyword\">elif</span> [ <span class=\"variable\">$FILTER_MODE</span> == <span class=\"string\">'khmer'</span> ]; <span class=\"keyword\">then</span></div><div class=\"line\">  <span class=\"built_in\">echo</span> <span class=\"string\">\"Using khmer mode\"</span></div><div class=\"line\">  <span class=\"built_in\">export</span> READS_TO_ALIGN=<span class=\"string\">\"<span class=\"variable\">$READSFQ</span>.trim.pe.khmer.fastq.gz\"</span></div><div class=\"line\">  ./filter_khmer.sh</div><div class=\"line\">  FINAL_OUTPUT=<span class=\"string\">\"<span class=\"variable\">$&#123;READS&#125;</span>-khmer.vcf\"</span></div><div class=\"line\"><span class=\"keyword\">else</span></div><div class=\"line\">  <span class=\"built_in\">echo</span> <span class=\"string\">\"No filter mode set. Continuing with reads with adapters trimmed.\"</span></div><div class=\"line\">  READS_TO_ALIGN=<span class=\"string\">\"<span class=\"variable\">$READSFQ</span>.trim.pe.fastq.gz\"</span></div><div class=\"line\"><span class=\"keyword\">fi</span></div></pre></td></tr></table></figure>\n<p>At this point, the pipeline branches into two modes. Currently, if we wanted to switch filter modes, we have to edit the definition of <code>FILTER_MODE</code> and restart the pipeline. If we are not checking for file existence before each command, this will be a big time waste. You can imagine how even deeper branching options can complicate it more. As well, it can be useful to change tool parameters and inspect the effects on final results. As an annoyance, changing these settings requires editing the file every time: this script would not scale to 100s of species well.</p>\n<p>After the filtering, the final section, alignment and calling, uses a variable set by the filtering option used to determine which reads to align with and the name of the final output. We can run the pipeline two times, wasting time, but at least the final results do not overwrite each other. </p>\n<h3 id=\"Iterative-Development-1\"><a href=\"#Iterative-Development-1\" class=\"headerlink\" title=\"Iterative Development\"></a>Iterative Development</h3><p>Iterative development is the practice of developing a pipeline peice by peice. It is important because these pipelines can take a long time to run and simply running from scratch every time something needs to be changed is unnacceptable. In a bash script it can be handled by placing<br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"meta\">#!/bin/bash</span></div><div class=\"line\"><span class=\"built_in\">set</span> <span class=\"_\">-e</span> <span class=\"comment\"># exits script when an error is thrown</span></div><div class=\"line\"></div><div class=\"line\">...</div><div class=\"line\"></div><div class=\"line\"><span class=\"built_in\">exit</span> 1</div></pre></td></tr></table></figure></p>\n<p>throughout the script. As well, I particulary enjoy [this][r-exec-atom] plugin, with it you can cmd+enter to send the current line or text selection to a terminal. However, there are still some remaining issues with this process. If you want to skip over previous steps, perhaps downloading reads or indexing, you will need to add explicit checks to your code. This clutters the code, and may still be error prone, as simply checking for a files existence might not be enough: it could be a 404 html document rather than a set of reads.</p>\n<h3 id=\"Metrics-1\"><a href=\"#Metrics-1\" class=\"headerlink\" title=\"Metrics\"></a>Metrics</h3><p>Any metrics you might want - time per task, cpu time per task, cpu and memory usage, would all have to be written manually. You could write a function to wrap commands and time them, etc, but this still clutters the code, obstructing the main message. Furthermore, in house metrics gathering is prone to error, especially when running on different machines. You may want to develop on a Mac and then deploy to a Linux server, where some commands may be different.</p>\n<p>Any sort of report on the pipeline performance, whether it be a text file or styled HTML document, would again require in house coding.</p>\n<h3 id=\"Scaling-1\"><a href=\"#Scaling-1\" class=\"headerlink\" title=\"Scaling\"></a>Scaling</h3><p>Bash scripts are not scalable. If we were to port the script to handle multiple species, we <em>could use</em> bash arrays:</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">nums=(1 2 3 4 5); <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"variable\">$nums</span>; <span class=\"keyword\">do</span> <span class=\"built_in\">echo</span> <span class=\"variable\">$i</span>; <span class=\"keyword\">done</span></div></pre></td></tr></table></figure>\n<p>The above script might work for pipelines which only take one input parameter for a specie, but we also need the reference URL, SRA ID, etc. Going one step further, we can use array indices to gather related data. Yet the syntax is not the cleanest:</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\">species=(Salmonella-enterica Staphylococcus-aureus)</div><div class=\"line\">readsID=(2492428 1274026)</div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"string\">\"<span class=\"variable\">$&#123;!species[@]&#125;</span>\"</span>; <span class=\"keyword\">do</span> </div><div class=\"line\">    <span class=\"built_in\">printf</span> <span class=\"string\">\"%s\\t%s\\t%s\\n\"</span> <span class=\"variable\">$i</span> <span class=\"variable\">$&#123;species[$i]&#125;</span> <span class=\"variable\">$&#123;readsID[$i]&#125;</span></div><div class=\"line\"><span class=\"keyword\">done</span></div></pre></td></tr></table></figure>\n<p>However, this would probably involve multiple code changes throughout the script. When deviating from the original purpose, bash scripts are likely to require relatively large codebase fixes. Changing species from a string to an array will break a lot of things.</p>\n<p>Furthermore, changing settings like maximum threads and memory usage will require manually editing the script when on the cluster, versus other tools which can provide this a command line option. Of course, <em>you could parse your own params</em>, but that is a lot of manual work.</p>\n<p>In terms of reproducibility, there is not much. The script will likely assume all dependencies are installed. It may assume required binaries are in a specific place (in my script I used the <code>BIN</code> variable for this). It may also even fail on systems with different commands. It may also even assume being in a certain directory and directory structures of the system. </p>\n<h2 id=\"make\"><a href=\"#make\" class=\"headerlink\" title=\"make\"></a>make</h2><p><code>make</code> and <code>Makefile</code> are old, well known tools that are normally used for C/C++ compilation. The complete <code>Makefile</code> is <a href=\"https://github.com/bionode/gsoc16/blob/543ba66cbb42d1622089764bf01090e318307a57/pipelines/with-make/Makefile\" target=\"_blank\" rel=\"external\">here</a>. </p>\n<h3 id=\"Basic-Structure-2\"><a href=\"#Basic-Structure-2\" class=\"headerlink\" title=\"Basic Structure\"></a>Basic Structure</h3><p>With <code>make</code>, the pipeline can be made a little more cleaner, particularly with respect to <em>input</em> and <em>output</em> from each command. A <code>makefile</code> is composed of rules:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">rule &lt;target&gt;: &lt;prerequisites...&gt;</div><div class=\"line\">\t&lt;command&gt;</div></pre></td></tr></table></figure>\n<p>Make will run the rules that have targets for a given set of prerequisites. Consider:</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div></pre></td><td class=\"code\"><pre><div class=\"line\">rule all: genome</div><div class=\"line\"></div><div class=\"line\">rule chromosome_1:</div><div class=\"line\">\t<span class=\"built_in\">echo</span> ATCG &gt; chromosome_1</div><div class=\"line\"></div><div class=\"line\">rule chromosome_2:</div><div class=\"line\">\t<span class=\"built_in\">echo</span> GATA &gt; chromosome_2</div><div class=\"line\"></div><div class=\"line\">rule genome: chromosome_1 chromosome_2</div><div class=\"line\">\tcat chromosome_1 chromsome_2 &gt; genome</div></pre></td></tr></table></figure>\n<p>If you trace the execution, it will first go into the first rule (which is named <code>all</code> by convention). The first rule states it requires <code>genome</code>. The third rule is capable of producing <code>genome</code> as that is its target, yet it requires two other prerequisites. So then rules that have <code>chromosome_*</code> in their target are used, these have no prereqs and can run right away. Then the program unwinds and everything works its way back to <code>rule all</code>. To learn more about <code>make</code> I highly recommend <a href=\"https://gist.github.com/isaacs/62a2d1825d04437c6f08\" target=\"_blank\" rel=\"external\">this gist</a> This technique, where pipelines are defined from the end-to-forwards, we will refer to as the <strong>pull</strong> method.</p>\n<p>We can handle branching using set variables to enforce ignoring some rules:</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\"># Toggle these variable declarations to switch between</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># 1. no trimming/filtering</span></div><div class=\"line\"><span class=\"comment\"># PRECONDITION_TO_USE=fastq-dump.log</span></div><div class=\"line\"><span class=\"comment\"># READS_1=reads_1.fastq.gz</span></div><div class=\"line\"><span class=\"comment\"># READS_2=reads_2.fastq.gz</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># 2. kmc</span></div><div class=\"line\"><span class=\"comment\"># PRECONDITION_TO_USE=reads.trim.pe.kmc.fastq.gz</span></div><div class=\"line\"><span class=\"comment\"># READS_1=reads.trim.pe.kmc.fastq.gz</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># 3. khmer</span></div><div class=\"line\">PRECONDITION_TO_USE=reads.trim.pe.khmer.fastq.gz</div><div class=\"line\">READS_1=reads.trim.pe.khmer.fastq.gz</div></pre></td></tr></table></figure>\n<p>And the corresponding rule which will invoke a given path in the branch:</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div></pre></td><td class=\"code\"><pre><div class=\"line\">reads.sam: bwa-index.log <span class=\"variable\">$&#123;PRECONDITION_TO_USE&#125;</span></div><div class=\"line\">\t@<span class=\"built_in\">echo</span> <span class=\"string\">\"=== Running <span class=\"variable\">$@</span> ===\"</span></div><div class=\"line\">\t@<span class=\"built_in\">export</span> start=`date +%s`; \\</div><div class=\"line\">\tbwa mem -t <span class=\"variable\">$&#123;THREADS&#125;</span> reference.genomic.fna.gz <span class=\"variable\">$&#123;READS_1&#125;</span> <span class=\"variable\">$&#123;READS_2&#125;</span> &gt; <span class=\"variable\">$@</span>; \\</div><div class=\"line\">\t<span class=\"built_in\">export</span> end=`date +%s`; \\</div><div class=\"line\">\t<span class=\"built_in\">export</span> runtime=$$(($<span class=\"variable\">$end</span>-$<span class=\"variable\">$start</span>)); \\</div><div class=\"line\">\t<span class=\"built_in\">echo</span> <span class=\"string\">\"Target <span class=\"variable\">$@</span> took $<span class=\"variable\">$runtime</span> seconds\"</span></div></pre></td></tr></table></figure>\n<h3 id=\"Iterative-Development-2\"><a href=\"#Iterative-Development-2\" class=\"headerlink\" title=\"Iterative Development\"></a>Iterative Development</h3><p>Iterative development in <code>make</code> is much improved over a <code>bash</code> script. This is because you can call a specific rule at invocation: <code>make reads.sam</code> for example. As well, it is common practice to include a clean rule. It is possible in <code>make</code> to define targets that dont actually create the target file. Since the target is not actually created (even though the command in that rule has ran and worked as expected), since make skips rules if the target already exists, it will unnecessarily rerun the rule. My general workflow writing a <code>make</code> pipeline was to update the prequisite of <code>rule all</code> as new rules were added, and each task from before would be skipped. One way to get around this issue is to create flag files:</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div></pre></td><td class=\"code\"><pre><div class=\"line\">trim.happened: fastq-dump.log</div><div class=\"line\">\t@<span class=\"built_in\">echo</span> <span class=\"string\">\"=== Running <span class=\"variable\">$@</span> ===\"</span></div><div class=\"line\">\t@<span class=\"built_in\">export</span> start=`date +%s`; \\</div><div class=\"line\">\tjava -jar <span class=\"variable\">$&#123;TRIMMOMATIC&#125;</span> PE -phred33 \\</div><div class=\"line\">\t  reads_1.fastq.gz reads_2.fastq.gz \\</div><div class=\"line\">\t  reads_1.trim.pe.fastq.gz reads_1.trim.se.fastq.gz \\</div><div class=\"line\">\t  reads_2.trim.pe.fastq.gz reads_2.trim.se.fastq.gz \\</div><div class=\"line\">\t  ILLUMINACLIP:<span class=\"variable\">$&#123;ADAPTERS&#125;</span>/TruSeq3-PE.fa:2:30:10 \\</div><div class=\"line\">\t  LEADING:3 TRAILING:3 SLIDINGWINDOW:4:20 MINLEN:36 &gt; <span class=\"variable\">$@</span>; \\</div><div class=\"line\">\t<span class=\"built_in\">export</span> end=`date +%s`; \\</div><div class=\"line\">\t<span class=\"built_in\">export</span> runtime=$$(($<span class=\"variable\">$end</span>-$<span class=\"variable\">$start</span>)); \\</div><div class=\"line\">\tcat <span class=\"variable\">$@</span>; \\</div><div class=\"line\">\t<span class=\"built_in\">echo</span> <span class=\"string\">\"Target <span class=\"variable\">$@</span> took $<span class=\"variable\">$runtime</span> seconds\"</span></div></pre></td></tr></table></figure>\n<p>Here, the stdout of trimmomatic is sent to <code>trim.happened</code>. Even if there is no stdout, an empty file will get made, and then the next run this rule can be skipped.</p>\n<h3 id=\"Metrics-2\"><a href=\"#Metrics-2\" class=\"headerlink\" title=\"Metrics\"></a>Metrics</h3><p>To get time spent metrics, I wrapped each command in a simple delta time calculation:</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div></pre></td><td class=\"code\"><pre><div class=\"line\">time:</div><div class=\"line\">\t@<span class=\"built_in\">echo</span> <span class=\"string\">\"=== Running <span class=\"variable\">$@</span> ===\"</span></div><div class=\"line\">\t@<span class=\"built_in\">export</span> start=`date +%s`; \\</div><div class=\"line\">\tsleep 2; \\</div><div class=\"line\">\t<span class=\"built_in\">export</span> end=`date +%s`; \\</div><div class=\"line\">\t<span class=\"built_in\">export</span> runtime=$$(($<span class=\"variable\">$end</span>-$<span class=\"variable\">$start</span>)); \\</div><div class=\"line\">\t<span class=\"built_in\">echo</span> <span class=\"string\">\"Target <span class=\"variable\">$@</span> took $<span class=\"variable\">$runtime</span> seconds\"</span></div></pre></td></tr></table></figure>\n<p>This clutters the code fast, and is unmaintainable. It might be possible to do it cleaner, but I did not spend time trying obscure solutions. The point is that there is no out of the box way to get task metrics with <code>make</code>. You can imagine how tricky it might be to also get RAM/CPU usage for each task.</p>\n<h3 id=\"Scaling-2\"><a href=\"#Scaling-2\" class=\"headerlink\" title=\"Scaling\"></a>Scaling</h3><p>Scaling with make is also difficult. There is no easy way to expand the pipeline to multiple species, and it is difficulty to variabilize outputs: targets need to strings.</p>\n<h2 id=\"snakemake\"><a href=\"#snakemake\" class=\"headerlink\" title=\"snakemake\"></a>snakemake</h2><p>The <code>Makefile</code> was much more organized than the bash script, but still of a low level.</p>\n<p>Snakemake was a refreshing take on the make style, but with many more features and powered by a high level language. Including Python integration everywhere, tasks to script in Python, Bash, and R, and some metrics out of the box. As well as some neat wildcarding. The complete Snakefile is <a href=\"https://github.com/bionode/gsoc16/blob/543ba66cbb42d1622089764bf01090e318307a57/pipelines/with-snakemake/Snakefile\" target=\"_blank\" rel=\"external\">here</a>.</p>\n<h3 id=\"Basic-Structure-3\"><a href=\"#Basic-Structure-3\" class=\"headerlink\" title=\"Basic Structure\"></a>Basic Structure</h3><p>You start a Snakemake workflow very similar to a Makefile, defining a global rule and what you want it to create:</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">rule all:</div><div class=\"line\">    input: FINAL_FILES</div></pre></td></tr></table></figure>\n<p>So what is <code>FINAL_FILES</code>? Since Snakemake has direct Python integration, we can actually compute it as a local variable. Heres the header for the <code>Snakefile</code> just before</p>\n<p><code>rule all</code>:</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div></pre></td><td class=\"code\"><pre><div class=\"line\">species = &#123;</div><div class=\"line\">    <span class=\"string\">'Salmonella-enterica'</span>: &#123;</div><div class=\"line\">        <span class=\"string\">'readsID'</span>: <span class=\"string\">'2492428'</span>,</div><div class=\"line\">        <span class=\"string\">'reference_url'</span>: <span class=\"string\">'http://ftp.ncbi.nlm.nih.gov/genomes/all/GCA_000988525.2_ASM98852v2/GCA_000988525.2_ASM98852v2_genomic.fna.gz'</span></div><div class=\"line\">    &#125;</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\">THREADS=<span class=\"number\">2</span></div><div class=\"line\">TEMP=<span class=\"string\">'./tmp'</span></div><div class=\"line\"></div><div class=\"line\">FINAL_FILES = [specie+<span class=\"string\">'.vcf'</span> <span class=\"keyword\">for</span> specie <span class=\"keyword\">in</span> species]</div></pre></td></tr></table></figure>\n<p>This will end up with <code>FINAL_FILES</code> being <code>[&#39;Salmonella-enterica.vcf&#39;]</code>.  See this rule which will be one of the first in the executed pipeline (here I hardcoded the specie name to better illustrate):</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div></pre></td><td class=\"code\"><pre><div class=\"line\">rule all:</div><div class=\"line\">    input: <span class=\"string\">'Salmonella-enterica.vcf'</span></div><div class=\"line\"></div><div class=\"line\">rule download_sra:</div><div class=\"line\">    output: <span class=\"string\">'&#123;specie&#125;.sra'</span></div><div class=\"line\">    run:</div><div class=\"line\">        readsID = species[wildcards.specie][<span class=\"string\">'readsID'</span>]</div><div class=\"line\">        shell(<span class=\"string\">'''</span></div><div class=\"line\">            bionode-ncbi download sra &#123;readsID&#125;;</div><div class=\"line\">            cp &#123;readsID&#125;/*.sra &#123;output&#125; &amp;&amp; rm -rf &#123;readsID&#125;;</div><div class=\"line\">        ''')       </div><div class=\"line\">        </div><div class=\"line\">rule call:</div><div class=\"line\">    input: <span class=\"string\">'&#123;specie&#125;.sra'</span></div><div class=\"line\">    output: <span class=\"string\">'&#123;specie&#125;.vcf'</span></div><div class=\"line\">    shell: <span class=\"string\">'magic &#123;input&#125; &gt; &#123;output&#125;'</span></div></pre></td></tr></table></figure>\n<p>The first rule will be triggered and will be looking for another rule that has <code>Salmonella-enterica.vcf</code> in its <code>output</code>. It wont find <em>exactly</em> that, but because of Snakemakes wildcarding, <code>{specie}.vcf</code> will do (from <code>rule call</code>), and then within the <code>call</code> rule, the value of <code>wildcards.specie</code> will be <code>Salmonella-enterica</code>. Then it will move onto <code>download_sra</code>. This example also illustrates how you can use <code>run</code> and <code>shell()</code> to mix Python and shell code. You can also use <code>script</code> to run a Python or R script, and use the <code>R()</code> function to execute R code in a rule.</p>\n<p>Another neat feature with Snakemake is the ability to drop in wrappers:</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div></pre></td><td class=\"code\"><pre><div class=\"line\">rule bwa_mem:</div><div class=\"line\">    input:</div><div class=\"line\">        ref = <span class=\"string\">'&#123;specie&#125;.genomic.fna.gz'</span>,</div><div class=\"line\">        sample = [<span class=\"string\">'&#123;specie&#125;_'</span>+num+<span class=\"string\">'.fastq.gz'</span> <span class=\"keyword\">for</span> num <span class=\"keyword\">in</span> [<span class=\"string\">'1'</span>, <span class=\"string\">'2'</span>]],</div><div class=\"line\">        index_files = [<span class=\"string\">'&#123;specie&#125;.genomic.fna.gz.'</span>+suffix <span class=\"keyword\">for</span> suffix <span class=\"keyword\">in</span> [<span class=\"string\">'amb'</span>, <span class=\"string\">'ann'</span>, <span class=\"string\">'bwt'</span>, <span class=\"string\">'pac'</span>, <span class=\"string\">'sa'</span>]]</div><div class=\"line\">    log: <span class=\"string\">'logs/bwa_mem/&#123;specie&#125;.log'</span></div><div class=\"line\">    output: <span class=\"string\">'&#123;specie&#125;.sam'</span></div><div class=\"line\">    threads: THREADS</div><div class=\"line\">    wrapper: <span class=\"string\">'0.0.8/bio/bwa_mem'</span></div></pre></td></tr></table></figure>\n<p>These wrappers come from the <a href=\"https://bitbucket.org/snakemake/snakemake-wrappers\" target=\"_blank\" rel=\"external\">wrappers repository</a>. They run predefined commands using specific input variables. While this is great, there is some overhead in having to check the source to see what is actually happening, and by extension, you then need an internet connection to do so. There does not seem to be a huge list of wrappers, it is too bad it does not seem to have caught on extensively.</p>\n<p>Still have the problem of using log/flag files for task dependency, but less so, since you can define custom outputs, also has touch(flag) built in</p>\n<p>Could not figure out how to ecomical branching, wildcard regexes create ambiguity:</p>\n<p><img src=\"https://raw.githubusercontent.com/bionode/gsoc16/fdf22b630e33dd11302ea8822c547ef9399c3ea4/pipelines/with-snakemake/dag.png\" alt=\"attempt-branching\"></p>\n<p>Here it unnecessarily redoes certain steps. This is because <code>Salmonella-enterica.trim.vcf</code> and <code>Salmonella-enterica.trim.vcf</code> both match <code>{specie}.vcf</code>. At first, Snakemake complained about ambiguous rules due to this conflict. I was able to generate the DAG above by using custom regexes for the wildcard in a few rules: <code>{specie,[a-zA-z-]+}</code>. However, since this then creates one set of rules with <code>wildcard.specie = Salmonella-enterica</code> and another with <code>wildcard.specie = Salmonella-enterica.trim</code>, and then that wildcard moves all the way to the first rule, and basically two pipelines are ran. I was not able to figure out how optimize this using Snakemake - if anyone can, let me know!</p>\n<p>Moreover - because the <strong>pull</strong> workflow style depends on <em>targets</em> and <em>prerequisites</em>, it can be difficult to achieve a branch-merge pipeline. We will see with Nextflow, which follows the dataflow paradigm, or <strong>push</strong>, it is natural to describe such pipelines.</p>\n<p>For a more complicated (and real world) snakemake setup, take a look at <a href=\"https://github.com/pachterlab/kallisto_paper_analysis/blob/nbt/Snakefile\" target=\"_blank\" rel=\"external\">pachterlab/kallisto_paper_analysis/Snakefile</a>.</p>\n<h3 id=\"Iterative-Development-3\"><a href=\"#Iterative-Development-3\" class=\"headerlink\" title=\"Iterative Development\"></a>Iterative Development</h3><p>Iterative development in Snakemake is about the same as with <code>make</code>. With wildcarding, the clean rule can be made more specific, for example:</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\">rule clean:</div><div class=\"line\">    input: <span class=\"string\">'&#123;specie&#125;.vcf'</span></div><div class=\"line\">    shell: <span class=\"string\">'''</span></div><div class=\"line\">    \trm &#123;specie&#125;.sra &#123;specie&#125;.bam;</div><div class=\"line\">    '''</div></pre></td></tr></table></figure>\n<p>As opposed to <code>rm *.sra *.bam</code>. </p>\n<h3 id=\"Metrics-3\"><a href=\"#Metrics-3\" class=\"headerlink\" title=\"Metrics\"></a>Metrics</h3><p>You can opt in to store rule benchmarks and logs:</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div></pre></td><td class=\"code\"><pre><div class=\"line\">rule call:</div><div class=\"line\">    input:</div><div class=\"line\">        bam = <span class=\"string\">'&#123;specie&#125;.bam'</span>,</div><div class=\"line\">        alignment_index = <span class=\"string\">'&#123;specie&#125;.bam.bai'</span>,</div><div class=\"line\">        reference = <span class=\"string\">'&#123;specie&#125;.genomic.fna'</span></div><div class=\"line\">    output: <span class=\"string\">'&#123;specie&#125;.vcf'</span></div><div class=\"line\">    log: <span class=\"string\">'logs/call/&#123;specie&#125;.log'</span></div><div class=\"line\">    benchmark: <span class=\"string\">'benchmarks/call/&#123;specie&#125;.txt'</span></div><div class=\"line\">    shell: <span class=\"string\">'samtools mpileup -uf &#123;input.reference&#125; &#123;input.bam&#125; | bcftools call -c - &gt; &#123;output&#125;'</span></div></pre></td></tr></table></figure>\n<p>However, you will need to make sure you name these files appropiately to avoid overwriting. An example benchmark file looks like:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">s\th:m:s</div><div class=\"line\">30.32477617263794\t0:00:30.324776</div></pre></td></tr></table></figure>\n<p>There is no indication of which task the benchmark is for, meaning if you want a comprehensive report of your pipeline you will need to write a script that concatenates the benchmarks in the correct order.</p>\n<p>With graphviz installed, snakemake can create a plot of the DAG for your pipeline:</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">snakemake --dag | dot -Tpng &gt; dag.png</div></pre></td></tr></table></figure>\n<p>You can see an example of this above.</p>\n<h3 id=\"Scaling-3\"><a href=\"#Scaling-3\" class=\"headerlink\" title=\"Scaling\"></a>Scaling</h3><p>With Snakemake it is easy to scale to multiple species. In my Snakefile, I simply just add more keys to the <code>species</code> dictionary. Since each output file is prefixed by the species name, there will be no file overlaps. You need to be vigilant with output names and ensure yourself there will be no conflicts.</p>\n<p>You can specify clustering configuration by writing a <code>cluster.config</code> file and using its values in the call to Snakemake. The example <a href=\"https://bitbucket.org/snakemake/snakemake/wiki/Documentation#markdown-header-cluster-configuration\" target=\"_blank\" rel=\"external\">from the documentation</a>:</p>\n<p><em>Snakefile</em></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div></pre></td><td class=\"code\"><pre><div class=\"line\">rule all:</div><div class=\"line\">    input: <span class=\"string\">\"input1.txt\"</span>, <span class=\"string\">\"input2.txt\"</span></div><div class=\"line\"></div><div class=\"line\">rule compute1:</div><div class=\"line\">    output: <span class=\"string\">\"input1.txt\"</span></div><div class=\"line\">    shell: <span class=\"string\">\"touch input1.txt\"</span></div><div class=\"line\"></div><div class=\"line\">rule compute2:</div><div class=\"line\">    output: <span class=\"string\">\"input2.txt\"</span></div><div class=\"line\">    shell: <span class=\"string\">\"touch input2.txt\"</span></div></pre></td></tr></table></figure>\n<p><em>cluster.json</em></p>\n<figure class=\"highlight json\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div></pre></td><td class=\"code\"><pre><div class=\"line\">&#123;</div><div class=\"line\">    <span class=\"attr\">\"__default__\"</span> :</div><div class=\"line\">    &#123;</div><div class=\"line\">        <span class=\"attr\">\"account\"</span> : <span class=\"string\">\"my account\"</span>,</div><div class=\"line\">        <span class=\"attr\">\"time\"</span> : <span class=\"string\">\"00:15:00\"</span>,</div><div class=\"line\">        <span class=\"attr\">\"n\"</span> : <span class=\"number\">1</span>,</div><div class=\"line\">        <span class=\"attr\">\"partition\"</span> : <span class=\"string\">\"core\"</span></div><div class=\"line\">    &#125;,</div><div class=\"line\">    <span class=\"attr\">\"compute1\"</span> :</div><div class=\"line\">    &#123;</div><div class=\"line\">        <span class=\"attr\">\"time\"</span> : <span class=\"string\">\"00:20:00\"</span></div><div class=\"line\">    &#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<p>And the call to Snakemake:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">snakemake -j 999 --cluster-config cluster.json --cluster &quot;sbatch -A &#123;cluster.account&#125; -p &#123;cluster.partition&#125; -n &#123;cluster.n&#125;  -t &#123;cluster.time&#125;&quot;</div></pre></td></tr></table></figure>\n<p>While nextflow is providing a simple config based way to pass params into your clustering command, the actual command: <code>sbatch</code> is written manually. We will say with Nextflow, even this aspect of clustering is abstracted away (into executor). See also <a href=\"https://bitbucket.org/snakemake/snakemake/wiki/Documentation#markdown-header-job-properties\" target=\"_blank\" rel=\"external\">job properties</a> to see how to write a generic job wrapper for clustering.</p>\n<h2 id=\"nextflow\"><a href=\"#nextflow\" class=\"headerlink\" title=\"nextflow\"></a>nextflow</h2><p>Nextflow is a more recent tool, and approaches the workflow problem in a <strong>push</strong> sense. Similar to how Snakemake allows direct Python integration, Nextflow uses <a href=\"http://www.groovy-lang.org/\" target=\"_blank\" rel=\"external\">Groovy</a>. While Snakemake let you write rules with inline Python, R, and shell, in Nextflow, you <em>can use any scripting language</em>. See <a href=\"https://github.com/bionode/gsoc16/blob/543ba66cbb42d1622089764bf01090e318307a57/pipelines/with-nextflow/main.nf\" target=\"_blank\" rel=\"external\">main.nf</a> for the complete pipeline.</p>\n<p>Nextflow follows the <a href=\"https://en.wikipedia.org/wiki/Dataflow_programming\" target=\"_blank\" rel=\"external\">dataflow programming paradigm</a> (push) which is also called <a href=\"https://en.wikipedia.org/wiki/Stream_processing\" target=\"_blank\" rel=\"external\">stream processing</a> or <a href=\"https://en.wikipedia.org/wiki/Reactive_programming\" target=\"_blank\" rel=\"external\">reactive programming</a>. While in Snakemake you define <code>rule all</code>s prerequisites, and then work backwards from there (pull), in the dataflow model you can think and write in the order that tasks will happen. Since the push model does not depend on inferring a complete dependency tree at the initialization, you can introduce dynamic structures into the pipeline. For example, choosing between two tasks to run <em>based on the output</em> of a previous task. One fallback with the push model is that since it can be dynamic, it is difficult to then perform a dry run, or generate a DAG ahead of time.</p>\n<h3 id=\"Basic-Structure-4\"><a href=\"#Basic-Structure-4\" class=\"headerlink\" title=\"Basic Structure\"></a>Basic Structure</h3><p>Nextflow uses <strong>processes</strong> as rules, and each process will occur in its own folder in <code>/work</code>. Since each process takes place in its own folder, using output from one task as input in another cannot be done the conventional way. But that is alright, because Nextflow provides <strong>channels</strong> to communicate between processes. </p>\n<p>Consider:</p>\n<figure class=\"highlight groovy\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"meta\">#!/usr/bin/env nextflow</span></div><div class=\"line\"></div><div class=\"line\">species = [</div><div class=\"line\">  <span class=\"string\">'Salmonella-enterica'</span>: [</div><div class=\"line\">    <span class=\"string\">'referenceURL'</span>: <span class=\"string\">'http://ftp.ncbi.nlm.nih.gov/genomes/all/GCA_000988525.2_ASM98852v2/GCA_000988525.2_ASM98852v2_genomic.fna.gz'</span>,</div><div class=\"line\">    <span class=\"string\">'readsID'</span>: <span class=\"string\">'2492428'</span></div><div class=\"line\">  ]</div><div class=\"line\">]</div><div class=\"line\"></div><div class=\"line\">process downloadSRA &#123;</div><div class=\"line\">  container <span class=\"string\">'bionode/bionode-ncbi'</span></div><div class=\"line\"><span class=\"symbol\"></span></div><div class=\"line\">  input: val readsID from species.collect &#123; it.value.readsID &#125;</div><div class=\"line\"><span class=\"symbol\">  output:</span> file <span class=\"string\">'**/*.sra'</span> into reads</div><div class=\"line\"></div><div class=\"line\">  <span class=\"string\">\"\"\"</span></div><div class=\"line\">  bionode-ncbi download sra $readsID &gt; tmp</div><div class=\"line\">  \"\"\"</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\">process extractSRA &#123;</div><div class=\"line\">  container <span class=\"string\">'inutano/sra-toolkit'</span></div><div class=\"line\"><span class=\"symbol\"></span></div><div class=\"line\">  input: file read from reads</div><div class=\"line\"><span class=\"symbol\">  output:</span> file <span class=\"string\">'*.fastq.gz'</span> into samples</div><div class=\"line\"></div><div class=\"line\">  <span class=\"string\">\"\"\"</span></div><div class=\"line\">  fastq-dump --split-files --skip-technical --gzip $read</div><div class=\"line\">  \"\"\"</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<p>See the documentation on [processes] for more. As well, there are more types of channels than <code>val</code> and <code>file</code>, see [channels]. If you are new to [Groovy], and <code>species.collect { it.value.readsID }</code> confuses you, think of it as <code>species.map(specie =&gt; specie.value.readsID)</code>.  It is important to note that while channels may appear confusing at first (why not just use filenames?), they are an elegant solution to a problem we had with Snakemake: file name overlap. With Nextflow, you can be guarenteed no files will ever overwrite each other (unless you do so yourself in one process). As well, this abstracts away the filename, making commands appear generalized:</p>\n<figure class=\"highlight groovy\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div></pre></td><td class=\"code\"><pre><div class=\"line\">process indexReference &#123;</div><div class=\"line\">  container <span class=\"string\">'biodckr/bwa'</span></div><div class=\"line\"><span class=\"symbol\"></span></div><div class=\"line\">  input: file reference from referenceGenomeGz2</div><div class=\"line\"><span class=\"symbol\">  output:</span> file <span class=\"string\">'*.gz.*'</span> into referenceIndexes</div><div class=\"line\"></div><div class=\"line\">  <span class=\"string\">\"\"\"</span></div><div class=\"line\">  bwa index $reference</div><div class=\"line\">  \"\"\"</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<p>There is no mental overhead from managing <code>{specie}.genomic.fna.gz</code>, instead we can simply use the variable <code>$reference</code>. This makes the commands in our processes easy to read and comprehend. We can even hardcode output file names in an <em>extremely general way</em> without any fear of file overlap:</p>\n<figure class=\"highlight groovy\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div></pre></td><td class=\"code\"><pre><div class=\"line\">process decompressReference &#123;</div><div class=\"line\">  container <span class=\"string\">'biodckrdev/htslib'</span></div><div class=\"line\"><span class=\"symbol\"></span></div><div class=\"line\">  input: file referenceGenome from referenceGenomeGz1</div><div class=\"line\"><span class=\"symbol\">  output:</span> file <span class=\"string\">'reference.genomic.fna'</span> into referenceGenomes</div><div class=\"line\"></div><div class=\"line\">  <span class=\"string\">\"\"\"</span></div><div class=\"line\">  bgzip -d $referenceGenome --stdout &gt; reference.genomic.fna</div><div class=\"line\">  \"\"\"</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<p>Here we output to <code>reference.genomic.fna</code>. A filename that would surely cause problems in any other worflow system.</p>\n<p>Something else to discuss with Nextflow is how to handle a channel being consumed by multiple processes. Since a channel is a FIFO queue in Nextflow, once one process uses it, it is emptied and cannot be consumed by another process. The way to get around this is to create the original channel called <code>myOutput</code> and then clone it via <code>into</code> $n$ channels to be consumed by $n$ processes:</p>\n<figure class=\"highlight groovy\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div></pre></td><td class=\"code\"><pre><div class=\"line\">process downloadReference &#123;</div><div class=\"line\">  container <span class=\"literal\">true</span></div><div class=\"line\"><span class=\"symbol\"></span></div><div class=\"line\">  input: val referenceURL from species.collect &#123; it.value.referenceURL &#125;</div><div class=\"line\"><span class=\"symbol\">  output:</span> file <span class=\"string\">'reference.genomic.fna.gz'</span> into referenceGenomeGz</div><div class=\"line\"></div><div class=\"line\">  <span class=\"string\">\"\"\"</span></div><div class=\"line\">  appropriate/curl $referenceURL -o reference.genomic.fna.gz</div><div class=\"line\">  \"\"\"</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// fork reference genome into three other channels</span></div><div class=\"line\">( referenceGenomeGz1,</div><div class=\"line\">  referenceGenomeGz2,</div><div class=\"line\">  referenceGenomeGz3 ) = referenceGenomeGz.into(<span class=\"number\">3</span>)</div></pre></td></tr></table></figure>\n<p>This is also an example of an <em>executable container</em>, where the Dockerfile ends in <code>CMD [&quot;curl&quot;]</code>. </p>\n<p>However, I encountered some issues when trying to dockerize everything. TODO talk about dockerizing processes in a pipe.</p>\n<p>Finally, wherease Snakemake only allowed Python and R inside rules, with Nextflow you can use any scripting language.</p>\n<h3 id=\"Iterative-Development-4\"><a href=\"#Iterative-Development-4\" class=\"headerlink\" title=\"Iterative Development\"></a>Iterative Development</h3><p>Iterative developmen with Nextflow is much improved over the other tools. I felt as though I could extend the pipeline, adding new processes, without having to worry about updating a <code>rule all</code> rule, or cleaning up old files. Nextflow seamlessly recognizes cached process results when given the <code>-resume</code> flag.</p>\n<p>Debugging errors in processes works great. When an error occurs, you can <code>cd</code> into the relevant <code>/work</code> directory, and Nextflow provides log files, and a file that describes which command exactly was ran. This allows you to debug the error in the same environment as it will be ran, and then resume the workflow from that point.</p>\n<h3 id=\"Metrics-4\"><a href=\"#Metrics-4\" class=\"headerlink\" title=\"Metrics\"></a>Metrics</h3><p>Nextflow can create timeline charts:</p>\n<p><img src=\"https://github.com/bionode/gsoc16/raw/543ba66cbb42d1622089764bf01090e318307a57/pipelines/with-nextflow/timeline.png\" alt=\"nextflow-timeline\"></p>\n<p>and a DAG diagram of the pipeline (also notice I was able to create a forking pipeline unlike with Snakemake):</p>\n<p><img src=\"https://github.com/bionode/gsoc16/raw/543ba66cbb42d1622089764bf01090e318307a57/pipelines/with-nextflow/workflow.png\" alt=\"nextflow-workflow\"></p>\n<h3 id=\"Scaling-4\"><a href=\"#Scaling-4\" class=\"headerlink\" title=\"Scaling\"></a>Scaling</h3><p>You might have noticed the <code>container</code> <a href=\"http://www.nextflow.io/docs/latest/process.html#directives\" target=\"_blank\" rel=\"external\">directive</a> in the above processes. Nextflow comes with built-in docker integration, which is great. <em>Each process can run in its own container</em>. This helps improve the portability and reproducibiltiy of the workflow. All a consumer needs installed on their system is Docker and Nextflow (and you can even run Nextflow in Docker). Each container can use a tagged version, ensuring when someone else runs the pipeline, they are using the <em>exact same version of each tool</em>. You could, of course use Docker in your Snakemake commands, but there would be overhead from volume mounting (mapping a local folder to a folder inside the container); Nextflow handles all that for you. As well, Nextflow has built in support for many cluster engines, which can be enabled by defining the <code>executor</code> in <code>nextflow.config</code>. Another feature to not is Nextflows integration with GitHub. With a <code>main.nf</code> in your repo, you can run a pipeline with <code>nextflow run username/repo</code>. </p>\n<p>Finally, be sure to check out <a href=\"https://github.com/nextflow-io/awesome-nextflow\" target=\"_blank\" rel=\"external\">awesome-nextflow</a>!</p>\n<h2 id=\"others\"><a href=\"#others\" class=\"headerlink\" title=\"others\"></a>others</h2><p><a href=\"https://github.com/pditommaso/awesome-pipeline\" target=\"_blank\" rel=\"external\">awesome-pipeline</a> is a great list of many pipeline and workflow tools. Here are some I think are more relevant, and a few extras (though you should try to look through all of them, at least look at these):</p>\n<ul>\n<li><a href=\"http://anduril.org/userguide/\" target=\"_blank\" rel=\"external\">anduril</a> - Component-based workflow framework for scientific data analysis</li>\n<li><a href=\"https://www.antha-lang.org/docs/concepts/flow-based-programming.html\" target=\"_blank\" rel=\"external\">antha</a> - High-level language for biology</li>\n</ul>\n<ul>\n<li><a href=\"http://docs.bpipe.org/Examples/PairedEndAlignment/\" target=\"_blank\" rel=\"external\">bpipe</a> - Tool for running and managing bioinformatics pipelines</li>\n<li><a href=\"https://github.com/tburdett/Conan2\" target=\"_blank\" rel=\"external\">Conan2</a> - Light-weight workflow management application</li>\n<li><a href=\"https://github.com/LPM-HMS/COSMOS2\" target=\"_blank\" rel=\"external\">cosmos</a> - Python library for massively parallel workflows</li>\n<li><a href=\"https://github.com/Factual/drake\" target=\"_blank\" rel=\"external\">drake</a> - Robust DSL akin to Make, implemented in Clojure</li>\n<li><a href=\"http://kronos.readthedocs.io/en/latest/\" target=\"_blank\" rel=\"external\">kronos</a> - Workflow assembler for cancer genome analytics and informatics</li>\n<li><a href=\"http://kronos.readthedocs.io/en/latest/\" target=\"_blank\" rel=\"external\">loom</a> - Tool for running bioinformatics workflows locally or in the cloud</li>\n<li><a href=\"http://moa.readthedocs.io/en/latest/\" target=\"_blank\" rel=\"external\">moa</a> - Lightweight workflows in bioinformatics</li>\n<li><a href=\"https://github.com/adaptivegenome/openge\" target=\"_blank\" rel=\"external\">OpenGE</a> - Accelerated framework for manipulating and interpreting high-throughput sequencing data</li>\n<li><a href=\"https://github.com/fstrozzi/bioruby-pipengine#-the-pipeline-yaml-\" target=\"_blank\" rel=\"external\">pipengine</a> - Ruby based launcher for complex biological pipelines</li>\n<li><a href=\"http://www.ruffus.org.uk/\" target=\"_blank\" rel=\"external\">ruffus</a> - Computation Pipeline library for Python</li>\n<li><a href=\"http://opensource.nibr.com/yap/\" target=\"_blank\" rel=\"external\">YAP</a> - Extensible parallel framework, written in Python using OpenMPI libraries</li>\n</ul>\n<ul>\n<li><a href=\"http://clusterflow.io/examples/\" target=\"_blank\" rel=\"external\">clusterflow</a> - Command-line tool which uses common cluster managers to run bioinformatics pipelines</li>\n</ul>\n<ul>\n<li><a href=\"https://github.com/Ensembl/ensembl-hive\" target=\"_blank\" rel=\"external\">hive</a> - System for creating and running pipelines on a distributed compute resource</li>\n<li><a href=\"https://cloud.google.com/dataflow/docs/\" target=\"_blank\" rel=\"external\">google cloud dataflow</a> - unified programming model and a managed service for developing and executing a wide range of data processing patterns including ETL, batch computation, and continuous computation</li>\n<li><a href=\"http://ccl.cse.nd.edu/software/makeflow/\" target=\"_blank\" rel=\"external\">makeflow</a> - Workflow engine for executing large complex workflows on clusters</li>\n<li><a href=\"https://github.com/soravux/scoop/\" target=\"_blank\" rel=\"external\">scoop</a> - Scalable Concurrent Operations in Python</li>\n<li><a href=\"https://github.com/spotify/luigi\" target=\"_blank\" rel=\"external\">luigi</a> and <a href=\"https://github.com/pharmbio/sciluigi\" target=\"_blank\" rel=\"external\">sciluigi</a> - Python module that helps you build complex pipelines of batch jobs, luigi wrapper for scientific workflows</li>\n</ul>\n<ul>\n<li><a href=\"https://usegalaxy.org/\" target=\"_blank\" rel=\"external\">galaxy</a> - Web-based platform for biomedical research</li>\n<li><a href=\"https://taverna.incubator.apache.org/introduction/\" target=\"_blank\" rel=\"external\">taverna</a> - Domain independent workflow system</li>\n</ul>\n<ul>\n<li><a href=\"https://github.com/rabix/rabix\" target=\"_blank\" rel=\"external\">rabix</a> - implementation of CWL2</li>\n</ul>\n<ul>\n<li><a href=\"https://github.com/joergen7/cuneiform\" target=\"_blank\" rel=\"external\">cuneiform</a> - Advanced functional workflow language and framework, implemented in Erlang</li>\n<li><a href=\"http://ccl.cse.nd.edu/software/makeflow/\" target=\"_blank\" rel=\"external\">mario</a> - Scala library for defining data pipelines</li>\n</ul>\n<ul>\n<li><a href=\"https://github.com/apache/incubator-airflow\" target=\"_blank\" rel=\"external\">airflow</a> - Python-based workflow system created by AirBnb</li>\n<li><a href=\"https://github.com/pinterest/pinball\" target=\"_blank\" rel=\"external\">pinball</a> - Python based workflow engine by Pinterest</li>\n</ul>\n<ul>\n<li><a href=\"https://aws.amazon.com/datapipeline/\" target=\"_blank\" rel=\"external\">AWS Data Pipeline</a> and <a href=\"https://aws.amazon.com/swf/\" target=\"_blank\" rel=\"external\">SWF</a> - data workflow orchestration</li>\n<li><a href=\"http://dray.it/\" target=\"_blank\" rel=\"external\">DRAY: Docker Workflow Engine</a> - UNIX pipes for Docker</li>\n<li><a href=\"https://github.com/BD2KGenomics/toil\" target=\"_blank\" rel=\"external\">toil</a> - CWL3 and WDL support</li>\n<li><a href=\"https://github.com/chapmanb/bcbio-nextgen\" target=\"_blank\" rel=\"external\">bcbio</a> -  best-practice pipelines for automated analysis of high throughput sequencing data</li>\n<li><a href=\"https://github.com/pachterlab/kallisto/blob/master/gulpfile.js\" target=\"_blank\" rel=\"external\">example gulpfile simple pipeline</a></li>\n<li><a href=\"https://github.com/scipipe/scipipe\" target=\"_blank\" rel=\"external\">scipipe</a> - workflow system in Go inspired by Flow-based programming</li>\n<li><a href=\"https://github.com/agmen-hu/node-datapumps\" target=\"_blank\" rel=\"external\">node-datapumps</a> - Node.js ETL toolkit for easy data import, export or transfer between systems</li>\n</ul>\n<p>Quite a lot. </p>\n<h2 id=\"Conclusion\"><a href=\"#Conclusion\" class=\"headerlink\" title=\"Conclusion\"></a>Conclusion</h2><p>On a scale from 1-5, these are my ratings for each tool. Mostly as relative to each other, rather than absolutely.</p>\n<table>\n<thead>\n<tr>\n<th>Tool</th>\n<th>Structure</th>\n<th>Iterative Dev.</th>\n<th>Metrics</th>\n<th>Scale</th>\n<th>Reproducibility</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>bash</td>\n<td>1</td>\n<td>1</td>\n<td>1</td>\n<td>1</td>\n<td>1</td>\n</tr>\n<tr>\n<td>make</td>\n<td>2</td>\n<td>2</td>\n<td>1</td>\n<td>1</td>\n<td>1</td>\n</tr>\n<tr>\n<td>Snakemake</td>\n<td>4</td>\n<td>3</td>\n<td>3</td>\n<td>3</td>\n<td>4</td>\n</tr>\n<tr>\n<td>Nextflow</td>\n<td>4</td>\n<td>5</td>\n<td>5</td>\n<td>5</td>\n<td>5</td>\n</tr>\n</tbody>\n</table>\n<p>I have not investigated the others from above to warrant their ranking.</p>\n<p>A quick summary:</p>\n<p><strong>bash</strong></p>\n<ul>\n<li>generic scripting</li>\n<li>simple variable interpolation - <code>$foo</code></li>\n<li>no metrics, difficult to scale</li>\n<li>no reentrancy, poor iterative development</li>\n</ul>\n<p><strong>make</strong></p>\n<ul>\n<li>structure more pipeline oriented than bash - tasks with <em>input</em> (prerequisites) and <em>output</em> (target)</li>\n<li>introduces some element of reentrancy by skipping rules whose target already exists = improved iterative development over bash</li>\n<li>can fork by defining variables that change a rules prerequisites, and running again</li>\n<li>no metrics, difficult to scale</li>\n</ul>\n<p><strong>Snakemake</strong></p>\n<ul>\n<li>enhanced scripting over bash/make with Python while using makes well known rules structure</li>\n<li>improved <code>{input}</code>, <code>{output}</code> syntax and power (e.g. <code>{input.a}</code> or <code>{input[n]}</code>) over make</li>\n<li>run Python, shell, or R in rule</li>\n<li>enhanced wildcarding: easier to understand, more powerful than makes <code>%</code></li>\n<li>benchmarks are OK - need to do manual work to get a full pipeline report</li>\n<li>DAG is nice</li>\n<li>dry run is nice (and possible because of pull paradigm)</li>\n<li>can scale to multiple species easily, just need to make sure file names do not overlap</li>\n<li>cannot create forking paths (e.g. use same reference for two different filtering tools). Could do it by changing <code>FINAL_OUTPUT</code> and running again, but then thats basically the same as it was in <code>make</code>. (again, if you know how to do this without wildcarding ambiguity - let me know!)</li>\n</ul>\n<p><strong>Nextflow</strong></p>\n<ul>\n<li>dataflow paradigm - push</li>\n<li>as a consequence of push, no dry run</li>\n<li>as a consequence of push, flexible forking and dynamic pipelines</li>\n<li>scale to multiple species effortlessly with no overlapping file name concerns</li>\n<li>timeline, DAG, benchmarks metrics are all very nice</li>\n<li>Docker is nice - improves reproducibility and shareability</li>\n<li>cannot handle large <code>stdout | stdin</code> pipes across channels (e.g. one process for <code>bwa mem</code> and another for <code>samtools view</code>)</li>\n</ul>\n<p><strong>bionode-waterwheel</strong> <em>proposal</em></p>\n<ul>\n<li><p>no DSL to learn - just JavaScript. Functional, built around async, events, and streams.</p>\n</li>\n<li><p>streams resonate well with <em>push</em> paradigm</p>\n</li>\n<li><p>support for piping <code>stdin</code>, <code>stdout</code> around, e.g. <code>bwa.mem().pipe(samtools.view())</code> . This enhances modularity of pipeline, rather than having one rule or process doing <code>bwa mem | samtools view</code>, which is really two commands, each deserving their own input, output, parameter definitions. This can aid in improving reproducibility and modularity, as tools with defined input, output, params could be confidentally dropped into the pipeline.</p>\n</li>\n<li><p>moduler interoperation with web apps, native apps (Electron), services (Slack, email). Imagine pipeline logs being sent over a websocket to your browser.</p>\n</li>\n<li><p>interoperation with npm ecosystem</p>\n</li>\n<li><p>integrate with CWL spec</p>\n</li>\n<li><p>metrics will be easier to consume since output will be JSON to be consumed by d3, browser apps, etc</p>\n</li>\n<li><p>modular, specific, customizable waterwheel backend can be integrated into browser or native apps, bringing more power to pipeline GUIs than Galaxy for instance, where it is complicated to develop a custom pipeline (some labs hire someone to create a custom xml config for their specific pipeline so that the wet lab can click to play in Galaxy)</p>\n</li>\n<li><p>variable interpolation - like ES6 template literals, which can take any valid JS expression:</p>\n<figure class=\"highlight javascript\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">const</span> PORT = config.PORT</div><div class=\"line\"><span class=\"keyword\">const</span> template = <span class=\"string\">`Server listening on port <span class=\"subst\">$&#123;PORT&#125;</span>`</span></div></pre></td></tr></table></figure>\n<p>which can actually evaluate any JS expression:</p>\n<figure class=\"highlight javascript\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">[<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">4</span>].map(num =&gt; <span class=\"string\">`is even: <span class=\"subst\">$&#123;num % 2 === 0 ? 'true' : 'false'&#125;</span>`</span>)</div></pre></td></tr></table></figure>\n</li>\n<li><p>simple examples with small datasets can be browser compatibile (see: <a href=\"https://github.com/charto/nbind\" target=\"_blank\" rel=\"external\">nbind</a>). Live browser examples are great for education - run your own NGS pipeline, from the browser</p>\n</li>\n<li>how to handle interative development is an open question. Can fork all streams into files for reentrancy while in a develop mode. </li>\n</ul>\n<p>A draft pipeline with <strong>bionode-waterwheel</strong>:</p>\n<figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div><div class=\"line\">59</div><div class=\"line\">60</div><div class=\"line\">61</div><div class=\"line\">62</div><div class=\"line\">63</div><div class=\"line\">64</div><div class=\"line\">65</div><div class=\"line\">66</div><div class=\"line\">67</div><div class=\"line\">68</div><div class=\"line\">69</div><div class=\"line\">70</div><div class=\"line\">71</div><div class=\"line\">72</div><div class=\"line\">73</div><div class=\"line\">74</div><div class=\"line\">75</div><div class=\"line\">76</div><div class=\"line\">77</div><div class=\"line\">78</div><div class=\"line\">79</div><div class=\"line\">80</div><div class=\"line\">81</div><div class=\"line\">82</div><div class=\"line\">83</div><div class=\"line\">84</div><div class=\"line\">85</div><div class=\"line\">86</div><div class=\"line\">87</div><div class=\"line\">88</div><div class=\"line\">89</div><div class=\"line\">90</div><div class=\"line\">91</div><div class=\"line\">92</div><div class=\"line\">93</div><div class=\"line\">94</div><div class=\"line\">95</div><div class=\"line\">96</div><div class=\"line\">97</div><div class=\"line\">98</div><div class=\"line\">99</div><div class=\"line\">100</div><div class=\"line\">101</div><div class=\"line\">102</div><div class=\"line\">103</div><div class=\"line\">104</div><div class=\"line\">105</div><div class=\"line\">106</div><div class=\"line\">107</div><div class=\"line\">108</div><div class=\"line\">109</div><div class=\"line\">110</div><div class=\"line\">111</div><div class=\"line\">112</div><div class=\"line\">113</div><div class=\"line\">114</div><div class=\"line\">115</div><div class=\"line\">116</div><div class=\"line\">117</div><div class=\"line\">118</div><div class=\"line\">119</div><div class=\"line\">120</div><div class=\"line\">121</div><div class=\"line\">122</div><div class=\"line\">123</div><div class=\"line\">124</div><div class=\"line\">125</div><div class=\"line\">126</div><div class=\"line\">127</div><div class=\"line\">128</div><div class=\"line\">129</div><div class=\"line\">130</div><div class=\"line\">131</div><div class=\"line\">132</div><div class=\"line\">133</div><div class=\"line\">134</div><div class=\"line\">135</div><div class=\"line\">136</div><div class=\"line\">137</div><div class=\"line\">138</div><div class=\"line\">139</div><div class=\"line\">140</div><div class=\"line\">141</div><div class=\"line\">142</div><div class=\"line\">143</div><div class=\"line\">144</div><div class=\"line\">145</div></pre></td><td class=\"code\"><pre><div class=\"line\">const ncbi = require('bionode-ncbi')</div><div class=\"line\">const wrapper = require('bionode-wrapper')</div><div class=\"line\">const waterwheel = require('bionode-waterwheel')</div><div class=\"line\"></div><div class=\"line\">const &#123; task, join, run &#125; = waterwheel</div><div class=\"line\">const &#123; stdout, stdin, file, directory &#125; = waterwheel.types</div><div class=\"line\"></div><div class=\"line\">// Can be passed in from CLI</div><div class=\"line\">// params for final pipeline call. params are things that do not</div><div class=\"line\">// change how items are passed between processes, but decide output for the</div><div class=\"line\">// pipeline as a whole. For example, species name and reads accession.</div><div class=\"line\">const pipelineParams = &#123;</div><div class=\"line\">  specie: 'Salmonella-enterica',</div><div class=\"line\">  readsID: '2492428',</div><div class=\"line\">  output: 'Salmonella-enterica.vcf'</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\">const sra = task(&#123;</div><div class=\"line\">  output: stdout()</div><div class=\"line\">&#125;, ncbi.download('sra', '&#123;params.readsID&#125;'))</div><div class=\"line\"></div><div class=\"line\">const reference = task(&#123;</div><div class=\"line\">  output: stdout()</div><div class=\"line\">&#125;, ncbi.download('assembly', '&#123;params.specie&#125;'))</div><div class=\"line\">const bwa_index = task(&#123;</div><div class=\"line\">  input: file(),</div><div class=\"line\">  output: file()</div><div class=\"line\">&#125;, wrapper('bwa index &#123;input&#125;'))</div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\">// some tools you cannot decide output file</div><div class=\"line\">// so, tell waterwheel to stream a file as stdout</div><div class=\"line\">// there is --stdout for fastq-dump to give a streamed fastq, but trimmomatic</div><div class=\"line\">// wants reads_1 and reads_2</div><div class=\"line\">// tools that are not bionode will need to wrapper()ed</div><div class=\"line\">const extract = task(&#123;</div><div class=\"line\">  input: file('reads.sra'),</div><div class=\"line\">  output: file([1, 2].map(n =&gt; `reads_$&#123;n&#125;.fastq.gz`))</div><div class=\"line\">&#125;, wrapper('fastq-dump --split-files --skip-technical --gzip &#123;input&#125;'))</div><div class=\"line\"></div><div class=\"line\">const trim = task(&#123;</div><div class=\"line\">  input: file([1, 2].map(n =&gt; `reads_$&#123;n&#125;.fastq.gz`)),</div><div class=\"line\">  output: &#123;</div><div class=\"line\">    pe: file([1, 2].map(n =&gt; `reads_$&#123;n&#125;.trim.pe.fastq.gz`),</div><div class=\"line\">    se: file([1, 2].map(n =&gt; `reads_$&#123;n&#125;.trim.se.fastq.gz`)</div><div class=\"line\">  &#125;,</div><div class=\"line\">  opts: &#123;</div><div class=\"line\">    adapters: '../adapters'</div><div class=\"line\">  &#125;</div><div class=\"line\">&#125;, wrapper('''</div><div class=\"line\">  trimmomatic PE -phred33 \\</div><div class=\"line\">  &#123;input[0]&#125; &#123;input[1]&#125; \\</div><div class=\"line\">  &#123;output.pe[0]&#125; &#123;output.se[0]&#125; \\</div><div class=\"line\">  &#123;output.pe[1]&#125; &#123;output.se[1]&#125; \\</div><div class=\"line\">  ILLUMINACLIP:&#123;opts.adapters&#125;/TruSeq3-PE.fa:2:30:10 \\</div><div class=\"line\">  LEADING:3 TRAILING:3 SLIDINGWINDOW:4:20 MINLEN:36 \\</div><div class=\"line\">'''))</div><div class=\"line\"></div><div class=\"line\">const merge = task(&#123;</div><div class=\"line\">  input: file(),</div><div class=\"line\">  output: stdout()</div><div class=\"line\">&#125;, wrapper('seqtk mergepe &#123;input&#125;'))</div><div class=\"line\"></div><div class=\"line\">const gzip = task(&#123;</div><div class=\"line\">  input: stdin(),</div><div class=\"line\">  output: stdout()</div><div class=\"line\">&#125;, wrapper('gzip - &gt; &#123;output&#125;'))</div><div class=\"line\"></div><div class=\"line\">// Branching: these two filtering types produce the same type of output</div><div class=\"line\">// So we can pass an array, and define them both under the same input/output</div><div class=\"line\">const filter = task(&#123;</div><div class=\"line\">  input: file()</div><div class=\"line\">  output: file()</div><div class=\"line\">  opts: &#123;</div><div class=\"line\">    tmpDir: directory()</div><div class=\"line\">  &#125;</div><div class=\"line\">&#125;, [wrapper('''</div><div class=\"line\">  kmc -k&#123;params.KMERSIZE&#125; -m&#123;params.MEMORYGB&#125; -t&#123;params.THREADS&#125; &#123;input&#125; reads.trim.pe.kmc &#123;opts.tmp&#125; \\</div><div class=\"line\">  kmc_tools filter reads.trim.pe.kmc -cx&#123;params.MINCOVERAGE&#125; &#123;input&#125; -ci0 -cx0 &#123;output&#125; \\</div><div class=\"line\">'''), wrapper('''</div><div class=\"line\">  load-into-counting.py -N 4 -k &#123;params.KMERSIZE&#125; -M &#123;params.MEMORYGB&#125;e9 -T &#123;params.THREADS&#125; reads.trim.pe.fastq.gz.kh &#123;input&#125; \\</div><div class=\"line\">  abundance-dist.py reads.trim.pe.fastq.gz.kh &#123;input&#125; reads.trim.pe.fastq.gz.kh.hist \\</div><div class=\"line\">  filter-abund.py -T &#123;params.THREADS&#125; -C $&#123;MINCOVERAGE&#125; reads.trim.pe.fastq.gz.kh -o &#123;output&#125; &#123;input&#125; \\</div><div class=\"line\">''')])</div><div class=\"line\"></div><div class=\"line\">// file() dependencies will stop stream.</div><div class=\"line\">// for example, need to wait on an index file to be made before aligning</div><div class=\"line\">const bwa_mem = task(&#123;</div><div class=\"line\">  input: &#123;</div><div class=\"line\">    reference: file(),</div><div class=\"line\">    index: file(), // check for reference indexing</div><div class=\"line\">    sample: file() // output of filter</div><div class=\"line\">  &#125;</div><div class=\"line\">  output: stdout()</div><div class=\"line\">&#125;, wrapper('bwa mem &#123;input.reference&#125; &#123;input.sample&#125;'))</div><div class=\"line\"></div><div class=\"line\">// This is where streams in Node can really show</div><div class=\"line\">// In snakemake or Nextflow, this would be bwa mem | samtools view</div><div class=\"line\">// Which is less modular, reproducible, containerizable</div><div class=\"line\">const samtools_view = task(&#123;</div><div class=\"line\">  input: stdin(),</div><div class=\"line\">  output: stdout()</div><div class=\"line\">&#125;, wrapper('samtools view -Sbh &#123;input&#125;'))</div><div class=\"line\"></div><div class=\"line\">const samtools_sort = task(&#123;</div><div class=\"line\">  input: stdin(),</div><div class=\"line\">  output: stdout()</div><div class=\"line\">&#125;, wrapper('samtools sort &#123;input&#125;'))</div><div class=\"line\"></div><div class=\"line\">const samtools_index = task(&#123;</div><div class=\"line\">  input: stdin(),</div><div class=\"line\">  output: stdout()</div><div class=\"line\">&#125;, wrapper('samtools index &#123;input&#125;'))</div><div class=\"line\"></div><div class=\"line\">// these will all get piped through each other:</div><div class=\"line\">// bwa_mem().pipe(samtools_view()).pipe(samtools_sort()).pipe(samtools_index())</div><div class=\"line\">const align = join([bwa_mem, samtools_view, samtools_sort, samtools_index])</div><div class=\"line\"></div><div class=\"line\">const samtools_mpileup = task(&#123;</div><div class=\"line\">  input: &#123;</div><div class=\"line\">    bam: file(),</div><div class=\"line\">    index: file(),</div><div class=\"line\">    reference: file()</div><div class=\"line\">  &#125;,</div><div class=\"line\">  output: stdout()</div><div class=\"line\">&#125;, wrapper('samtools mpileup -uf &#123;input.bam&#125; &#123;input.reference&#125;'))</div><div class=\"line\"></div><div class=\"line\">const bcftools_call = task(&#123;</div><div class=\"line\">  input: stdin(),</div><div class=\"line\">  output: stdout()</div><div class=\"line\">&#125;, wrapper('bcftools call -c &#123;input&#125;'))</div><div class=\"line\"></div><div class=\"line\">const callVariants = join([samtools_mpileup, bcftools_call])</div><div class=\"line\"></div><div class=\"line\">// need a way to use output of another task as input for this one</div><div class=\"line\">const pipeline = join(</div><div class=\"line\">  [sra, extract, trim, merge],</div><div class=\"line\">  // this creates a branching</div><div class=\"line\">  filter,</div><div class=\"line\">  align,</div><div class=\"line\">  callVariants</div><div class=\"line\">)</div><div class=\"line\"></div><div class=\"line\">// Run the whole pipeline, passing in params</div><div class=\"line\">run(pipelineParams, pipeline).pipe(task(fs.createWriteStream('&#123;output&#125;')))</div></pre></td></tr></table></figure>\n","excerpt":"","more":"<p><em>Next generation sequencing</em>. We all know that a fundamental practice in bioinformatics is the analysis of biological sequences. Similarities, functions, structures, associations, transcripts, proteins, RNA interference, regulation, interaction, DNA binding, the list goes on. Much can be hypothesized given some ATCGs (and some annotations). </p>\n<p>However, its <strong>not plug and play</strong>.  Various tools and algorithms exists for each step in NGS data pipelines. Each with their own advantages and disadvantages for a given set of data (e.g. bacterial vs. eukaryotic genomes). Their underlying algorithms can make assumptions which may not be true in all cases. New tools and methods are being developed and there are <strong>rarely adopted standards</strong>. Researchers today regularly construct hardcoded and unmaintanable scripts. I am not calling out these individuals on their coding practice, but rather positing that scripts without community maintained modular dependencies, with dependence on a specific environment configuration - let alone hardcoded absolute file references, are by their nature <strong>unfit for providing reproducible NGS workflows to the community at large</strong>. But hey, it <a href=\"https://github.com/nikku/works-on-my-machine\"><img src=\"https://cdn.rawgit.com/nikku/works-on-my-machine/v0.2.0/badge.svg\" alt=\"works badge\"></a> right?</p>\n<p>A well written <strong>bash script</strong> <em>can</em> be version controlled, and dependencies <em>can</em> be described, however the consumer <em>may</em> not be able to achieve an identical environment. At the very least, it will be a painful setup process.  Similarly, a <strong>python script</strong> is definitely more elegant and modular, but still suffers from issues such as pipeline reentrancy. One popular old tool is <strong>make</strong>, which can improve reentrancy by defining <em>rules</em> which have a <em>target</em> (output) and <em>input</em>s. However, the syntax is not newcomer friendly and file pattern matching can be confusing or limited. </p>\n<p>Not all hope is lost. There are have been many great efforts approaching this issue. One is <strong>snakemake</strong> which defines an elegant python-esque makefile with filename wildcarding, support for inline Python and R, and more. Another is <strong>nextflow</strong>, which goes a step further and describes pipelines through isolated (and containerizable) <em>process</em> blocks which communicate through channels. As well there are extras like galaxy, luigi, and bcbio.</p>\n<p>In this blog post, I will define a simple variant calling pipeline. Then walk through the implementation of this pipeline using these four technologies:</p>\n<ol>\n<li>bash</li>\n<li>make</li>\n<li>snakemake</li>\n<li>nextflow</li>\n</ol>\n<p>Then discuss other alternatives in brief. Finally I will propose where Bionode and JavaScript can fit into this ecosystem, and which specific issues can be addressed.</p>\n<h2 id=\"Variant-Calling-Pipeline\"><a href=\"#Variant-Calling-Pipeline\" class=\"headerlink\" title=\"Variant Calling Pipeline\"></a>Variant Calling Pipeline</h2><p>We will be running a simple variant calling pipeline using a referenence genome and paired end genomic reads. For the sake of time when running the pipeline locally, we will use a small genome, <em>Salmonella enterica</em>, which has some paired end reads at about 100mb in size. With the reference genome at about 1.4mb, that provides about 70x coverage. </p>\n<p>To follow along with this blog post, you will need to install: bionode-ncbi, sra-toolkit, bwa, samtools, bcftools, khmer, kmc, trimmomatic, snakemake, and nextflow.</p>\n<p>Ha, Im just kidding. Here is the Dockerfile: <a href=\"https://hub.docker.com/r/thejmazz/polyglot-ngs-01/\">polyglot-ngs-01</a>.</p>\n<p><a href=\"https://github.com/bionode/bionode-ncbi\">bionode-ncbi</a> will be used to download the reads from the NCBI SRA, or <em>sequence read archive</em>. The reference could be downloaded with <code>bionode-ncbi download assembly $specie</code> , but at the moment there is a <a href=\"https://github.com/bionode/bionode-ncbi/issues/19\">bug</a> where that downloads the <code>rna_from_genomic</code> rather than <code>genomic</code> file for some species.</p>\n<p>Once we have the an <code>sra</code> for <em>Salmonella enterica</em>, the next step is to generate the two <code>fastq</code> files for the two ended reads. Paired ends have a 5 $\\rightarrow$ 3 set of reads and a 3 $\\rightarrow$ 5 set of reads. This allows for much more confident alignment and is generally preferred over a single set of reads. For this we can use <code>fastq-dump</code> from <a href=\"https://github.com/ncbi/sra-tools\">sra tools</a>:</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">fastq-dump --split-files --skip-technical --gzip reads.sra</div></pre></td></tr></table></figure>\n<p>This will produce <code>reads_1.fastq.gz</code> and <code>reads_2.fastq.gz</code>.</p>\n<blockquote>\n<p>fastq is essentially a fasta file that also includes <strong>quality</strong> scores. Quality scores are produced by the <strong>base calling</strong> methods employed by the given NGS machine. See the wikipedia pages for the <a href=\"https://en.wikipedia.org/wiki/FASTA_format\">fasta format</a> and the <a href=\"https://en.wikipedia.org/wiki/FASTQ_format\">fastq format</a>.</p>\n</blockquote>\n<p>Filtering will be two step:</p>\n<ol>\n<li><p>trim reads adapters with <a href=\"http://www.usadellab.org/cms/?page=trimmomatic\">trimmomatic</a></p>\n</li>\n<li><p>filter out bad <a href=\"https://en.wikipedia.org/wiki/K-mer\"><em>k</em>-mers</a> with </p>\n<p>a) khmer</p>\n<p>b) kmc</p>\n</li>\n</ol>\n<p>This allows us to illustrate how much the pipeline tools let us swap in and out different tools for the same step (in this instance, khmer vs kmc) and then compare results. </p>\n<p>The filtering steps will complete by producing a <code>reads.filtered.fastq.gz</code> file. With other tools, it could have been a <code>reads_1.filtered.fastq.gz</code> and <code>reads_2.fastq.gz</code>.  It depends whether or not one of the filtering tools creates an <em>interleaved</em> file holding both read directions. It does not matter too much, as the next step can handle both cases. If we wanted to skip filtering, we could just use <code>reads_1.fastq.gz</code> and <code>reads_2.fastq.gz</code>.</p>\n<p>Now we want to align the reads to the reference using <a href=\"https://github.com/lh3/bwa\">bwa</a> which is a <a href=\"https://en.wikipedia.org/wiki/Burrows%E2%80%93Wheeler_transform\">Burrows-Wheeler transform</a> based alignment tool. First, an <a href=\"https://en.wikipedia.org/wiki/FM-index\">FM-index</a> needs to be constructed for the reference genome:</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">bwa index reference.genomic.fna.gz</div></pre></td></tr></table></figure>\n<p>Then we align using the reads, producing a <em>sequence alignment map</em>:</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">bwa mem reference.genomic.fna.gz reads.filtered.fastq.gz &gt; reads.sam</div></pre></td></tr></table></figure>\n<p>We use <a href=\"https://github.com/samtools/samtools\">samtools</a> (also see <a href=\"https://en.wikipedia.org/wiki/SAMtools\">wiki/SAMtools</a>) to generate a <em>binary alignment map</em>:</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">samtools view -bh reads.sam  &gt; reads.unsorted.bam</div></pre></td></tr></table></figure>\n<p>and sort it:</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">samtools sort reads.unsorted.bam -o reads.bam</div></pre></td></tr></table></figure>\n<p>We could have piped the last three steps together, avoiding file writing/reading overhead:</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">bwa mem ref reads | samtools view -bh - | samtools sort - -o reads.bam</div></pre></td></tr></table></figure>\n<p>and then index it (creates <code>reads.bam.bai</code> - <em>binary alignment index</em>):</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">samtools index reads.bam</div></pre></td></tr></table></figure>\n<p>At this point, we have everything we need to call variants:</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">samtools mpileup -uf reference.genomic.fna reads.bam | \\</div><div class=\"line\">bcftools call -c - &gt; reads.vcf</div></pre></td></tr></table></figure>\n<p><code>mpileup</code> creates a BCF <a href=\"https://en.wikipedia.org/wiki/Pileup_format\">pileup</a> file describing the base calls of aligned reads. Then <code>bcftools call</code> takes this and generates a <a href=\"https://en.wikipedia.org/wiki/Variant_Call_Format\">variant call format</a> file.</p>\n<p>Since <code>samtools</code> was complaining about the <code>reference.genomic.fna.gz</code> that comes from the NCBI Assemblies database - something to do with the compression format, I first decompressed it with:<br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">bgzip <span class=\"_\">-d</span> reference.genomic.fna.gz</div></pre></td></tr></table></figure></p>\n<p>Thats it! Thats how to get from SRA $\\rightarrow$ VCF using bionode, sra tools, trimming tools, filtering tools, bwa, samtools, and bcftools. The next sections will go over how to improve the reproducibility, reentrancy, ease of development, etc. of this workflow. </p>\n<h2 id=\"Topics\"><a href=\"#Topics\" class=\"headerlink\" title=\"Topics\"></a>Topics</h2><p>For each tool, we will inspect the following topics.</p>\n<h3 id=\"Basic-Structure\"><a href=\"#Basic-Structure\" class=\"headerlink\" title=\"Basic Structure\"></a>Basic Structure</h3><ul>\n<li>quick intro to the tool, how it is written</li>\n</ul>\n<h3 id=\"Iterative-Development\"><a href=\"#Iterative-Development\" class=\"headerlink\" title=\"Iterative Development\"></a>Iterative Development</h3><ul>\n<li>reentrancy</li>\n<li>error debugging</li>\n</ul>\n<h3 id=\"Metrics\"><a href=\"#Metrics\" class=\"headerlink\" title=\"Metrics\"></a>Metrics</h3><ul>\n<li>time for each task</li>\n<li>resource usage</li>\n<li>reports</li>\n</ul>\n<h3 id=\"Scaling\"><a href=\"#Scaling\" class=\"headerlink\" title=\"Scaling\"></a>Scaling</h3><ul>\n<li>extend to many species</li>\n<li>run on a cluster/cloud system</li>\n<li>distribute/reproducibility</li>\n</ul>\n<h2 id=\"bash\"><a href=\"#bash\" class=\"headerlink\" title=\"bash\"></a>bash</h2><p>The first obvious tool to use constructing a pipeline composed of a list of shell commands is a simple bash script. Essentially, we can take the commands from above and arrange them in a linear manner. You can see the full bash pipeline here: <a href=\"https://github.com/bionode/gsoc16/blob/543ba66cbb42d1622089764bf01090e318307a57/pipelines/with-bash/basic-snp-calling.sh\">basic-snp-calling.sh</a>. </p>\n<h3 id=\"Basic-Structure-1\"><a href=\"#Basic-Structure-1\" class=\"headerlink\" title=\"Basic Structure\"></a>Basic Structure</h3><p>The above section introduced the tools and their commands required to run a simple variant calling pipeline. However, all of the file names were hardcoded: <code>reference.genomic.fna.gz</code>, <code>reads.fastq.gz</code>, etc. In a real analysis, we would like to generalize so that we can provide paramaters defining the specie and reads to use. Then, for example, we can run the pipeline over many species and inspect related hypotheses. Within a bash script we can attempt to carry this out using <em>environment variables</em>. Depending on how these variables are defined, they may be accessible from different places. For example, the <code>PATH</code> variable (try <code>echo $PATH</code>) in your shell is available all the time. This is because it was defined using <code>export</code>. Without <code>export</code>, a variable is only available within in its current context. Consider:</p>\n<p><em>main.sh</em></p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"meta\">#!/bin/bash</span></div><div class=\"line\">READS_ID=2492428 <span class=\"comment\"># no spaces around =!</span></div><div class=\"line\">./downloadReads.sh</div></pre></td></tr></table></figure>\n<p><em>downloadReads.sh</em></p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"meta\">#!/bin/bash</span></div><div class=\"line\">bionode-ncbi download sra <span class=\"variable\">$READS_ID</span></div></pre></td></tr></table></figure>\n<p>In this case, <code>READS_ID</code> will evaluate to blank. Alternatively you need to <code>export READS_ID=2492428</code> to have the variable available in other contexts. This is important to make note of, because complex shell scripts will likely be split across multiple files, in which case it is important to understand when and when not to use <code>export</code>. <em>Blindly using export everywhere is not recommended.</em></p>\n<p>With an understanding of environment variables out of the way, we can use them to define some initial settings for the script:</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\"># Salmonella enterica</span></div><div class=\"line\">REFERENCE_NAME=<span class=\"string\">'GCA_000006945.2_ASM694v2'</span></div><div class=\"line\">REFERENCE=<span class=\"string\">\"<span class=\"variable\">$&#123;REFERENCE_NAME&#125;</span>_genomic.fna.gz\"</span></div><div class=\"line\">REFERENCE_URL=<span class=\"string\">\"http://ftp.ncbi.nlm.nih.gov/genomes/all/<span class=\"variable\">$REFERENCE_NAME</span>/<span class=\"variable\">$REFERENCE</span>\"</span></div><div class=\"line\">READS=<span class=\"string\">'2492428'</span></div><div class=\"line\"><span class=\"built_in\">export</span> READSFQ=<span class=\"string\">'ERR1229296'</span></div><div class=\"line\">FILTER_MODE=<span class=\"string\">'khmer'</span> <span class=\"comment\"># 'kmc', 'none'</span></div></pre></td></tr></table></figure>\n<p><code>READSFQ</code> is defined with <code>export</code> so it can be available from the filter scripts which have been separated out from the main file. Then files used as input and output can be described with these variables:</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"built_in\">echo</span> <span class=\"string\">\"START fastq-dump\"</span></div><div class=\"line\">fastq-dump --split-files --skip-technical --gzip <span class=\"variable\">$READS</span>/**.sra</div><div class=\"line\"><span class=\"built_in\">echo</span> <span class=\"string\">\"END fastq-dump\"</span></div></pre></td></tr></table></figure>\n<p>I also notify the start and end of each task, <em>manually</em>. This is cumbersome and cluttering, and adding more useful features like time taken would be even more messy. Up until the filtering, the pipeline is completely linear. However, we would like to filter using two tools: <code>khmer</code> and <code>kmc</code>. One is significantly faster, but may produce more erroneous results. But that does not mean its useless, it can be useful to favour speed and number of species over specificity at the expense of less trials. Ideally, the swap from fast to slow should be a simple swap. As well, it can be useful to compare where exactly the two methods differ in their results. Enter the <code>if [ condtion ]; then  fi</code> block:</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">if</span> [ <span class=\"variable\">$FILTER_MODE</span> == <span class=\"string\">'kmc'</span> ]; <span class=\"keyword\">then</span></div><div class=\"line\">  <span class=\"keyword\">if</span> [ ! <span class=\"_\">-d</span> <span class=\"variable\">$TMPDIR</span> ]; <span class=\"keyword\">then</span></div><div class=\"line\">    mkdir -p <span class=\"variable\">$TMPDIR</span></div><div class=\"line\">  <span class=\"keyword\">fi</span></div><div class=\"line\">  <span class=\"built_in\">echo</span> <span class=\"string\">\"START filtering with kmc\"</span></div><div class=\"line\">  <span class=\"built_in\">export</span> READS_TO_ALIGN=<span class=\"string\">\"<span class=\"variable\">$READSFQ</span>.trim.pe.kmc.fastq.gz\"</span></div><div class=\"line\">  ./filter_kmc.sh</div><div class=\"line\">  <span class=\"built_in\">echo</span> <span class=\"string\">\"END filtering with kmc\"</span></div><div class=\"line\"><span class=\"keyword\">elif</span> [ <span class=\"variable\">$FILTER_MODE</span> == <span class=\"string\">'khmer'</span> ]; <span class=\"keyword\">then</span></div><div class=\"line\">  <span class=\"built_in\">echo</span> <span class=\"string\">\"Using khmer mode\"</span></div><div class=\"line\">  <span class=\"built_in\">export</span> READS_TO_ALIGN=<span class=\"string\">\"<span class=\"variable\">$READSFQ</span>.trim.pe.khmer.fastq.gz\"</span></div><div class=\"line\">  ./filter_khmer.sh</div><div class=\"line\">  FINAL_OUTPUT=<span class=\"string\">\"<span class=\"variable\">$&#123;READS&#125;</span>-khmer.vcf\"</span></div><div class=\"line\"><span class=\"keyword\">else</span></div><div class=\"line\">  <span class=\"built_in\">echo</span> <span class=\"string\">\"No filter mode set. Continuing with reads with adapters trimmed.\"</span></div><div class=\"line\">  READS_TO_ALIGN=<span class=\"string\">\"<span class=\"variable\">$READSFQ</span>.trim.pe.fastq.gz\"</span></div><div class=\"line\"><span class=\"keyword\">fi</span></div></pre></td></tr></table></figure>\n<p>At this point, the pipeline branches into two modes. Currently, if we wanted to switch filter modes, we have to edit the definition of <code>FILTER_MODE</code> and restart the pipeline. If we are not checking for file existence before each command, this will be a big time waste. You can imagine how even deeper branching options can complicate it more. As well, it can be useful to change tool parameters and inspect the effects on final results. As an annoyance, changing these settings requires editing the file every time: this script would not scale to 100s of species well.</p>\n<p>After the filtering, the final section, alignment and calling, uses a variable set by the filtering option used to determine which reads to align with and the name of the final output. We can run the pipeline two times, wasting time, but at least the final results do not overwrite each other. </p>\n<h3 id=\"Iterative-Development-1\"><a href=\"#Iterative-Development-1\" class=\"headerlink\" title=\"Iterative Development\"></a>Iterative Development</h3><p>Iterative development is the practice of developing a pipeline peice by peice. It is important because these pipelines can take a long time to run and simply running from scratch every time something needs to be changed is unnacceptable. In a bash script it can be handled by placing<br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"meta\">#!/bin/bash</span></div><div class=\"line\"><span class=\"built_in\">set</span> <span class=\"_\">-e</span> <span class=\"comment\"># exits script when an error is thrown</span></div><div class=\"line\"></div><div class=\"line\">...</div><div class=\"line\"></div><div class=\"line\"><span class=\"built_in\">exit</span> 1</div></pre></td></tr></table></figure></p>\n<p>throughout the script. As well, I particulary enjoy [this][r-exec-atom] plugin, with it you can cmd+enter to send the current line or text selection to a terminal. However, there are still some remaining issues with this process. If you want to skip over previous steps, perhaps downloading reads or indexing, you will need to add explicit checks to your code. This clutters the code, and may still be error prone, as simply checking for a files existence might not be enough: it could be a 404 html document rather than a set of reads.</p>\n<h3 id=\"Metrics-1\"><a href=\"#Metrics-1\" class=\"headerlink\" title=\"Metrics\"></a>Metrics</h3><p>Any metrics you might want - time per task, cpu time per task, cpu and memory usage, would all have to be written manually. You could write a function to wrap commands and time them, etc, but this still clutters the code, obstructing the main message. Furthermore, in house metrics gathering is prone to error, especially when running on different machines. You may want to develop on a Mac and then deploy to a Linux server, where some commands may be different.</p>\n<p>Any sort of report on the pipeline performance, whether it be a text file or styled HTML document, would again require in house coding.</p>\n<h3 id=\"Scaling-1\"><a href=\"#Scaling-1\" class=\"headerlink\" title=\"Scaling\"></a>Scaling</h3><p>Bash scripts are not scalable. If we were to port the script to handle multiple species, we <em>could use</em> bash arrays:</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">nums=(1 2 3 4 5); <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"variable\">$nums</span>; <span class=\"keyword\">do</span> <span class=\"built_in\">echo</span> <span class=\"variable\">$i</span>; <span class=\"keyword\">done</span></div></pre></td></tr></table></figure>\n<p>The above script might work for pipelines which only take one input parameter for a specie, but we also need the reference URL, SRA ID, etc. Going one step further, we can use array indices to gather related data. Yet the syntax is not the cleanest:</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\">species=(Salmonella-enterica Staphylococcus-aureus)</div><div class=\"line\">readsID=(2492428 1274026)</div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"string\">\"<span class=\"variable\">$&#123;!species[@]&#125;</span>\"</span>; <span class=\"keyword\">do</span> </div><div class=\"line\">    <span class=\"built_in\">printf</span> <span class=\"string\">\"%s\\t%s\\t%s\\n\"</span> <span class=\"variable\">$i</span> <span class=\"variable\">$&#123;species[$i]&#125;</span> <span class=\"variable\">$&#123;readsID[$i]&#125;</span></div><div class=\"line\"><span class=\"keyword\">done</span></div></pre></td></tr></table></figure>\n<p>However, this would probably involve multiple code changes throughout the script. When deviating from the original purpose, bash scripts are likely to require relatively large codebase fixes. Changing species from a string to an array will break a lot of things.</p>\n<p>Furthermore, changing settings like maximum threads and memory usage will require manually editing the script when on the cluster, versus other tools which can provide this a command line option. Of course, <em>you could parse your own params</em>, but that is a lot of manual work.</p>\n<p>In terms of reproducibility, there is not much. The script will likely assume all dependencies are installed. It may assume required binaries are in a specific place (in my script I used the <code>BIN</code> variable for this). It may also even fail on systems with different commands. It may also even assume being in a certain directory and directory structures of the system. </p>\n<h2 id=\"make\"><a href=\"#make\" class=\"headerlink\" title=\"make\"></a>make</h2><p><code>make</code> and <code>Makefile</code> are old, well known tools that are normally used for C/C++ compilation. The complete <code>Makefile</code> is <a href=\"https://github.com/bionode/gsoc16/blob/543ba66cbb42d1622089764bf01090e318307a57/pipelines/with-make/Makefile\">here</a>. </p>\n<h3 id=\"Basic-Structure-2\"><a href=\"#Basic-Structure-2\" class=\"headerlink\" title=\"Basic Structure\"></a>Basic Structure</h3><p>With <code>make</code>, the pipeline can be made a little more cleaner, particularly with respect to <em>input</em> and <em>output</em> from each command. A <code>makefile</code> is composed of rules:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">rule &lt;target&gt;: &lt;prerequisites...&gt;</div><div class=\"line\">\t&lt;command&gt;</div></pre></td></tr></table></figure>\n<p>Make will run the rules that have targets for a given set of prerequisites. Consider:</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div></pre></td><td class=\"code\"><pre><div class=\"line\">rule all: genome</div><div class=\"line\"></div><div class=\"line\">rule chromosome_1:</div><div class=\"line\">\t<span class=\"built_in\">echo</span> ATCG &gt; chromosome_1</div><div class=\"line\"></div><div class=\"line\">rule chromosome_2:</div><div class=\"line\">\t<span class=\"built_in\">echo</span> GATA &gt; chromosome_2</div><div class=\"line\"></div><div class=\"line\">rule genome: chromosome_1 chromosome_2</div><div class=\"line\">\tcat chromosome_1 chromsome_2 &gt; genome</div></pre></td></tr></table></figure>\n<p>If you trace the execution, it will first go into the first rule (which is named <code>all</code> by convention). The first rule states it requires <code>genome</code>. The third rule is capable of producing <code>genome</code> as that is its target, yet it requires two other prerequisites. So then rules that have <code>chromosome_*</code> in their target are used, these have no prereqs and can run right away. Then the program unwinds and everything works its way back to <code>rule all</code>. To learn more about <code>make</code> I highly recommend <a href=\"https://gist.github.com/isaacs/62a2d1825d04437c6f08\">this gist</a> This technique, where pipelines are defined from the end-to-forwards, we will refer to as the <strong>pull</strong> method.</p>\n<p>We can handle branching using set variables to enforce ignoring some rules:</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\"># Toggle these variable declarations to switch between</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># 1. no trimming/filtering</span></div><div class=\"line\"><span class=\"comment\"># PRECONDITION_TO_USE=fastq-dump.log</span></div><div class=\"line\"><span class=\"comment\"># READS_1=reads_1.fastq.gz</span></div><div class=\"line\"><span class=\"comment\"># READS_2=reads_2.fastq.gz</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># 2. kmc</span></div><div class=\"line\"><span class=\"comment\"># PRECONDITION_TO_USE=reads.trim.pe.kmc.fastq.gz</span></div><div class=\"line\"><span class=\"comment\"># READS_1=reads.trim.pe.kmc.fastq.gz</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># 3. khmer</span></div><div class=\"line\">PRECONDITION_TO_USE=reads.trim.pe.khmer.fastq.gz</div><div class=\"line\">READS_1=reads.trim.pe.khmer.fastq.gz</div></pre></td></tr></table></figure>\n<p>And the corresponding rule which will invoke a given path in the branch:</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div></pre></td><td class=\"code\"><pre><div class=\"line\">reads.sam: bwa-index.log <span class=\"variable\">$&#123;PRECONDITION_TO_USE&#125;</span></div><div class=\"line\">\t@<span class=\"built_in\">echo</span> <span class=\"string\">\"=== Running <span class=\"variable\">$@</span> ===\"</span></div><div class=\"line\">\t@<span class=\"built_in\">export</span> start=`date +%s`; \\</div><div class=\"line\">\tbwa mem -t <span class=\"variable\">$&#123;THREADS&#125;</span> reference.genomic.fna.gz <span class=\"variable\">$&#123;READS_1&#125;</span> <span class=\"variable\">$&#123;READS_2&#125;</span> &gt; <span class=\"variable\">$@</span>; \\</div><div class=\"line\">\t<span class=\"built_in\">export</span> end=`date +%s`; \\</div><div class=\"line\">\t<span class=\"built_in\">export</span> runtime=$$(($<span class=\"variable\">$end</span>-$<span class=\"variable\">$start</span>)); \\</div><div class=\"line\">\t<span class=\"built_in\">echo</span> <span class=\"string\">\"Target <span class=\"variable\">$@</span> took $<span class=\"variable\">$runtime</span> seconds\"</span></div></pre></td></tr></table></figure>\n<h3 id=\"Iterative-Development-2\"><a href=\"#Iterative-Development-2\" class=\"headerlink\" title=\"Iterative Development\"></a>Iterative Development</h3><p>Iterative development in <code>make</code> is much improved over a <code>bash</code> script. This is because you can call a specific rule at invocation: <code>make reads.sam</code> for example. As well, it is common practice to include a clean rule. It is possible in <code>make</code> to define targets that dont actually create the target file. Since the target is not actually created (even though the command in that rule has ran and worked as expected), since make skips rules if the target already exists, it will unnecessarily rerun the rule. My general workflow writing a <code>make</code> pipeline was to update the prequisite of <code>rule all</code> as new rules were added, and each task from before would be skipped. One way to get around this issue is to create flag files:</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div></pre></td><td class=\"code\"><pre><div class=\"line\">trim.happened: fastq-dump.log</div><div class=\"line\">\t@<span class=\"built_in\">echo</span> <span class=\"string\">\"=== Running <span class=\"variable\">$@</span> ===\"</span></div><div class=\"line\">\t@<span class=\"built_in\">export</span> start=`date +%s`; \\</div><div class=\"line\">\tjava -jar <span class=\"variable\">$&#123;TRIMMOMATIC&#125;</span> PE -phred33 \\</div><div class=\"line\">\t  reads_1.fastq.gz reads_2.fastq.gz \\</div><div class=\"line\">\t  reads_1.trim.pe.fastq.gz reads_1.trim.se.fastq.gz \\</div><div class=\"line\">\t  reads_2.trim.pe.fastq.gz reads_2.trim.se.fastq.gz \\</div><div class=\"line\">\t  ILLUMINACLIP:<span class=\"variable\">$&#123;ADAPTERS&#125;</span>/TruSeq3-PE.fa:2:30:10 \\</div><div class=\"line\">\t  LEADING:3 TRAILING:3 SLIDINGWINDOW:4:20 MINLEN:36 &gt; <span class=\"variable\">$@</span>; \\</div><div class=\"line\">\t<span class=\"built_in\">export</span> end=`date +%s`; \\</div><div class=\"line\">\t<span class=\"built_in\">export</span> runtime=$$(($<span class=\"variable\">$end</span>-$<span class=\"variable\">$start</span>)); \\</div><div class=\"line\">\tcat <span class=\"variable\">$@</span>; \\</div><div class=\"line\">\t<span class=\"built_in\">echo</span> <span class=\"string\">\"Target <span class=\"variable\">$@</span> took $<span class=\"variable\">$runtime</span> seconds\"</span></div></pre></td></tr></table></figure>\n<p>Here, the stdout of trimmomatic is sent to <code>trim.happened</code>. Even if there is no stdout, an empty file will get made, and then the next run this rule can be skipped.</p>\n<h3 id=\"Metrics-2\"><a href=\"#Metrics-2\" class=\"headerlink\" title=\"Metrics\"></a>Metrics</h3><p>To get time spent metrics, I wrapped each command in a simple delta time calculation:</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div></pre></td><td class=\"code\"><pre><div class=\"line\">time:</div><div class=\"line\">\t@<span class=\"built_in\">echo</span> <span class=\"string\">\"=== Running <span class=\"variable\">$@</span> ===\"</span></div><div class=\"line\">\t@<span class=\"built_in\">export</span> start=`date +%s`; \\</div><div class=\"line\">\tsleep 2; \\</div><div class=\"line\">\t<span class=\"built_in\">export</span> end=`date +%s`; \\</div><div class=\"line\">\t<span class=\"built_in\">export</span> runtime=$$(($<span class=\"variable\">$end</span>-$<span class=\"variable\">$start</span>)); \\</div><div class=\"line\">\t<span class=\"built_in\">echo</span> <span class=\"string\">\"Target <span class=\"variable\">$@</span> took $<span class=\"variable\">$runtime</span> seconds\"</span></div></pre></td></tr></table></figure>\n<p>This clutters the code fast, and is unmaintainable. It might be possible to do it cleaner, but I did not spend time trying obscure solutions. The point is that there is no out of the box way to get task metrics with <code>make</code>. You can imagine how tricky it might be to also get RAM/CPU usage for each task.</p>\n<h3 id=\"Scaling-2\"><a href=\"#Scaling-2\" class=\"headerlink\" title=\"Scaling\"></a>Scaling</h3><p>Scaling with make is also difficult. There is no easy way to expand the pipeline to multiple species, and it is difficulty to variabilize outputs: targets need to strings.</p>\n<h2 id=\"snakemake\"><a href=\"#snakemake\" class=\"headerlink\" title=\"snakemake\"></a>snakemake</h2><p>The <code>Makefile</code> was much more organized than the bash script, but still of a low level.</p>\n<p>Snakemake was a refreshing take on the make style, but with many more features and powered by a high level language. Including Python integration everywhere, tasks to script in Python, Bash, and R, and some metrics out of the box. As well as some neat wildcarding. The complete Snakefile is <a href=\"https://github.com/bionode/gsoc16/blob/543ba66cbb42d1622089764bf01090e318307a57/pipelines/with-snakemake/Snakefile\">here</a>.</p>\n<h3 id=\"Basic-Structure-3\"><a href=\"#Basic-Structure-3\" class=\"headerlink\" title=\"Basic Structure\"></a>Basic Structure</h3><p>You start a Snakemake workflow very similar to a Makefile, defining a global rule and what you want it to create:</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">rule all:</div><div class=\"line\">    input: FINAL_FILES</div></pre></td></tr></table></figure>\n<p>So what is <code>FINAL_FILES</code>? Since Snakemake has direct Python integration, we can actually compute it as a local variable. Heres the header for the <code>Snakefile</code> just before</p>\n<p><code>rule all</code>:</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div></pre></td><td class=\"code\"><pre><div class=\"line\">species = &#123;</div><div class=\"line\">    <span class=\"string\">'Salmonella-enterica'</span>: &#123;</div><div class=\"line\">        <span class=\"string\">'readsID'</span>: <span class=\"string\">'2492428'</span>,</div><div class=\"line\">        <span class=\"string\">'reference_url'</span>: <span class=\"string\">'http://ftp.ncbi.nlm.nih.gov/genomes/all/GCA_000988525.2_ASM98852v2/GCA_000988525.2_ASM98852v2_genomic.fna.gz'</span></div><div class=\"line\">    &#125;</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\">THREADS=<span class=\"number\">2</span></div><div class=\"line\">TEMP=<span class=\"string\">'./tmp'</span></div><div class=\"line\"></div><div class=\"line\">FINAL_FILES = [specie+<span class=\"string\">'.vcf'</span> <span class=\"keyword\">for</span> specie <span class=\"keyword\">in</span> species]</div></pre></td></tr></table></figure>\n<p>This will end up with <code>FINAL_FILES</code> being <code>[&#39;Salmonella-enterica.vcf&#39;]</code>.  See this rule which will be one of the first in the executed pipeline (here I hardcoded the specie name to better illustrate):</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div></pre></td><td class=\"code\"><pre><div class=\"line\">rule all:</div><div class=\"line\">    input: <span class=\"string\">'Salmonella-enterica.vcf'</span></div><div class=\"line\"></div><div class=\"line\">rule download_sra:</div><div class=\"line\">    output: <span class=\"string\">'&#123;specie&#125;.sra'</span></div><div class=\"line\">    run:</div><div class=\"line\">        readsID = species[wildcards.specie][<span class=\"string\">'readsID'</span>]</div><div class=\"line\">        shell(<span class=\"string\">'''</div><div class=\"line\">            bionode-ncbi download sra &#123;readsID&#125;;</div><div class=\"line\">            cp &#123;readsID&#125;/*.sra &#123;output&#125; &amp;&amp; rm -rf &#123;readsID&#125;;</div><div class=\"line\">        '''</span>)       </div><div class=\"line\">        </div><div class=\"line\">rule call:</div><div class=\"line\">    input: <span class=\"string\">'&#123;specie&#125;.sra'</span></div><div class=\"line\">    output: <span class=\"string\">'&#123;specie&#125;.vcf'</span></div><div class=\"line\">    shell: <span class=\"string\">'magic &#123;input&#125; &gt; &#123;output&#125;'</span></div></pre></td></tr></table></figure>\n<p>The first rule will be triggered and will be looking for another rule that has <code>Salmonella-enterica.vcf</code> in its <code>output</code>. It wont find <em>exactly</em> that, but because of Snakemakes wildcarding, <code>{specie}.vcf</code> will do (from <code>rule call</code>), and then within the <code>call</code> rule, the value of <code>wildcards.specie</code> will be <code>Salmonella-enterica</code>. Then it will move onto <code>download_sra</code>. This example also illustrates how you can use <code>run</code> and <code>shell()</code> to mix Python and shell code. You can also use <code>script</code> to run a Python or R script, and use the <code>R()</code> function to execute R code in a rule.</p>\n<p>Another neat feature with Snakemake is the ability to drop in wrappers:</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div></pre></td><td class=\"code\"><pre><div class=\"line\">rule bwa_mem:</div><div class=\"line\">    input:</div><div class=\"line\">        ref = <span class=\"string\">'&#123;specie&#125;.genomic.fna.gz'</span>,</div><div class=\"line\">        sample = [<span class=\"string\">'&#123;specie&#125;_'</span>+num+<span class=\"string\">'.fastq.gz'</span> <span class=\"keyword\">for</span> num <span class=\"keyword\">in</span> [<span class=\"string\">'1'</span>, <span class=\"string\">'2'</span>]],</div><div class=\"line\">        index_files = [<span class=\"string\">'&#123;specie&#125;.genomic.fna.gz.'</span>+suffix <span class=\"keyword\">for</span> suffix <span class=\"keyword\">in</span> [<span class=\"string\">'amb'</span>, <span class=\"string\">'ann'</span>, <span class=\"string\">'bwt'</span>, <span class=\"string\">'pac'</span>, <span class=\"string\">'sa'</span>]]</div><div class=\"line\">    log: <span class=\"string\">'logs/bwa_mem/&#123;specie&#125;.log'</span></div><div class=\"line\">    output: <span class=\"string\">'&#123;specie&#125;.sam'</span></div><div class=\"line\">    threads: THREADS</div><div class=\"line\">    wrapper: <span class=\"string\">'0.0.8/bio/bwa_mem'</span></div></pre></td></tr></table></figure>\n<p>These wrappers come from the <a href=\"https://bitbucket.org/snakemake/snakemake-wrappers\">wrappers repository</a>. They run predefined commands using specific input variables. While this is great, there is some overhead in having to check the source to see what is actually happening, and by extension, you then need an internet connection to do so. There does not seem to be a huge list of wrappers, it is too bad it does not seem to have caught on extensively.</p>\n<p>Still have the problem of using log/flag files for task dependency, but less so, since you can define custom outputs, also has touch(flag) built in</p>\n<p>Could not figure out how to ecomical branching, wildcard regexes create ambiguity:</p>\n<p><img src=\"https://raw.githubusercontent.com/bionode/gsoc16/fdf22b630e33dd11302ea8822c547ef9399c3ea4/pipelines/with-snakemake/dag.png\" alt=\"attempt-branching\"></p>\n<p>Here it unnecessarily redoes certain steps. This is because <code>Salmonella-enterica.trim.vcf</code> and <code>Salmonella-enterica.trim.vcf</code> both match <code>{specie}.vcf</code>. At first, Snakemake complained about ambiguous rules due to this conflict. I was able to generate the DAG above by using custom regexes for the wildcard in a few rules: <code>{specie,[a-zA-z-]+}</code>. However, since this then creates one set of rules with <code>wildcard.specie = Salmonella-enterica</code> and another with <code>wildcard.specie = Salmonella-enterica.trim</code>, and then that wildcard moves all the way to the first rule, and basically two pipelines are ran. I was not able to figure out how optimize this using Snakemake - if anyone can, let me know!</p>\n<p>Moreover - because the <strong>pull</strong> workflow style depends on <em>targets</em> and <em>prerequisites</em>, it can be difficult to achieve a branch-merge pipeline. We will see with Nextflow, which follows the dataflow paradigm, or <strong>push</strong>, it is natural to describe such pipelines.</p>\n<p>For a more complicated (and real world) snakemake setup, take a look at <a href=\"https://github.com/pachterlab/kallisto_paper_analysis/blob/nbt/Snakefile\">pachterlab/kallisto_paper_analysis/Snakefile</a>.</p>\n<h3 id=\"Iterative-Development-3\"><a href=\"#Iterative-Development-3\" class=\"headerlink\" title=\"Iterative Development\"></a>Iterative Development</h3><p>Iterative development in Snakemake is about the same as with <code>make</code>. With wildcarding, the clean rule can be made more specific, for example:</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\">rule clean:</div><div class=\"line\">    input: <span class=\"string\">'&#123;specie&#125;.vcf'</span></div><div class=\"line\">    shell: <span class=\"string\">'''</div><div class=\"line\">    \trm &#123;specie&#125;.sra &#123;specie&#125;.bam;</div><div class=\"line\">    '''</span></div></pre></td></tr></table></figure>\n<p>As opposed to <code>rm *.sra *.bam</code>. </p>\n<h3 id=\"Metrics-3\"><a href=\"#Metrics-3\" class=\"headerlink\" title=\"Metrics\"></a>Metrics</h3><p>You can opt in to store rule benchmarks and logs:</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div></pre></td><td class=\"code\"><pre><div class=\"line\">rule call:</div><div class=\"line\">    input:</div><div class=\"line\">        bam = <span class=\"string\">'&#123;specie&#125;.bam'</span>,</div><div class=\"line\">        alignment_index = <span class=\"string\">'&#123;specie&#125;.bam.bai'</span>,</div><div class=\"line\">        reference = <span class=\"string\">'&#123;specie&#125;.genomic.fna'</span></div><div class=\"line\">    output: <span class=\"string\">'&#123;specie&#125;.vcf'</span></div><div class=\"line\">    log: <span class=\"string\">'logs/call/&#123;specie&#125;.log'</span></div><div class=\"line\">    benchmark: <span class=\"string\">'benchmarks/call/&#123;specie&#125;.txt'</span></div><div class=\"line\">    shell: <span class=\"string\">'samtools mpileup -uf &#123;input.reference&#125; &#123;input.bam&#125; | bcftools call -c - &gt; &#123;output&#125;'</span></div></pre></td></tr></table></figure>\n<p>However, you will need to make sure you name these files appropiately to avoid overwriting. An example benchmark file looks like:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">s\th:m:s</div><div class=\"line\">30.32477617263794\t0:00:30.324776</div></pre></td></tr></table></figure>\n<p>There is no indication of which task the benchmark is for, meaning if you want a comprehensive report of your pipeline you will need to write a script that concatenates the benchmarks in the correct order.</p>\n<p>With graphviz installed, snakemake can create a plot of the DAG for your pipeline:</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">snakemake --dag | dot -Tpng &gt; dag.png</div></pre></td></tr></table></figure>\n<p>You can see an example of this above.</p>\n<h3 id=\"Scaling-3\"><a href=\"#Scaling-3\" class=\"headerlink\" title=\"Scaling\"></a>Scaling</h3><p>With Snakemake it is easy to scale to multiple species. In my Snakefile, I simply just add more keys to the <code>species</code> dictionary. Since each output file is prefixed by the species name, there will be no file overlaps. You need to be vigilant with output names and ensure yourself there will be no conflicts.</p>\n<p>You can specify clustering configuration by writing a <code>cluster.config</code> file and using its values in the call to Snakemake. The example <a href=\"https://bitbucket.org/snakemake/snakemake/wiki/Documentation#markdown-header-cluster-configuration\">from the documentation</a>:</p>\n<p><em>Snakefile</em></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div></pre></td><td class=\"code\"><pre><div class=\"line\">rule all:</div><div class=\"line\">    input: <span class=\"string\">\"input1.txt\"</span>, <span class=\"string\">\"input2.txt\"</span></div><div class=\"line\"></div><div class=\"line\">rule compute1:</div><div class=\"line\">    output: <span class=\"string\">\"input1.txt\"</span></div><div class=\"line\">    shell: <span class=\"string\">\"touch input1.txt\"</span></div><div class=\"line\"></div><div class=\"line\">rule compute2:</div><div class=\"line\">    output: <span class=\"string\">\"input2.txt\"</span></div><div class=\"line\">    shell: <span class=\"string\">\"touch input2.txt\"</span></div></pre></td></tr></table></figure>\n<p><em>cluster.json</em></p>\n<figure class=\"highlight json\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div></pre></td><td class=\"code\"><pre><div class=\"line\">&#123;</div><div class=\"line\">    <span class=\"attr\">\"__default__\"</span> :</div><div class=\"line\">    &#123;</div><div class=\"line\">        <span class=\"attr\">\"account\"</span> : <span class=\"string\">\"my account\"</span>,</div><div class=\"line\">        <span class=\"attr\">\"time\"</span> : <span class=\"string\">\"00:15:00\"</span>,</div><div class=\"line\">        <span class=\"attr\">\"n\"</span> : <span class=\"number\">1</span>,</div><div class=\"line\">        <span class=\"attr\">\"partition\"</span> : <span class=\"string\">\"core\"</span></div><div class=\"line\">    &#125;,</div><div class=\"line\">    <span class=\"attr\">\"compute1\"</span> :</div><div class=\"line\">    &#123;</div><div class=\"line\">        <span class=\"attr\">\"time\"</span> : <span class=\"string\">\"00:20:00\"</span></div><div class=\"line\">    &#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<p>And the call to Snakemake:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">snakemake -j 999 --cluster-config cluster.json --cluster &quot;sbatch -A &#123;cluster.account&#125; -p &#123;cluster.partition&#125; -n &#123;cluster.n&#125;  -t &#123;cluster.time&#125;&quot;</div></pre></td></tr></table></figure>\n<p>While nextflow is providing a simple config based way to pass params into your clustering command, the actual command: <code>sbatch</code> is written manually. We will say with Nextflow, even this aspect of clustering is abstracted away (into executor). See also <a href=\"https://bitbucket.org/snakemake/snakemake/wiki/Documentation#markdown-header-job-properties\">job properties</a> to see how to write a generic job wrapper for clustering.</p>\n<h2 id=\"nextflow\"><a href=\"#nextflow\" class=\"headerlink\" title=\"nextflow\"></a>nextflow</h2><p>Nextflow is a more recent tool, and approaches the workflow problem in a <strong>push</strong> sense. Similar to how Snakemake allows direct Python integration, Nextflow uses <a href=\"http://www.groovy-lang.org/\">Groovy</a>. While Snakemake let you write rules with inline Python, R, and shell, in Nextflow, you <em>can use any scripting language</em>. See <a href=\"https://github.com/bionode/gsoc16/blob/543ba66cbb42d1622089764bf01090e318307a57/pipelines/with-nextflow/main.nf\">main.nf</a> for the complete pipeline.</p>\n<p>Nextflow follows the <a href=\"https://en.wikipedia.org/wiki/Dataflow_programming\">dataflow programming paradigm</a> (push) which is also called <a href=\"https://en.wikipedia.org/wiki/Stream_processing\">stream processing</a> or <a href=\"https://en.wikipedia.org/wiki/Reactive_programming\">reactive programming</a>. While in Snakemake you define <code>rule all</code>s prerequisites, and then work backwards from there (pull), in the dataflow model you can think and write in the order that tasks will happen. Since the push model does not depend on inferring a complete dependency tree at the initialization, you can introduce dynamic structures into the pipeline. For example, choosing between two tasks to run <em>based on the output</em> of a previous task. One fallback with the push model is that since it can be dynamic, it is difficult to then perform a dry run, or generate a DAG ahead of time.</p>\n<h3 id=\"Basic-Structure-4\"><a href=\"#Basic-Structure-4\" class=\"headerlink\" title=\"Basic Structure\"></a>Basic Structure</h3><p>Nextflow uses <strong>processes</strong> as rules, and each process will occur in its own folder in <code>/work</code>. Since each process takes place in its own folder, using output from one task as input in another cannot be done the conventional way. But that is alright, because Nextflow provides <strong>channels</strong> to communicate between processes. </p>\n<p>Consider:</p>\n<figure class=\"highlight groovy\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"meta\">#!/usr/bin/env nextflow</span></div><div class=\"line\"></div><div class=\"line\">species = [</div><div class=\"line\">  <span class=\"string\">'Salmonella-enterica'</span>: [</div><div class=\"line\">    <span class=\"string\">'referenceURL'</span>: <span class=\"string\">'http://ftp.ncbi.nlm.nih.gov/genomes/all/GCA_000988525.2_ASM98852v2/GCA_000988525.2_ASM98852v2_genomic.fna.gz'</span>,</div><div class=\"line\">    <span class=\"string\">'readsID'</span>: <span class=\"string\">'2492428'</span></div><div class=\"line\">  ]</div><div class=\"line\">]</div><div class=\"line\"></div><div class=\"line\">process downloadSRA &#123;</div><div class=\"line\">  container <span class=\"string\">'bionode/bionode-ncbi'</span></div><div class=\"line\"><span class=\"symbol\"></div><div class=\"line\">  input:</span> val readsID from species.collect &#123; it.value.readsID &#125;</div><div class=\"line\"><span class=\"symbol\">  output:</span> file <span class=\"string\">'**/*.sra'</span> into reads</div><div class=\"line\"></div><div class=\"line\">  <span class=\"string\">\"\"\"</div><div class=\"line\">  bionode-ncbi download sra $readsID &gt; tmp</div><div class=\"line\">  \"\"\"</span></div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\">process extractSRA &#123;</div><div class=\"line\">  container <span class=\"string\">'inutano/sra-toolkit'</span></div><div class=\"line\"><span class=\"symbol\"></div><div class=\"line\">  input:</span> file read from reads</div><div class=\"line\"><span class=\"symbol\">  output:</span> file <span class=\"string\">'*.fastq.gz'</span> into samples</div><div class=\"line\"></div><div class=\"line\">  <span class=\"string\">\"\"\"</div><div class=\"line\">  fastq-dump --split-files --skip-technical --gzip $read</div><div class=\"line\">  \"\"\"</span></div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<p>See the documentation on [processes] for more. As well, there are more types of channels than <code>val</code> and <code>file</code>, see [channels]. If you are new to [Groovy], and <code>species.collect { it.value.readsID }</code> confuses you, think of it as <code>species.map(specie =&gt; specie.value.readsID)</code>.  It is important to note that while channels may appear confusing at first (why not just use filenames?), they are an elegant solution to a problem we had with Snakemake: file name overlap. With Nextflow, you can be guarenteed no files will ever overwrite each other (unless you do so yourself in one process). As well, this abstracts away the filename, making commands appear generalized:</p>\n<figure class=\"highlight groovy\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div></pre></td><td class=\"code\"><pre><div class=\"line\">process indexReference &#123;</div><div class=\"line\">  container <span class=\"string\">'biodckr/bwa'</span></div><div class=\"line\"><span class=\"symbol\"></div><div class=\"line\">  input:</span> file reference from referenceGenomeGz2</div><div class=\"line\"><span class=\"symbol\">  output:</span> file <span class=\"string\">'*.gz.*'</span> into referenceIndexes</div><div class=\"line\"></div><div class=\"line\">  <span class=\"string\">\"\"\"</div><div class=\"line\">  bwa index $reference</div><div class=\"line\">  \"\"\"</span></div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<p>There is no mental overhead from managing <code>{specie}.genomic.fna.gz</code>, instead we can simply use the variable <code>$reference</code>. This makes the commands in our processes easy to read and comprehend. We can even hardcode output file names in an <em>extremely general way</em> without any fear of file overlap:</p>\n<figure class=\"highlight groovy\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div></pre></td><td class=\"code\"><pre><div class=\"line\">process decompressReference &#123;</div><div class=\"line\">  container <span class=\"string\">'biodckrdev/htslib'</span></div><div class=\"line\"><span class=\"symbol\"></div><div class=\"line\">  input:</span> file referenceGenome from referenceGenomeGz1</div><div class=\"line\"><span class=\"symbol\">  output:</span> file <span class=\"string\">'reference.genomic.fna'</span> into referenceGenomes</div><div class=\"line\"></div><div class=\"line\">  <span class=\"string\">\"\"\"</div><div class=\"line\">  bgzip -d $referenceGenome --stdout &gt; reference.genomic.fna</div><div class=\"line\">  \"\"\"</span></div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<p>Here we output to <code>reference.genomic.fna</code>. A filename that would surely cause problems in any other worflow system.</p>\n<p>Something else to discuss with Nextflow is how to handle a channel being consumed by multiple processes. Since a channel is a FIFO queue in Nextflow, once one process uses it, it is emptied and cannot be consumed by another process. The way to get around this is to create the original channel called <code>myOutput</code> and then clone it via <code>into</code> $n$ channels to be consumed by $n$ processes:</p>\n<figure class=\"highlight groovy\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div></pre></td><td class=\"code\"><pre><div class=\"line\">process downloadReference &#123;</div><div class=\"line\">  container <span class=\"literal\">true</span></div><div class=\"line\"><span class=\"symbol\"></div><div class=\"line\">  input:</span> val referenceURL from species.collect &#123; it.value.referenceURL &#125;</div><div class=\"line\"><span class=\"symbol\">  output:</span> file <span class=\"string\">'reference.genomic.fna.gz'</span> into referenceGenomeGz</div><div class=\"line\"></div><div class=\"line\">  <span class=\"string\">\"\"\"</div><div class=\"line\">  appropriate/curl $referenceURL -o reference.genomic.fna.gz</div><div class=\"line\">  \"\"\"</span></div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// fork reference genome into three other channels</span></div><div class=\"line\">( referenceGenomeGz1,</div><div class=\"line\">  referenceGenomeGz2,</div><div class=\"line\">  referenceGenomeGz3 ) = referenceGenomeGz.into(<span class=\"number\">3</span>)</div></pre></td></tr></table></figure>\n<p>This is also an example of an <em>executable container</em>, where the Dockerfile ends in <code>CMD [&quot;curl&quot;]</code>. </p>\n<p>However, I encountered some issues when trying to dockerize everything. TODO talk about dockerizing processes in a pipe.</p>\n<p>Finally, wherease Snakemake only allowed Python and R inside rules, with Nextflow you can use any scripting language.</p>\n<h3 id=\"Iterative-Development-4\"><a href=\"#Iterative-Development-4\" class=\"headerlink\" title=\"Iterative Development\"></a>Iterative Development</h3><p>Iterative developmen with Nextflow is much improved over the other tools. I felt as though I could extend the pipeline, adding new processes, without having to worry about updating a <code>rule all</code> rule, or cleaning up old files. Nextflow seamlessly recognizes cached process results when given the <code>-resume</code> flag.</p>\n<p>Debugging errors in processes works great. When an error occurs, you can <code>cd</code> into the relevant <code>/work</code> directory, and Nextflow provides log files, and a file that describes which command exactly was ran. This allows you to debug the error in the same environment as it will be ran, and then resume the workflow from that point.</p>\n<h3 id=\"Metrics-4\"><a href=\"#Metrics-4\" class=\"headerlink\" title=\"Metrics\"></a>Metrics</h3><p>Nextflow can create timeline charts:</p>\n<p><img src=\"https://github.com/bionode/gsoc16/raw/543ba66cbb42d1622089764bf01090e318307a57/pipelines/with-nextflow/timeline.png\" alt=\"nextflow-timeline\"></p>\n<p>and a DAG diagram of the pipeline (also notice I was able to create a forking pipeline unlike with Snakemake):</p>\n<p><img src=\"https://github.com/bionode/gsoc16/raw/543ba66cbb42d1622089764bf01090e318307a57/pipelines/with-nextflow/workflow.png\" alt=\"nextflow-workflow\"></p>\n<h3 id=\"Scaling-4\"><a href=\"#Scaling-4\" class=\"headerlink\" title=\"Scaling\"></a>Scaling</h3><p>You might have noticed the <code>container</code> <a href=\"http://www.nextflow.io/docs/latest/process.html#directives\">directive</a> in the above processes. Nextflow comes with built-in docker integration, which is great. <em>Each process can run in its own container</em>. This helps improve the portability and reproducibiltiy of the workflow. All a consumer needs installed on their system is Docker and Nextflow (and you can even run Nextflow in Docker). Each container can use a tagged version, ensuring when someone else runs the pipeline, they are using the <em>exact same version of each tool</em>. You could, of course use Docker in your Snakemake commands, but there would be overhead from volume mounting (mapping a local folder to a folder inside the container); Nextflow handles all that for you. As well, Nextflow has built in support for many cluster engines, which can be enabled by defining the <code>executor</code> in <code>nextflow.config</code>. Another feature to not is Nextflows integration with GitHub. With a <code>main.nf</code> in your repo, you can run a pipeline with <code>nextflow run username/repo</code>. </p>\n<p>Finally, be sure to check out <a href=\"https://github.com/nextflow-io/awesome-nextflow\">awesome-nextflow</a>!</p>\n<h2 id=\"others\"><a href=\"#others\" class=\"headerlink\" title=\"others\"></a>others</h2><p><a href=\"https://github.com/pditommaso/awesome-pipeline\">awesome-pipeline</a> is a great list of many pipeline and workflow tools. Here are some I think are more relevant, and a few extras (though you should try to look through all of them, at least look at these):</p>\n<ul>\n<li><a href=\"http://anduril.org/userguide/\">anduril</a> - Component-based workflow framework for scientific data analysis</li>\n<li><a href=\"https://www.antha-lang.org/docs/concepts/flow-based-programming.html\">antha</a> - High-level language for biology</li>\n</ul>\n<ul>\n<li><a href=\"http://docs.bpipe.org/Examples/PairedEndAlignment/\">bpipe</a> - Tool for running and managing bioinformatics pipelines</li>\n<li><a href=\"https://github.com/tburdett/Conan2\">Conan2</a> - Light-weight workflow management application</li>\n<li><a href=\"https://github.com/LPM-HMS/COSMOS2\">cosmos</a> - Python library for massively parallel workflows</li>\n<li><a href=\"https://github.com/Factual/drake\">drake</a> - Robust DSL akin to Make, implemented in Clojure</li>\n<li><a href=\"http://kronos.readthedocs.io/en/latest/\">kronos</a> - Workflow assembler for cancer genome analytics and informatics</li>\n<li><a href=\"http://kronos.readthedocs.io/en/latest/\">loom</a> - Tool for running bioinformatics workflows locally or in the cloud</li>\n<li><a href=\"http://moa.readthedocs.io/en/latest/\">moa</a> - Lightweight workflows in bioinformatics</li>\n<li><a href=\"https://github.com/adaptivegenome/openge\">OpenGE</a> - Accelerated framework for manipulating and interpreting high-throughput sequencing data</li>\n<li><a href=\"https://github.com/fstrozzi/bioruby-pipengine#-the-pipeline-yaml-\">pipengine</a> - Ruby based launcher for complex biological pipelines</li>\n<li><a href=\"http://www.ruffus.org.uk/\">ruffus</a> - Computation Pipeline library for Python</li>\n<li><a href=\"http://opensource.nibr.com/yap/\">YAP</a> - Extensible parallel framework, written in Python using OpenMPI libraries</li>\n</ul>\n<ul>\n<li><a href=\"http://clusterflow.io/examples/\">clusterflow</a> - Command-line tool which uses common cluster managers to run bioinformatics pipelines</li>\n</ul>\n<ul>\n<li><a href=\"https://github.com/Ensembl/ensembl-hive\">hive</a> - System for creating and running pipelines on a distributed compute resource</li>\n<li><a href=\"https://cloud.google.com/dataflow/docs/\">google cloud dataflow</a> - unified programming model and a managed service for developing and executing a wide range of data processing patterns including ETL, batch computation, and continuous computation</li>\n<li><a href=\"http://ccl.cse.nd.edu/software/makeflow/\">makeflow</a> - Workflow engine for executing large complex workflows on clusters</li>\n<li><a href=\"https://github.com/soravux/scoop/\">scoop</a> - Scalable Concurrent Operations in Python</li>\n<li><a href=\"https://github.com/spotify/luigi\">luigi</a> and <a href=\"https://github.com/pharmbio/sciluigi\">sciluigi</a> - Python module that helps you build complex pipelines of batch jobs, luigi wrapper for scientific workflows</li>\n</ul>\n<ul>\n<li><a href=\"https://usegalaxy.org/\">galaxy</a> - Web-based platform for biomedical research</li>\n<li><a href=\"https://taverna.incubator.apache.org/introduction/\">taverna</a> - Domain independent workflow system</li>\n</ul>\n<ul>\n<li><a href=\"https://github.com/rabix/rabix\">rabix</a> - implementation of CWL2</li>\n</ul>\n<ul>\n<li><a href=\"https://github.com/joergen7/cuneiform\">cuneiform</a> - Advanced functional workflow language and framework, implemented in Erlang</li>\n<li><a href=\"http://ccl.cse.nd.edu/software/makeflow/\">mario</a> - Scala library for defining data pipelines</li>\n</ul>\n<ul>\n<li><a href=\"https://github.com/apache/incubator-airflow\">airflow</a> - Python-based workflow system created by AirBnb</li>\n<li><a href=\"https://github.com/pinterest/pinball\">pinball</a> - Python based workflow engine by Pinterest</li>\n</ul>\n<ul>\n<li><a href=\"https://aws.amazon.com/datapipeline/\">AWS Data Pipeline</a> and <a href=\"https://aws.amazon.com/swf/\">SWF</a> - data workflow orchestration</li>\n<li><a href=\"http://dray.it/\">DRAY: Docker Workflow Engine</a> - UNIX pipes for Docker</li>\n<li><a href=\"https://github.com/BD2KGenomics/toil\">toil</a> - CWL3 and WDL support</li>\n<li><a href=\"https://github.com/chapmanb/bcbio-nextgen\">bcbio</a> -  best-practice pipelines for automated analysis of high throughput sequencing data</li>\n<li><a href=\"https://github.com/pachterlab/kallisto/blob/master/gulpfile.js\">example gulpfile simple pipeline</a></li>\n<li><a href=\"https://github.com/scipipe/scipipe\">scipipe</a> - workflow system in Go inspired by Flow-based programming</li>\n<li><a href=\"https://github.com/agmen-hu/node-datapumps\">node-datapumps</a> - Node.js ETL toolkit for easy data import, export or transfer between systems</li>\n</ul>\n<p>Quite a lot. </p>\n<h2 id=\"Conclusion\"><a href=\"#Conclusion\" class=\"headerlink\" title=\"Conclusion\"></a>Conclusion</h2><p>On a scale from 1-5, these are my ratings for each tool. Mostly as relative to each other, rather than absolutely.</p>\n<table>\n<thead>\n<tr>\n<th>Tool</th>\n<th>Structure</th>\n<th>Iterative Dev.</th>\n<th>Metrics</th>\n<th>Scale</th>\n<th>Reproducibility</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>bash</td>\n<td>1</td>\n<td>1</td>\n<td>1</td>\n<td>1</td>\n<td>1</td>\n</tr>\n<tr>\n<td>make</td>\n<td>2</td>\n<td>2</td>\n<td>1</td>\n<td>1</td>\n<td>1</td>\n</tr>\n<tr>\n<td>Snakemake</td>\n<td>4</td>\n<td>3</td>\n<td>3</td>\n<td>3</td>\n<td>4</td>\n</tr>\n<tr>\n<td>Nextflow</td>\n<td>4</td>\n<td>5</td>\n<td>5</td>\n<td>5</td>\n<td>5</td>\n</tr>\n</tbody>\n</table>\n<p>I have not investigated the others from above to warrant their ranking.</p>\n<p>A quick summary:</p>\n<p><strong>bash</strong></p>\n<ul>\n<li>generic scripting</li>\n<li>simple variable interpolation - <code>$foo</code></li>\n<li>no metrics, difficult to scale</li>\n<li>no reentrancy, poor iterative development</li>\n</ul>\n<p><strong>make</strong></p>\n<ul>\n<li>structure more pipeline oriented than bash - tasks with <em>input</em> (prerequisites) and <em>output</em> (target)</li>\n<li>introduces some element of reentrancy by skipping rules whose target already exists = improved iterative development over bash</li>\n<li>can fork by defining variables that change a rules prerequisites, and running again</li>\n<li>no metrics, difficult to scale</li>\n</ul>\n<p><strong>Snakemake</strong></p>\n<ul>\n<li>enhanced scripting over bash/make with Python while using makes well known rules structure</li>\n<li>improved <code>{input}</code>, <code>{output}</code> syntax and power (e.g. <code>{input.a}</code> or <code>{input[n]}</code>) over make</li>\n<li>run Python, shell, or R in rule</li>\n<li>enhanced wildcarding: easier to understand, more powerful than makes <code>%</code></li>\n<li>benchmarks are OK - need to do manual work to get a full pipeline report</li>\n<li>DAG is nice</li>\n<li>dry run is nice (and possible because of pull paradigm)</li>\n<li>can scale to multiple species easily, just need to make sure file names do not overlap</li>\n<li>cannot create forking paths (e.g. use same reference for two different filtering tools). Could do it by changing <code>FINAL_OUTPUT</code> and running again, but then thats basically the same as it was in <code>make</code>. (again, if you know how to do this without wildcarding ambiguity - let me know!)</li>\n</ul>\n<p><strong>Nextflow</strong></p>\n<ul>\n<li>dataflow paradigm - push</li>\n<li>as a consequence of push, no dry run</li>\n<li>as a consequence of push, flexible forking and dynamic pipelines</li>\n<li>scale to multiple species effortlessly with no overlapping file name concerns</li>\n<li>timeline, DAG, benchmarks metrics are all very nice</li>\n<li>Docker is nice - improves reproducibility and shareability</li>\n<li>cannot handle large <code>stdout | stdin</code> pipes across channels (e.g. one process for <code>bwa mem</code> and another for <code>samtools view</code>)</li>\n</ul>\n<p><strong>bionode-waterwheel</strong> <em>proposal</em></p>\n<ul>\n<li><p>no DSL to learn - just JavaScript. Functional, built around async, events, and streams.</p>\n</li>\n<li><p>streams resonate well with <em>push</em> paradigm</p>\n</li>\n<li><p>support for piping <code>stdin</code>, <code>stdout</code> around, e.g. <code>bwa.mem().pipe(samtools.view())</code> . This enhances modularity of pipeline, rather than having one rule or process doing <code>bwa mem | samtools view</code>, which is really two commands, each deserving their own input, output, parameter definitions. This can aid in improving reproducibility and modularity, as tools with defined input, output, params could be confidentally dropped into the pipeline.</p>\n</li>\n<li><p>moduler interoperation with web apps, native apps (Electron), services (Slack, email). Imagine pipeline logs being sent over a websocket to your browser.</p>\n</li>\n<li><p>interoperation with npm ecosystem</p>\n</li>\n<li><p>integrate with CWL spec</p>\n</li>\n<li><p>metrics will be easier to consume since output will be JSON to be consumed by d3, browser apps, etc</p>\n</li>\n<li><p>modular, specific, customizable waterwheel backend can be integrated into browser or native apps, bringing more power to pipeline GUIs than Galaxy for instance, where it is complicated to develop a custom pipeline (some labs hire someone to create a custom xml config for their specific pipeline so that the wet lab can click to play in Galaxy)</p>\n</li>\n<li><p>variable interpolation - like ES6 template literals, which can take any valid JS expression:</p>\n<figure class=\"highlight javascript\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">const</span> PORT = config.PORT</div><div class=\"line\"><span class=\"keyword\">const</span> template = <span class=\"string\">`Server listening on port <span class=\"subst\">$&#123;PORT&#125;</span>`</span></div></pre></td></tr></table></figure>\n<p>which can actually evaluate any JS expression:</p>\n<figure class=\"highlight javascript\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">[<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">4</span>].map(num =&gt; <span class=\"string\">`is even: <span class=\"subst\">$&#123;num % 2 === 0 ? 'true' : 'false'&#125;</span>`</span>)</div></pre></td></tr></table></figure>\n</li>\n<li><p>simple examples with small datasets can be browser compatibile (see: <a href=\"https://github.com/charto/nbind\">nbind</a>). Live browser examples are great for education - run your own NGS pipeline, from the browser</p>\n</li>\n<li>how to handle interative development is an open question. Can fork all streams into files for reentrancy while in a develop mode. </li>\n</ul>\n<p>A draft pipeline with <strong>bionode-waterwheel</strong>:</p>\n<figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div><div class=\"line\">59</div><div class=\"line\">60</div><div class=\"line\">61</div><div class=\"line\">62</div><div class=\"line\">63</div><div class=\"line\">64</div><div class=\"line\">65</div><div class=\"line\">66</div><div class=\"line\">67</div><div class=\"line\">68</div><div class=\"line\">69</div><div class=\"line\">70</div><div class=\"line\">71</div><div class=\"line\">72</div><div class=\"line\">73</div><div class=\"line\">74</div><div class=\"line\">75</div><div class=\"line\">76</div><div class=\"line\">77</div><div class=\"line\">78</div><div class=\"line\">79</div><div class=\"line\">80</div><div class=\"line\">81</div><div class=\"line\">82</div><div class=\"line\">83</div><div class=\"line\">84</div><div class=\"line\">85</div><div class=\"line\">86</div><div class=\"line\">87</div><div class=\"line\">88</div><div class=\"line\">89</div><div class=\"line\">90</div><div class=\"line\">91</div><div class=\"line\">92</div><div class=\"line\">93</div><div class=\"line\">94</div><div class=\"line\">95</div><div class=\"line\">96</div><div class=\"line\">97</div><div class=\"line\">98</div><div class=\"line\">99</div><div class=\"line\">100</div><div class=\"line\">101</div><div class=\"line\">102</div><div class=\"line\">103</div><div class=\"line\">104</div><div class=\"line\">105</div><div class=\"line\">106</div><div class=\"line\">107</div><div class=\"line\">108</div><div class=\"line\">109</div><div class=\"line\">110</div><div class=\"line\">111</div><div class=\"line\">112</div><div class=\"line\">113</div><div class=\"line\">114</div><div class=\"line\">115</div><div class=\"line\">116</div><div class=\"line\">117</div><div class=\"line\">118</div><div class=\"line\">119</div><div class=\"line\">120</div><div class=\"line\">121</div><div class=\"line\">122</div><div class=\"line\">123</div><div class=\"line\">124</div><div class=\"line\">125</div><div class=\"line\">126</div><div class=\"line\">127</div><div class=\"line\">128</div><div class=\"line\">129</div><div class=\"line\">130</div><div class=\"line\">131</div><div class=\"line\">132</div><div class=\"line\">133</div><div class=\"line\">134</div><div class=\"line\">135</div><div class=\"line\">136</div><div class=\"line\">137</div><div class=\"line\">138</div><div class=\"line\">139</div><div class=\"line\">140</div><div class=\"line\">141</div><div class=\"line\">142</div><div class=\"line\">143</div><div class=\"line\">144</div><div class=\"line\">145</div></pre></td><td class=\"code\"><pre><div class=\"line\">const ncbi = require('bionode-ncbi')</div><div class=\"line\">const wrapper = require('bionode-wrapper')</div><div class=\"line\">const waterwheel = require('bionode-waterwheel')</div><div class=\"line\"></div><div class=\"line\">const &#123; task, join, run &#125; = waterwheel</div><div class=\"line\">const &#123; stdout, stdin, file, directory &#125; = waterwheel.types</div><div class=\"line\"></div><div class=\"line\">// Can be passed in from CLI</div><div class=\"line\">// params for final pipeline call. params are things that do not</div><div class=\"line\">// change how items are passed between processes, but decide output for the</div><div class=\"line\">// pipeline as a whole. For example, species name and reads accession.</div><div class=\"line\">const pipelineParams = &#123;</div><div class=\"line\">  specie: 'Salmonella-enterica',</div><div class=\"line\">  readsID: '2492428',</div><div class=\"line\">  output: 'Salmonella-enterica.vcf'</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\">const sra = task(&#123;</div><div class=\"line\">  output: stdout()</div><div class=\"line\">&#125;, ncbi.download('sra', '&#123;params.readsID&#125;'))</div><div class=\"line\"></div><div class=\"line\">const reference = task(&#123;</div><div class=\"line\">  output: stdout()</div><div class=\"line\">&#125;, ncbi.download('assembly', '&#123;params.specie&#125;'))</div><div class=\"line\">const bwa_index = task(&#123;</div><div class=\"line\">  input: file(),</div><div class=\"line\">  output: file()</div><div class=\"line\">&#125;, wrapper('bwa index &#123;input&#125;'))</div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\">// some tools you cannot decide output file</div><div class=\"line\">// so, tell waterwheel to stream a file as stdout</div><div class=\"line\">// there is --stdout for fastq-dump to give a streamed fastq, but trimmomatic</div><div class=\"line\">// wants reads_1 and reads_2</div><div class=\"line\">// tools that are not bionode will need to wrapper()ed</div><div class=\"line\">const extract = task(&#123;</div><div class=\"line\">  input: file('reads.sra'),</div><div class=\"line\">  output: file([1, 2].map(n =&gt; `reads_$&#123;n&#125;.fastq.gz`))</div><div class=\"line\">&#125;, wrapper('fastq-dump --split-files --skip-technical --gzip &#123;input&#125;'))</div><div class=\"line\"></div><div class=\"line\">const trim = task(&#123;</div><div class=\"line\">  input: file([1, 2].map(n =&gt; `reads_$&#123;n&#125;.fastq.gz`)),</div><div class=\"line\">  output: &#123;</div><div class=\"line\">    pe: file([1, 2].map(n =&gt; `reads_$&#123;n&#125;.trim.pe.fastq.gz`),</div><div class=\"line\">    se: file([1, 2].map(n =&gt; `reads_$&#123;n&#125;.trim.se.fastq.gz`)</div><div class=\"line\">  &#125;,</div><div class=\"line\">  opts: &#123;</div><div class=\"line\">    adapters: '../adapters'</div><div class=\"line\">  &#125;</div><div class=\"line\">&#125;, wrapper('''</div><div class=\"line\">  trimmomatic PE -phred33 \\</div><div class=\"line\">  &#123;input[0]&#125; &#123;input[1]&#125; \\</div><div class=\"line\">  &#123;output.pe[0]&#125; &#123;output.se[0]&#125; \\</div><div class=\"line\">  &#123;output.pe[1]&#125; &#123;output.se[1]&#125; \\</div><div class=\"line\">  ILLUMINACLIP:&#123;opts.adapters&#125;/TruSeq3-PE.fa:2:30:10 \\</div><div class=\"line\">  LEADING:3 TRAILING:3 SLIDINGWINDOW:4:20 MINLEN:36 \\</div><div class=\"line\">'''))</div><div class=\"line\"></div><div class=\"line\">const merge = task(&#123;</div><div class=\"line\">  input: file(),</div><div class=\"line\">  output: stdout()</div><div class=\"line\">&#125;, wrapper('seqtk mergepe &#123;input&#125;'))</div><div class=\"line\"></div><div class=\"line\">const gzip = task(&#123;</div><div class=\"line\">  input: stdin(),</div><div class=\"line\">  output: stdout()</div><div class=\"line\">&#125;, wrapper('gzip - &gt; &#123;output&#125;'))</div><div class=\"line\"></div><div class=\"line\">// Branching: these two filtering types produce the same type of output</div><div class=\"line\">// So we can pass an array, and define them both under the same input/output</div><div class=\"line\">const filter = task(&#123;</div><div class=\"line\">  input: file()</div><div class=\"line\">  output: file()</div><div class=\"line\">  opts: &#123;</div><div class=\"line\">    tmpDir: directory()</div><div class=\"line\">  &#125;</div><div class=\"line\">&#125;, [wrapper('''</div><div class=\"line\">  kmc -k&#123;params.KMERSIZE&#125; -m&#123;params.MEMORYGB&#125; -t&#123;params.THREADS&#125; &#123;input&#125; reads.trim.pe.kmc &#123;opts.tmp&#125; \\</div><div class=\"line\">  kmc_tools filter reads.trim.pe.kmc -cx&#123;params.MINCOVERAGE&#125; &#123;input&#125; -ci0 -cx0 &#123;output&#125; \\</div><div class=\"line\">'''), wrapper('''</div><div class=\"line\">  load-into-counting.py -N 4 -k &#123;params.KMERSIZE&#125; -M &#123;params.MEMORYGB&#125;e9 -T &#123;params.THREADS&#125; reads.trim.pe.fastq.gz.kh &#123;input&#125; \\</div><div class=\"line\">  abundance-dist.py reads.trim.pe.fastq.gz.kh &#123;input&#125; reads.trim.pe.fastq.gz.kh.hist \\</div><div class=\"line\">  filter-abund.py -T &#123;params.THREADS&#125; -C $&#123;MINCOVERAGE&#125; reads.trim.pe.fastq.gz.kh -o &#123;output&#125; &#123;input&#125; \\</div><div class=\"line\">''')])</div><div class=\"line\"></div><div class=\"line\">// file() dependencies will stop stream.</div><div class=\"line\">// for example, need to wait on an index file to be made before aligning</div><div class=\"line\">const bwa_mem = task(&#123;</div><div class=\"line\">  input: &#123;</div><div class=\"line\">    reference: file(),</div><div class=\"line\">    index: file(), // check for reference indexing</div><div class=\"line\">    sample: file() // output of filter</div><div class=\"line\">  &#125;</div><div class=\"line\">  output: stdout()</div><div class=\"line\">&#125;, wrapper('bwa mem &#123;input.reference&#125; &#123;input.sample&#125;'))</div><div class=\"line\"></div><div class=\"line\">// This is where streams in Node can really show</div><div class=\"line\">// In snakemake or Nextflow, this would be bwa mem | samtools view</div><div class=\"line\">// Which is less modular, reproducible, containerizable</div><div class=\"line\">const samtools_view = task(&#123;</div><div class=\"line\">  input: stdin(),</div><div class=\"line\">  output: stdout()</div><div class=\"line\">&#125;, wrapper('samtools view -Sbh &#123;input&#125;'))</div><div class=\"line\"></div><div class=\"line\">const samtools_sort = task(&#123;</div><div class=\"line\">  input: stdin(),</div><div class=\"line\">  output: stdout()</div><div class=\"line\">&#125;, wrapper('samtools sort &#123;input&#125;'))</div><div class=\"line\"></div><div class=\"line\">const samtools_index = task(&#123;</div><div class=\"line\">  input: stdin(),</div><div class=\"line\">  output: stdout()</div><div class=\"line\">&#125;, wrapper('samtools index &#123;input&#125;'))</div><div class=\"line\"></div><div class=\"line\">// these will all get piped through each other:</div><div class=\"line\">// bwa_mem().pipe(samtools_view()).pipe(samtools_sort()).pipe(samtools_index())</div><div class=\"line\">const align = join([bwa_mem, samtools_view, samtools_sort, samtools_index])</div><div class=\"line\"></div><div class=\"line\">const samtools_mpileup = task(&#123;</div><div class=\"line\">  input: &#123;</div><div class=\"line\">    bam: file(),</div><div class=\"line\">    index: file(),</div><div class=\"line\">    reference: file()</div><div class=\"line\">  &#125;,</div><div class=\"line\">  output: stdout()</div><div class=\"line\">&#125;, wrapper('samtools mpileup -uf &#123;input.bam&#125; &#123;input.reference&#125;'))</div><div class=\"line\"></div><div class=\"line\">const bcftools_call = task(&#123;</div><div class=\"line\">  input: stdin(),</div><div class=\"line\">  output: stdout()</div><div class=\"line\">&#125;, wrapper('bcftools call -c &#123;input&#125;'))</div><div class=\"line\"></div><div class=\"line\">const callVariants = join([samtools_mpileup, bcftools_call])</div><div class=\"line\"></div><div class=\"line\">// need a way to use output of another task as input for this one</div><div class=\"line\">const pipeline = join(</div><div class=\"line\">  [sra, extract, trim, merge],</div><div class=\"line\">  // this creates a branching</div><div class=\"line\">  filter,</div><div class=\"line\">  align,</div><div class=\"line\">  callVariants</div><div class=\"line\">)</div><div class=\"line\"></div><div class=\"line\">// Run the whole pipeline, passing in params</div><div class=\"line\">run(pipelineParams, pipeline).pipe(task(fs.createWriteStream('&#123;output&#125;')))</div></pre></td></tr></table></figure>\n"}],"PostAsset":[],"PostCategory":[],"PostTag":[],"Tag":[]}}